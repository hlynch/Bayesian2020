<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Week 2 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes</title>
  <meta name="description" content="3 Week 2 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Week 2 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Week 2 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2020-12-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-1-lab.html"/>
<link rel="next" href="week-3-lecture.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a><ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#introduction-to-this-course"><i class="fa fa-check"></i><b>1.1</b> Introduction to this course</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#some-probability-vocabulary"><i class="fa fa-check"></i><b>1.2</b> Some probability vocabulary</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#statistical-philosophy-and-the-foundations-of-bayesian-analysis"><i class="fa fa-check"></i><b>1.3</b> Statistical philosophy and the foundations of Bayesian analysis</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#testing-jags-installation"><i class="fa fa-check"></i><b>1.4</b> Testing JAGS installation</a></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>1.5</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#bayes-theorem-and-all-that-follows-from-it"><i class="fa fa-check"></i><b>3.1</b> Bayes Theorem and all that follows from it</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#how-do-we-interpret-the-posteriors"><i class="fa fa-check"></i><b>3.2</b> How do we interpret the posteriors?</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#a-slight-detour-to-get-us-thinking-about-the-basic-philosophy-behind-bayesian-stats"><i class="fa fa-check"></i><b>3.3</b> A slight detour, to get us thinking about the basic philosophy behind Bayesian stats</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#getting-some-more-practice-with-jags"><i class="fa fa-check"></i><b>3.4</b> Getting some more practice with JAGS</a></li>
<li class="chapter" data-level="3.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>3.5</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>4</b> Week 3 Lecture</a><ul>
<li class="chapter" data-level="4.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#how-do-we-obtain-priors"><i class="fa fa-check"></i><b>4.1</b> How do we obtain priors?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#published-literature-as-a-source-of-prior-information"><i class="fa fa-check"></i><b>4.1.1</b> Published literature as a source of prior information</a></li>
<li class="chapter" data-level="4.1.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#expert-opinion"><i class="fa fa-check"></i><b>4.1.2</b> Expert opinion</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#conjugacy"><i class="fa fa-check"></i><b>4.2</b> Conjugacy</a></li>
<li class="chapter" data-level="4.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#sensitivity-analysis"><i class="fa fa-check"></i><b>4.3</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="4.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>4.4</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lab</a><ul>
<li class="chapter" data-level="5.1" data-path="week-3-lab.html"><a href="week-3-lab.html#congugacy"><i class="fa fa-check"></i><b>5.1</b> Congugacy</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lab.html"><a href="week-3-lab.html#moment-matching-two-distributions"><i class="fa fa-check"></i><b>5.2</b> Moment Matching two distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lab.html"><a href="week-3-lab.html#from-prior-to-posterior-to-prior"><i class="fa fa-check"></i><b>5.3</b> From Prior to Posterior to Prior</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lab.html"><a href="week-3-lab.html#adding-data-one-at-a-time-or-all-at-once"><i class="fa fa-check"></i><b>5.4</b> Adding data: One at a time or all at once?</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lab.html"><a href="week-3-lab.html#what-impact-did-the-choice-of-prior-have"><i class="fa fa-check"></i><b>5.5</b> What impact did the choice of prior have?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>6</b> Week 4 Lecture</a><ul>
<li class="chapter" data-level="6.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#conjugacy-aside-how-to-actually-calculate-the-posterior"><i class="fa fa-check"></i><b>6.1</b> Conjugacy aside, how to actually calculate the posterior</a></li>
<li class="chapter" data-level="6.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#monte-carlo-methods"><i class="fa fa-check"></i><b>6.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="6.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#rejection-sampling"><i class="fa fa-check"></i><b>6.3</b> Rejection Sampling</a></li>
<li class="chapter" data-level="6.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#adaptive-rejection-sampling"><i class="fa fa-check"></i><b>6.4</b> Adaptive Rejection Sampling</a></li>
<li class="chapter" data-level="6.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#monte-carlo-integration"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="6.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#sometimes-you-just-want-the-integral..."><i class="fa fa-check"></i><b>6.6</b> Sometimes you just want the integral...</a></li>
<li class="chapter" data-level="6.7" data-path="week-4-lecture.html"><a href="week-4-lecture.html#importance-sampling"><i class="fa fa-check"></i><b>6.7</b> Importance Sampling</a></li>
<li class="chapter" data-level="6.8" data-path="week-4-lecture.html"><a href="week-4-lecture.html#sampling-importance-resampling"><i class="fa fa-check"></i><b>6.8</b> Sampling Importance Resampling</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lab</a><ul>
<li class="chapter" data-level="7.1" data-path="week-4-lab.html"><a href="week-4-lab.html#smith-and-gelfand-1992"><i class="fa fa-check"></i><b>7.1</b> Smith and Gelfand (1992)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>8</b> Week 5 Lecture</a><ul>
<li class="chapter" data-level="8.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.1</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#metropolis-algorithm"><i class="fa fa-check"></i><b>8.2</b> Metropolis algorithm</a></li>
<li class="chapter" data-level="8.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-messy-reality-hybrid-of-m-h-and-gibbs"><i class="fa fa-check"></i><b>8.3</b> The Messy reality = Hybrid of M-H and Gibbs</a></li>
<li class="chapter" data-level="8.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#convergence"><i class="fa fa-check"></i><b>8.4</b> Convergence</a></li>
<li class="chapter" data-level="8.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#bayesian-change-point-example"><i class="fa fa-check"></i><b>8.5</b> Bayesian change point example</a></li>
<li class="chapter" data-level="8.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#hierarchical-model"><i class="fa fa-check"></i><b>8.6</b> Hierarchical model</a></li>
<li class="chapter" data-level="8.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>8.7</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lab</a><ul>
<li class="chapter" data-level="9.1" data-path="week-5-lab.html"><a href="week-5-lab.html#gibbs-sampler"><i class="fa fa-check"></i><b>9.1</b> Gibbs Sampler</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>10</b> Week 6 Lab</a><ul>
<li class="chapter" data-level="10.1" data-path="week-6-lab.html"><a href="week-6-lab.html#fitting-a-distribution"><i class="fa fa-check"></i><b>10.1</b> Fitting a distribution</a></li>
<li class="chapter" data-level="10.2" data-path="week-6-lab.html"><a href="week-6-lab.html#one-way-anova"><i class="fa fa-check"></i><b>10.2</b> One-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-7-lecture.html"><a href="week-7-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 7 Lecture</a><ul>
<li class="chapter" data-level="11.1" data-path="week-7-lecture.html"><a href="week-7-lecture.html#an-exercise-with-regression"><i class="fa fa-check"></i><b>11.1</b> An exercise with regression</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-7-lab.html"><a href="week-7-lab.html"><i class="fa fa-check"></i><b>12</b> Week 7 Lab</a><ul>
<li class="chapter" data-level="12.1" data-path="week-7-lab.html"><a href="week-7-lab.html#class-projects"><i class="fa fa-check"></i><b>12.1</b> Class projects</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>13</b> Week 8 Lecture</a></li>
<li class="chapter" data-level="14" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lab</a></li>
<li class="chapter" data-level="15" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>15</b> Week 9 Lecture</a><ul>
<li class="chapter" data-level="15.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#the-probability-of-estimability"><i class="fa fa-check"></i><b>15.1</b> The probability of estimability</a></li>
<li class="chapter" data-level="15.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#multilevel-modelling-ala-gelman-and-hill"><i class="fa fa-check"></i><b>15.2</b> Multilevel modelling ala Gelman and Hill</a></li>
<li class="chapter" data-level="15.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#in-sum...."><i class="fa fa-check"></i><b>15.3</b> In sum....</a></li>
<li class="chapter" data-level="15.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#nice-et-al.-2014"><i class="fa fa-check"></i><b>15.4</b> Nice et al. (2014)</a></li>
<li class="chapter" data-level="15.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>15.5</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lab</a></li>
<li class="chapter" data-level="17" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>17</b> Week 10 Lecture</a><ul>
<li class="chapter" data-level="17.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#convergence"><i class="fa fa-check"></i><b>17.1</b> Convergence</a></li>
<li class="chapter" data-level="17.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#testing-for-convergence"><i class="fa fa-check"></i><b>17.2</b> Testing for convergence</a></li>
<li class="chapter" data-level="17.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#gelman-rubin-statistic"><i class="fa fa-check"></i><b>17.3</b> Gelman-Rubin statistic</a></li>
<li class="chapter" data-level="17.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#the-take-away-what-should-we-be-checking-after-we-run-our-models"><i class="fa fa-check"></i><b>17.4</b> The take away: What should we be checking after we run our models</a></li>
<li class="chapter" data-level="17.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#missing-data"><i class="fa fa-check"></i><b>17.5</b> Missing data</a></li>
<li class="chapter" data-level="17.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#initial-values"><i class="fa fa-check"></i><b>17.6</b> Initial values</a></li>
<li class="chapter" data-level="17.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#sample-scripts-and-output-for-prior-posterior-overlap"><i class="fa fa-check"></i><b>17.7</b> Sample scripts and output for prior-posterior overlap</a></li>
<li class="chapter" data-level="17.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>17.8</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lab</a></li>
<li class="chapter" data-level="19" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>19</b> Week 11 Lecture</a><ul>
<li class="chapter" data-level="19.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#dynamical-time-series-models"><i class="fa fa-check"></i><b>19.1</b> Dynamical (time series) models</a></li>
<li class="chapter" data-level="19.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#process-error"><i class="fa fa-check"></i><b>19.2</b> Process error</a></li>
<li class="chapter" data-level="19.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#observation-error"><i class="fa fa-check"></i><b>19.3</b> Observation error</a></li>
<li class="chapter" data-level="19.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#other-kinds-of-state-space-models"><i class="fa fa-check"></i><b>19.4</b> Other kinds of state-space models</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#missing-data"><i class="fa fa-check"></i><b>19.5</b> Missing data</a></li>
<li class="chapter" data-level="19.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>19.6</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lab</a><ul>
<li class="chapter" data-level="20.1" data-path="week-11-lab.html"><a href="week-11-lab.html#simple-logistic"><i class="fa fa-check"></i><b>20.1</b> Simple logistic</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lab.html"><a href="week-11-lab.html#observation-error-only-model"><i class="fa fa-check"></i><b>20.2</b> Observation-error-only model</a></li>
<li class="chapter" data-level="20.3" data-path="week-11-lab.html"><a href="week-11-lab.html#process-error-only-model"><i class="fa fa-check"></i><b>20.3</b> Process-error-only model</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lab.html"><a href="week-11-lab.html#process-error-and-observation-error-together"><i class="fa fa-check"></i><b>20.4</b> Process-error and observation-error together</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lab.html"><a href="week-11-lab.html#final-thoughts"><i class="fa fa-check"></i><b>20.5</b> Final thoughts</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>21</b> Week 12 Lecture</a><ul>
<li class="chapter" data-level="21.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mark-recapture-modeling"><i class="fa fa-check"></i><b>21.1</b> Mark-recapture modeling</a></li>
<li class="chapter" data-level="21.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#cormack-jolly-seber"><i class="fa fa-check"></i><b>21.2</b> Cormack-Jolly-Seber</a></li>
<li class="chapter" data-level="21.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#method-1-brute-force"><i class="fa fa-check"></i><b>21.3</b> Method #1: Brute force</a></li>
<li class="chapter" data-level="21.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#method-2-modeling-the-entire-capture-history"><i class="fa fa-check"></i><b>21.4</b> Method #2: Modeling the entire capture history</a></li>
<li class="chapter" data-level="21.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#what-other-kind-of-models-might-you-fit"><i class="fa fa-check"></i><b>21.5</b> What other kind of models might you fit</a></li>
<li class="chapter" data-level="21.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#occupancy-modelling"><i class="fa fa-check"></i><b>21.6</b> Occupancy modelling</a></li>
<li class="chapter" data-level="21.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#dynamic-state-space-models-for-meta-population-dynamics"><i class="fa fa-check"></i><b>21.7</b> Dynamic state-space models for meta-population dynamics</a></li>
<li class="chapter" data-level="21.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>21.8</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lab</a><ul>
<li class="chapter" data-level="22.1" data-path="week-12-lab.html"><a href="week-12-lab.html#the-zeros-trick"><i class="fa fa-check"></i><b>22.1</b> The 'zeros' trick</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lab.html"><a href="week-12-lab.html#the-ones-trick"><i class="fa fa-check"></i><b>22.2</b> The 'ones' trick</a></li>
<li class="chapter" data-level="22.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#initial-values"><i class="fa fa-check"></i><b>22.3</b> Initial values</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lab.html"><a href="week-12-lab.html#first-a-warm-up"><i class="fa fa-check"></i><b>22.4</b> First, a warm up</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lab.html"><a href="week-12-lab.html#fitting-mark-recapture-models"><i class="fa fa-check"></i><b>22.5</b> Fitting mark-recapture models</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lab.html"><a href="week-12-lab.html#faq"><i class="fa fa-check"></i><b>22.6</b> FAQ</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>23</b> Week 13 Lecture</a><ul>
<li class="chapter" data-level="23.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#a-side-note-about-summary-statistics-the-mona-lisa"><i class="fa fa-check"></i><b>23.1</b> A side note about summary statistics: The Mona Lisa</a></li>
<li class="chapter" data-level="23.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#rejection-abc"><i class="fa fa-check"></i><b>23.2</b> Rejection ABC</a></li>
<li class="chapter" data-level="23.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-1-basic-rejection-abc"><i class="fa fa-check"></i><b>23.3</b> Option #1: Basic rejection ABC</a></li>
<li class="chapter" data-level="23.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-2-markov-chain-monte-carlo-abc"><i class="fa fa-check"></i><b>23.4</b> Option #2: Markov Chain Monte Carlo ABC</a></li>
<li class="chapter" data-level="23.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-3-sequential-monte-carlo-abc"><i class="fa fa-check"></i><b>23.5</b> Option #3: Sequential Monte Carlo ABC</a></li>
<li class="chapter" data-level="23.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#drawbacks-to-abc"><i class="fa fa-check"></i><b>23.6</b> Drawbacks to ABC</a></li>
<li class="chapter" data-level="23.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>23.7</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lab</a></li>
<li class="chapter" data-level="25" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>25</b> Week 14 Lecture</a><ul>
<li class="chapter" data-level="25.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>25.1</b> For more information about this week's topic</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lab</a></li>
<li class="chapter" data-level="27" data-path="week-15-lecture.html"><a href="week-15-lecture.html"><i class="fa fa-check"></i><b>27</b> Week 15 Lecture</a><ul>
<li class="chapter" data-level="27.1" data-path="week-15-lecture.html"><a href="week-15-lecture.html#a-quick-step-back-what-are-the-goals-of-model-selection"><i class="fa fa-check"></i><b>27.1</b> A quick step back: What are the goals of model selection?</a></li>
<li class="chapter" data-level="27.2" data-path="week-15-lecture.html"><a href="week-15-lecture.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>27.2</b> Bayesian model averaging</a></li>
<li class="chapter" data-level="27.3" data-path="week-15-lecture.html"><a href="week-15-lecture.html#beyond-bayes-factors"><i class="fa fa-check"></i><b>27.3</b> Beyond Bayes Factors</a></li>
<li class="chapter" data-level="27.4" data-path="week-15-lecture.html"><a href="week-15-lecture.html#variable-selection-for-nested-models"><i class="fa fa-check"></i><b>27.4</b> Variable selection for nested models</a></li>
<li class="chapter" data-level="27.5" data-path="week-15-lecture.html"><a href="week-15-lecture.html#prior-data-conflict"><i class="fa fa-check"></i><b>27.5</b> Prior-data conflict</a></li>
<li class="chapter" data-level="27.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>27.6</b> For more information about this week's topic</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-2-lecture" class="section level1">
<h1><span class="header-section-number">3</span> Week 2 Lecture</h1>
<p>Papers to read this week:</p>
<ul>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/Ellison1996.pdf">Ellison 1996</a></li>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/Ellison2004.pdf">Ellison 2004</a>: More advanced material that may take a few weeks to sink in</li>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/BergerBerry1988.pdf">Berger and Berry 1988</a>: This is a repeat from Biometry but worth refreshing even if you took Biometry recently and required reading for those that did not take Biometry.</li>
</ul>
<p>This week we will review the basic elements of Bayesian inference, building on our introduction to Bayes Theorem last week.</p>
<p>Everyone was asked to watch <a href="http://www.sumsar.net/blog/2017/02/introduction-to-bayesian-data-analysis-part-one/">this video</a> before lecture today. I think this video does a particularly nice job explaining the basic ideas underlying Bayesian inference, and having that intuition under your belt will help when we get into the math.</p>
<p>At the end of this video, you got a brief introduction to Approximate Bayesian Computation. I'll introduce a few highlights here, since doing so makes Bayesian analysis very intuitive, but we'll reserve a more complete discussion for Week #15.</p>
<div id="bayes-theorem-and-all-that-follows-from-it" class="section level2">
<h2><span class="header-section-number">3.1</span> Bayes Theorem and all that follows from it</h2>
<p>We start with Bayes theorem (here I am following the notation of Ellison [1996]):</p>
<p><span class="math display">\[
P(\theta|x) = \frac{P(x|\theta)P(\theta)}{P(x)}
\]</span> or, as it is more commonly written,</p>
<p><span class="math display">\[
P(\theta|x) \propto P(x|\theta)P(\theta)
\]</span></p>
<p>In other words, the posterior probability distribution for the parameter <span class="math inline">\(\theta\)</span> conditional on the data <span class="math inline">\(x\)</span> is proportional to the likelihood P(x|<span class="math inline">\(\theta\)</span>) times the prior distribution <span class="math inline">\(P(\theta)\)</span>. We can think about this (vis a vis the video we just saw) as the likelihood &quot;filtering out&quot; the prior distribution. In other words, we have some distribution that describes our prior understanding of the parameter, and we use the data to sift through which values make more or less sense. Values that make the data more likely are themselves more likely, and vice versa.</p>
<p>Next week we will focus on prior distributions and their interpretation, so we won’t say much more about prior distributions at this stage. We will however work through the algebra of Bayes Theorem to see how we obtain a posterior distribution.</p>
<p>Let’s say we have one observation <span class="math inline">\(y\)</span> from a Normal distribution of unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^{2}\)</span>. The likelihood of obtaining that observation is simply:</p>
<p><span class="math display">\[
P(y|\mu) \propto exp\left(\frac{-(y-\mu)^{2}}{2\sigma^2}\right)
\]</span> where I have included only the terms involving the unknown parameter <span class="math inline">\(\mu\)</span>. Note that I have not included <span class="math inline">\(\sigma\)</span> after the conditional &quot;|&quot; because I want to emphasize that only <span class="math inline">\(\mu\)</span> is unknown.</p>
<p>(Side note on notation. By tradition, both probability densities and likelihoods have the data on the left side and the parameters on the right side on the conditional &quot;|&quot;. This is not how I originally introduced them last week, because in Week #1's lecture I was trying to highlight the differences between PDFs and likelihoods, which involves what is known and what is unknown. However, to be consistent with the book and papers on the subject, here I revert back to the traditional notation where the data &quot;y&quot; is on the left hand side. Also, I use the letters <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> interchangeably so my choice of one or the other to represent data is arbitrary.)</p>
<p>Let’s assume a fairly broad (i.e. large variance) Normal distribution prior for <span class="math inline">\(\mu\)</span>, <span class="math inline">\(N(\mu_{0},\tau^{2})\)</span>, so that:</p>
<p><span class="math display">\[
P(\mu) \propto exp\left(\frac{-(\mu-\mu_{0})^{2}}{2\tau^2}\right)
\]</span></p>
<p>Make sure this distribution makes sense. <strong>In this case, the distribution is on <span class="math inline">\(\mu\)</span></strong> and its parameters are <span class="math inline">\(\mu_{0}\)</span> and <span class="math inline">\(\tau\)</span>. (We are not used to thinking about parameters as having distributions. In frequentist statistics, <span class="math inline">\(\mu\)</span> is considered a fixed quantity, but here and throughout the semester, we will assume that all quantities, including parameters, are random variables described by a probability density function.)</p>
<p>The posterior distribution is now given by the product:</p>
<p><span class="math display">\[
P(\mu|y) \propto exp\left(\frac{-(y-\mu)^{2}}{2\sigma^2}\right)exp\left(\frac{-(\mu-\mu_{0})^{2}}{2\tau^2}\right)
\]</span> <span class="math display">\[
P(\mu|y) \propto exp\left(-\frac{1}{2}\left(\frac{(y-\mu)^{2}}{\sigma^2}+\frac{(\mu-\mu_{0})^{2}}{\tau^2}\right)\right)
\]</span> I’ll spare you the algebra, but this can be simplified as:</p>
<p><span class="math display">\[
P(\mu|y) \propto exp\left(-\frac{1}{2}\frac{(\mu-V\nu)^{2}}{V}\right)
\]</span> where,</p>
<p><span class="math display">\[
\frac{1}{V} = \frac{1}{\sigma^2}+\frac{1}{\tau^2}
\]</span> and,</p>
<p><span class="math display">\[
\nu = \frac{y}{\sigma^2}+\frac{\mu_{0}}{\tau^2}
\]</span> While this expression looks kind of messy, in the limits it makes a lot of sense:</p>
<p><span class="math display">\[
lim_{\tau \rightarrow \infty} V\nu \rightarrow \frac{y/\sigma^2}{1/\sigma^2} = y
\]</span> In other words, the mean of the posterior distribution is just the value of the single data point if the prior is so broad as to contribute no information to the posterior.</p>
<p><strong>Question: What is <span class="math inline">\(lim_{\tau \rightarrow 0} V\nu\)</span>?</strong></p>
<p><details> <summary>Click for Answer</summary> <span style="color: blueviolet;"> In this case,</p>
<p><span class="math inline">\(V \rightarrow \tau^2\)</span></p>
<p><span class="math inline">\(\nu \rightarrow \frac{\mu_{0}}{\tau^2}\)</span></p>
<p>Therefore, <span class="math inline">\(V\nu \rightarrow \mu_{0}\)</span>. </span> </details></p>
<p> </p>
<p>Likewise, in the <span class="math inline">\(lim_{\tau \rightarrow \infty} V \rightarrow \frac{1}{1/\sigma^2} = \sigma^2\)</span></p>
<p>which is just the known variance for the data’s distribution. (This should make some sense keeping in mind that the posterior variance represents the uncertainty on the posterior mean. If we consider the central limit theorem (CLT), we know that the standard error of the mean is given by <span class="math inline">\(\sigma/\sqrt{n}\)</span> but in this case <span class="math inline">\(n=1\)</span> so this is exactly what we would expect.)</p>
<p>What happens if we have <span class="math inline">\(n &gt; 1\)</span> independent observations?</p>
<p>In this case the likelihood is a product:</p>
<p><span class="math display">\[
P(y|\mu) \propto \prod_{i=1}^{N}exp\left(-\frac{(y_{i}-\mu)^2}{2\sigma^2}\right) = exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_{i}-\mu)^2\right)
\]</span></p>
<p>Working through the same algebra, we have the same basic form for the posterior distribution:</p>
<p><span class="math display">\[
P(\mu|y) \propto exp\left(-\frac{1}{2}\frac{(\mu-V\nu)^2}{V}\right)
\]</span> except now,</p>
<p><span class="math display">\[
\frac{1}{V} = \frac{n}{\sigma^2}+\frac{1}{\tau^2}
\]</span> and,</p>
<p><span class="math display">\[
\nu = \frac{n\bar{y}}{\sigma^2}+\frac{\mu_{0}}{\tau^2}
\]</span> (Why do we only keep track of the posterior up to a proportion? Since we know the posterior has to integrate to 1 [not withstanding the use of an “improper” prior, more on that next week], we can always work out the constant if we needed. In practice, however, we usually use Monte Carlo methods to draw samples from the posterior and, for reasons we will discuss in a few weeks, this only requires knowledge of the posterior up to within a proportionality constant.)</p>
<p>You can see that as <span class="math inline">\(n\)</span> gets large, the data will start to dominate the prior. In fact:</p>
<p><span class="math display">\[
lim_{n \rightarrow \infty} V\nu \rightarrow \frac{\frac{n\bar{y}}{\sigma^2}}{\frac{n}{\sigma^2}} = \bar{y}
\]</span></p>
<p>Once again, this is precisely what we would expect under the CLT.</p>
<p>In general, we can see from this fairly simply case with Normal distributions that the posterior is a <strong>weighted average of the prior and the likelihood</strong>, with the weights determined by the variances <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\tau^2\)</span> and the sample size. <strong>Larger sample size weights the likelihood more strongly than the prior.</strong> What happens if <span class="math inline">\(\tau^2\)</span> is very very small? In this case, the prior dominates the posterior entirely, irrespective of the data. This is a good reminder that you want the prior to have reasonable support (non-negligible density) for all values that you think might be supported by the likelihood, because this will allow the posterior to shift and bend towards the likelihood as more data is used.</p>
<p>What if the variance <span class="math inline">\(\sigma^2\)</span> was not assumed known? Then we would have to include a prior for <span class="math inline">\(\sigma^2\)</span> as well. In this case, the joint posterior distribution would be given as:</p>
<p><span class="math display">\[
P(\mu,\sigma|y) \propto P(y|\mu,\sigma)P(\mu)P(\sigma)
\]</span> If we wanted the marginal posterior distribution for <span class="math inline">\(\mu\)</span>, then we would need to integrate out <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[
P(\mu|y) = \int P(\mu,\sigma|y)d\sigma
\]</span></p>
</div>
<div id="how-do-we-interpret-the-posteriors" class="section level2">
<h2><span class="header-section-number">3.2</span> How do we interpret the posteriors?</h2>
<p>Whereas frequentist methods are focused on rejecting a null hypothesis, Bayesian analyses are focused on the posterior distribution as a statement of the probability of the parameter lying within a certain interval (for a continuous parameter). Bayesian methods often (but not always) end with a statement about the mean (sometimes median) of the posterior distribution and some interval around that measure of central tendency.</p>
<p>Its worth reminding ourselves that there are three measures of central tendancy that we might want to use to describe the posterior distribution. While the <strong>mean</strong> is perhaps most common, it is not unusual in Bayesian analyses to report instead the <strong>median</strong> or the <strong>mode</strong>; it often depends on which measure of central tendency you think is the most biologically relevant. We can also use the posterior distribution to answer other questions about the parameter(s) of interest. For example, if we wanted to know, &quot;What is the probability that <span class="math inline">\(\mu\)</span> is positive?&quot; then all we need to do is integrate the posterior distribution from 0 to <span class="math inline">\(\infty\)</span>. That yields a direct and easily interpreted probability, and allows us to easily test a wider variety of hypotheses than was easy accomplished with a frequentist approach.</p>
<p>We define a 100(1-<span class="math inline">\(\alpha\)</span>)<span class="math inline">\(\%\)</span> credible interval for a parameter <span class="math inline">\(\theta\)</span> as:</p>
<p><span class="math display">\[
P(\theta \in B_{X}|X) = 1 - \alpha
\]</span> where <span class="math inline">\(B_{X}\)</span> is some interval. In the discussion to follow I will assume 100(1-<span class="math inline">\(\alpha\)</span>)<span class="math inline">\(\%\)</span> equals 95<span class="math inline">\(\%\)</span>, but this is only because it makes the discussion more intuitive and not because there is anything magical about <span class="math inline">\(\alpha\)</span>=0.05.</p>
<p>There are an infinite number of intervals that would contain 95<span class="math inline">\(\%\)</span> of the area under the curve (i.e. the pdf of the posterior), and so there are an infinite number of 95th percentile credible intervals (heretofore, CI). One possibility is to make sure there is 2.5<span class="math inline">\(\%\)</span> in each tail (the “central” CI). Another is to find the shortest interval that contains 95<span class="math inline">\(\%\)</span> of the probability. This latter interval is called the highest posterior density interval or HDPI. Calculation of the HDPI can be quite difficult, especially because we often lack an analytical form for the posterior, so often the central CI is used instead.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="CredibleInterval.png" alt="Source: Doug Schluter [link](https://www.zoology.ubc.ca/biol548/lecturepdf/09.Bayes.pdf)" width="75%" />
<p class="caption">
Figure 1.1: Source: Doug Schluter <a href="https://www.zoology.ubc.ca/biol548/lecturepdf/09.Bayes.pdf">link</a>
</p>
</div>
<p>Note that while summary statistics are useful, it is often the case that authors will publish a histogram of the entire posterior distribution. This can be particularly valuable if the posterior is highly skew (or even multimodal).</p>
</div>
<div id="a-slight-detour-to-get-us-thinking-about-the-basic-philosophy-behind-bayesian-stats" class="section level2">
<h2><span class="header-section-number">3.3</span> A slight detour, to get us thinking about the basic philosophy behind Bayesian stats</h2>
<p>For lecture today I provided an article (below) by Fenton and Neil (2012) and the accompanying article from the Guardian.</p>
<p>The basic story at hand: Convicted killer “T” appealed his conviction, which was based in part on a Bayesian analysis that found strong evidence linking his sneakers (found in his home) to a shoeprint at the crime scene. A judge threw out “T”s conviction on the basis that Bayesian statistics (specifically, expert opinion that had been used to generate a prior distribution for the number of sneakers of that type in the UK at the time) was not “firm” statistics. The court went further and basically rejected the use of Bayesian statistics in all cases except (somewhat arbitrarily, in my opinion) in the analysis of DNA evidence.</p>
<p>Fortunately, Bayesian statisticians have come to its defense noting, for example, “The fact that variables cannot be precisely expressed does not affect the validity of the relationships described by [Bayes] formula”.</p>
<p>A few questions that were raised in Fenton and Neil (2012):</p>
<p><strong>1) When is it OK to multiply likelihoods? Why was the practice of multiplying likelihoods (or likelihood ratios, equivalently in this case) criticized in the original court case?</strong></p>
<p><details> <summary>Click for Answer</summary> <span style="color: blueviolet;"> Multiplying probability/likleihoods is the <strong>correct</strong> thing to do <strong>if</strong> the traits being discussed are independent. However, many physical traits used in forensic analyses are not, in fact, independent (e.g., height and weight). When the traits being discussed are not independent, then it is incorrect to calculate the joint probability as the product of the probabilities for each trait. </span> </details></p>
<p><strong>2) Why is a 1 in 10,000 fingerprint match not necessarily strong evidence that the accused was at the scene? (Hint: Can you ever accept the null hypothesis? What is the null hypothesis in this case? Why does it matter?)</strong></p>
<details> <summary>Click for Answer</summary> <span style="color: blueviolet;">
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="BayesianInference.png" alt="Source: Doug Schluter [link](https://www.zoology.ubc.ca/biol548/lecturepdf/09.Bayes.pdf)" width="75%" />
<p class="caption">
Figure 2.1: Source: Doug Schluter <a href="https://www.zoology.ubc.ca/biol548/lecturepdf/09.Bayes.pdf">link</a>
</p>
</div>
<p>Note that if p=<span class="math inline">\(10^{-6}\)</span>, then <span class="math inline">\(P(\mbox{guilt}|\mbox{match})\)</span>=0.5. If p=<span class="math inline">\(0.5\)</span>, then <span class="math inline">\(P(\mbox{guilt}|\mbox{match})\)</span>=0.999999. So, your decision about the guilt or innocence of the defendent depends a lot on your prior expectations for their guilt or innocence. </span> </details></p>
<p> </p>
<p>Fenton and Neil raise the idea of a Bayesian network, which we will return to briefly next week. A Bayesian network is a graphical model that shows the conditional relations among variables, and is just a more visual way of understanding a Bayesian analysis with many conditional relationships.</p>
</div>
<div id="getting-some-more-practice-with-jags" class="section level2">
<h2><span class="header-section-number">3.4</span> Getting some more practice with JAGS</h2>
<p>We don’t have a formal lab this week, but we can use McCarthy’s example in Boxes 3.4 and 3.5 to practice running models in JAGS. This also gives us an opportunity to see how Bayesian models align with frequentist approaches we may be more familiar with.</p>
<p><strong>EXERCISE #1</strong>: Run the code from McCarthy Box 3.4, which fits the following model:</p>
<p><span class="math display">\[
\#Trees \sim Pois(\mbox{mean density})
\]</span> for the number of trees recorded in equal sized quadrats. First, use the prior and starting values suggested by McCarthy. Then try and break JAGS – use other prior distributions and wildly different starting values. Do the starting values influence the time to convergence? (Look at the posterior draws...)</p>
<p><strong>EXERCISE #2</strong>: Compare the result obtained using JAGS to that obtained in a frequentist analysis using the R function ‘glm’. Are they the same? Why or why not? Does it depend on the prior?</p>
<p><strong>EXERCISE #3</strong>: Add the extra variation as described in Box 3.5.</p>
<p>10 pt BONUS (turn in with problem set): Compare the result obtained using JAGS (from Exercise #3) to that obtained in a frequentist random effects analysis using the R function ‘glmer’. (Nominally, the Poisson model with extra variation is just a random effects model.)</p>
<ol style="list-style-type: lower-alpha">
<li>Is your glmer model actually equivalent to the JAGS model? If its different, how so?</li>
<li>How do you compare the output of glmer with that of JAGS? How do we even know whether they yield the same inference?</li>
<li>How do you calculate the variance of the random effect as output by glmer?</li>
</ol>
</div>
<div id="for-more-information-about-this-weeks-topic" class="section level2">
<h2><span class="header-section-number">3.5</span> For more information about this week's topic</h2>
<p>Both of the articles below discuss the use of Bayesian methods in the criminal justice system:</p>
<ul>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/FentonNeil2012.pdf">Fenton and Neil 2012</a></li>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/TheGuardian.pdf">A formula for justice in <span class="math inline">\(\textit{The Guardian}\)</span></a></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-1-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-3-lecture.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
