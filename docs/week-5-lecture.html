<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Data Analysis and Computation Lecture and Lab Notes</title>
  <meta name="description" content="Bayesian Data Analysis and Computation Lecture and Lab Notes">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch">


<meta name="date" content="2020-07-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="week-4-lab.html">
<link rel="next" href="week-5-lab.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a><ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#introduction-to-this-course"><i class="fa fa-check"></i><b>1.1</b> Introduction to this course</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#statistical-philosophy-and-the-foundations-of-bayesian-analysis"><i class="fa fa-check"></i><b>1.2</b> Statistical philosophy and the foundations of Bayesian analysis</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#some-probability-vocabulary"><i class="fa fa-check"></i><b>1.3</b> Some probability vocabulary</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#testing-jags-installation"><i class="fa fa-check"></i><b>1.4</b> Testing JAGS installation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#bayes-theorem-and-all-that-follows-from-it"><i class="fa fa-check"></i><b>3.1</b> Bayes Theorem and all that follows from it</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#a-slight-detour-to-get-us-thinking-about-the-basic-philosophy-behind-bayesian-stats"><i class="fa fa-check"></i><b>3.2</b> A slight detour, to get us thinking about the basic philosophy behind Bayesian stats</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#getting-some-more-practice-with-jags"><i class="fa fa-check"></i><b>3.3</b> Getting some more practice with JAGS</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>4</b> Week 3 Lecture</a><ul>
<li class="chapter" data-level="4.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#how-do-we-obtain-priors"><i class="fa fa-check"></i><b>4.1</b> How do we obtain priors?</a></li>
<li class="chapter" data-level="4.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#conjugacy"><i class="fa fa-check"></i><b>4.2</b> Conjugacy</a></li>
<li class="chapter" data-level="4.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#sensitivity-analysis"><i class="fa fa-check"></i><b>4.3</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="4.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#expert-elicitation"><i class="fa fa-check"></i><b>4.4</b> Expert elicitation</a></li>
<li class="chapter" data-level="4.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#more-information"><i class="fa fa-check"></i><b>4.5</b> More information</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lab</a><ul>
<li class="chapter" data-level="5.1" data-path="week-3-lab.html"><a href="week-3-lab.html#congugacy"><i class="fa fa-check"></i><b>5.1</b> Congugacy</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lab.html"><a href="week-3-lab.html#moment-matching-two-distributions"><i class="fa fa-check"></i><b>5.2</b> Moment Matching two distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lab.html"><a href="week-3-lab.html#from-prior-to-posterior-to-prior"><i class="fa fa-check"></i><b>5.3</b> From Prior to Posterior to Prior</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lab.html"><a href="week-3-lab.html#adding-data-one-at-a-time-or-all-at-once"><i class="fa fa-check"></i><b>5.4</b> Adding data: One at a time or all at once?</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lab.html"><a href="week-3-lab.html#what-impact-did-the-choice-of-prior-have"><i class="fa fa-check"></i><b>5.5</b> What impact did the choice of prior have?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>6</b> Week 4 Lecture</a><ul>
<li class="chapter" data-level="6.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#rejection-sampling"><i class="fa fa-check"></i><b>6.1</b> Rejection Sampling</a></li>
<li class="chapter" data-level="6.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#adaptive-rejection-sampling"><i class="fa fa-check"></i><b>6.2</b> Adaptive Rejection Sampling</a></li>
<li class="chapter" data-level="6.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#monte-carlo-integration"><i class="fa fa-check"></i><b>6.3</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="6.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#sometimes-you-just-want-the-integral"><i class="fa fa-check"></i><b>6.4</b> Sometimes you just want the integral…</a></li>
<li class="chapter" data-level="6.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#importance-sampling"><i class="fa fa-check"></i><b>6.5</b> Importance Sampling</a></li>
<li class="chapter" data-level="6.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#sampling-importance-resampling"><i class="fa fa-check"></i><b>6.6</b> Sampling Importance Resampling</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lab</a><ul>
<li class="chapter" data-level="7.1" data-path="week-4-lab.html"><a href="week-4-lab.html#smith-and-gelfand-1992"><i class="fa fa-check"></i><b>7.1</b> Smith and Gelfand (1992)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>8</b> Week 5 Lecture</a><ul>
<li class="chapter" data-level="8.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.1</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#metropolis-algorithm"><i class="fa fa-check"></i><b>8.2</b> Metropolis algorithm</a></li>
<li class="chapter" data-level="8.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-messy-reality-hybrid-of-m-h-and-gibbs"><i class="fa fa-check"></i><b>8.3</b> The Messy reality = Hybrid of M-H and Gibbs</a></li>
<li class="chapter" data-level="8.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#convergence"><i class="fa fa-check"></i><b>8.4</b> Convergence</a></li>
<li class="chapter" data-level="8.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#bayesian-change-point-example"><i class="fa fa-check"></i><b>8.5</b> Bayesian change point example</a></li>
<li class="chapter" data-level="8.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#hierarchical-model"><i class="fa fa-check"></i><b>8.6</b> Hierarchical model</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lab</a><ul>
<li class="chapter" data-level="9.1" data-path="week-5-lab.html"><a href="week-5-lab.html#gibbs-sampler"><i class="fa fa-check"></i><b>9.1</b> Gibbs Sampler</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>10</b> Week 6 Lab</a><ul>
<li class="chapter" data-level="10.1" data-path="week-6-lab.html"><a href="week-6-lab.html#fitting-a-distribution"><i class="fa fa-check"></i><b>10.1</b> Fitting a distribution</a></li>
<li class="chapter" data-level="10.2" data-path="week-6-lab.html"><a href="week-6-lab.html#one-way-anova"><i class="fa fa-check"></i><b>10.2</b> One-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-7-lecture.html"><a href="week-7-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 7 Lecture</a><ul>
<li class="chapter" data-level="11.1" data-path="week-7-lecture.html"><a href="week-7-lecture.html#class-projects"><i class="fa fa-check"></i><b>11.1</b> Class projects</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-7-lab.html"><a href="week-7-lab.html"><i class="fa fa-check"></i><b>12</b> Week 7 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>13</b> Week 8 Lecture</a></li>
<li class="chapter" data-level="14" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lab</a></li>
<li class="chapter" data-level="15" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>15</b> Week 9 Lecture</a><ul>
<li class="chapter" data-level="15.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#the-probability-of-estimability"><i class="fa fa-check"></i><b>15.1</b> The probability of estimability</a></li>
<li class="chapter" data-level="15.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#multilevel-modelling-ala-gelman-and-hill"><i class="fa fa-check"></i><b>15.2</b> Multilevel modelling ala Gelman and Hill</a></li>
<li class="chapter" data-level="15.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#in-sum."><i class="fa fa-check"></i><b>15.3</b> In sum….</a></li>
<li class="chapter" data-level="15.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#nice-et-al.-2014"><i class="fa fa-check"></i><b>15.4</b> Nice et al. (2014)</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lab</a></li>
<li class="chapter" data-level="17" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>17</b> Week 10 Lecture</a><ul>
<li class="chapter" data-level="17.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#convergence-1"><i class="fa fa-check"></i><b>17.1</b> Convergence</a></li>
<li class="chapter" data-level="17.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#testing-for-convergence"><i class="fa fa-check"></i><b>17.2</b> Testing for convergence</a></li>
<li class="chapter" data-level="17.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#gelman-rubin-statistic"><i class="fa fa-check"></i><b>17.3</b> Gelman-Rubin statistic</a></li>
<li class="chapter" data-level="17.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#the-take-away-what-should-we-be-checking-after-we-run-our-models"><i class="fa fa-check"></i><b>17.4</b> The take away: What should we be checking after we run our models</a></li>
<li class="chapter" data-level="17.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#missing-data"><i class="fa fa-check"></i><b>17.5</b> Missing data</a></li>
<li class="chapter" data-level="17.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#initial-values"><i class="fa fa-check"></i><b>17.6</b> Initial values</a></li>
<li class="chapter" data-level="17.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#sample-scripts-and-output-for-prior-posterior-overlap"><i class="fa fa-check"></i><b>17.7</b> Sample scripts and output for prior-posterior overlap</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lab</a></li>
<li class="chapter" data-level="19" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>19</b> Week 11 Lecture</a><ul>
<li class="chapter" data-level="19.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#dynamical-time-series-models"><i class="fa fa-check"></i><b>19.1</b> Dynamical (time series) models</a></li>
<li class="chapter" data-level="19.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#process-error"><i class="fa fa-check"></i><b>19.2</b> Process error</a></li>
<li class="chapter" data-level="19.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#observation-error"><i class="fa fa-check"></i><b>19.3</b> Observation error</a></li>
<li class="chapter" data-level="19.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#other-kinds-of-statespace-models"><i class="fa fa-check"></i><b>19.4</b> Other kinds of state=space models</a></li>
<li class="chapter" data-level="19.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#missing-data-1"><i class="fa fa-check"></i><b>19.5</b> Missing data</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lab</a><ul>
<li class="chapter" data-level="20.1" data-path="week-11-lab.html"><a href="week-11-lab.html#simple-logistic"><i class="fa fa-check"></i><b>20.1</b> Simple logistic</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lab.html"><a href="week-11-lab.html#observation-error-only-model"><i class="fa fa-check"></i><b>20.2</b> Observation-error-only model</a></li>
<li class="chapter" data-level="20.3" data-path="week-11-lab.html"><a href="week-11-lab.html#process-error-only-model"><i class="fa fa-check"></i><b>20.3</b> Process-error-only model</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lab.html"><a href="week-11-lab.html#process-error-and-observation-error-together"><i class="fa fa-check"></i><b>20.4</b> Process-error and observation-error together</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lab.html"><a href="week-11-lab.html#final-thoughts"><i class="fa fa-check"></i><b>20.5</b> Final thoughts</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>21</b> Week 12 Lecture</a><ul>
<li class="chapter" data-level="21.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mark-recapture-modeling"><i class="fa fa-check"></i><b>21.1</b> Mark-recapture modeling</a></li>
<li class="chapter" data-level="21.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#cormack-jolly-seber"><i class="fa fa-check"></i><b>21.2</b> Cormack-Jolly-Seber</a></li>
<li class="chapter" data-level="21.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#method-1-brute-force"><i class="fa fa-check"></i><b>21.3</b> Method #1: Brute force</a></li>
<li class="chapter" data-level="21.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#method-2-modeling-the-entire-capture-history"><i class="fa fa-check"></i><b>21.4</b> Method #2: Modeling the entire capture history</a></li>
<li class="chapter" data-level="21.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#what-other-kind-of-models-might-you-fit"><i class="fa fa-check"></i><b>21.5</b> What other kind of models might you fit</a></li>
<li class="chapter" data-level="21.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#occupancy-modelling"><i class="fa fa-check"></i><b>21.6</b> Occupancy modelling</a></li>
<li class="chapter" data-level="21.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#dynamic-state-space-models-for-meta-population-dynamics"><i class="fa fa-check"></i><b>21.7</b> Dynamic state-space models for meta-population dynamics</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lab</a><ul>
<li class="chapter" data-level="22.1" data-path="week-12-lab.html"><a href="week-12-lab.html#the-zeros-trick"><i class="fa fa-check"></i><b>22.1</b> The ‘zeros’ trick</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lab.html"><a href="week-12-lab.html#the-ones-trick"><i class="fa fa-check"></i><b>22.2</b> The ‘ones’ trick</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lab.html"><a href="week-12-lab.html#initial-values-1"><i class="fa fa-check"></i><b>22.3</b> Initial values</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lab.html"><a href="week-12-lab.html#first-a-warm-up"><i class="fa fa-check"></i><b>22.4</b> First, a warm up</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lab.html"><a href="week-12-lab.html#fitting-mark-recapture-models"><i class="fa fa-check"></i><b>22.5</b> Fitting mark-recapture models</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>23</b> Week 13 Lecture</a><ul>
<li class="chapter" data-level="23.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#rejection-abc"><i class="fa fa-check"></i><b>23.1</b> Rejection ABC</a></li>
<li class="chapter" data-level="23.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-1-basic-rejection-abc"><i class="fa fa-check"></i><b>23.2</b> Option #1: Basic rejection ABC</a></li>
<li class="chapter" data-level="23.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-2-markov-chain-monte-carlo-abc"><i class="fa fa-check"></i><b>23.3</b> Option #2: Markov Chain Monte Carlo ABC</a></li>
<li class="chapter" data-level="23.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-3-sequential-monte-carlo-abc"><i class="fa fa-check"></i><b>23.4</b> Option #3: Sequential Monte Carlo ABC</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 14 Lecture</a><ul>
<li class="chapter" data-level="24.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#the-mona-lisa"><i class="fa fa-check"></i><b>24.1</b> The Mona Lisa</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>25</b> Week 14 Lab</a></li>
<li class="chapter" data-level="26" data-path="week-15-lecture.html"><a href="week-15-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 15 Lecture</a><ul>
<li class="chapter" data-level="26.1" data-path="week-15-lecture.html"><a href="week-15-lecture.html#a-quick-step-back-what-are-the-goals-of-model-selection"><i class="fa fa-check"></i><b>26.1</b> A quick step back: What are the goals of model selection?</a></li>
<li class="chapter" data-level="26.2" data-path="week-15-lecture.html"><a href="week-15-lecture.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>26.2</b> Bayesian model averaging</a></li>
<li class="chapter" data-level="26.3" data-path="week-15-lecture.html"><a href="week-15-lecture.html#beyond-bayes-factors"><i class="fa fa-check"></i><b>26.3</b> Beyond Bayes Factors</a></li>
<li class="chapter" data-level="26.4" data-path="week-15-lecture.html"><a href="week-15-lecture.html#variable-selection-for-nested-models"><i class="fa fa-check"></i><b>26.4</b> Variable selection for nested models</a></li>
<li class="chapter" data-level="26.5" data-path="week-15-lecture.html"><a href="week-15-lecture.html#prior-data-conflict"><i class="fa fa-check"></i><b>26.5</b> Prior-data conflict</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-5-lecture" class="section level1">
<h1><span class="header-section-number">8</span> Week 5 Lecture</h1>
<p>Last week we talked about several applications of MC methods. This week, we are going to expand on these ideas to cover two additional, very important, applications:</p>
<ol style="list-style-type: decimal">
<li>Gibbs sampling</li>
<li>Metropolis / Metropolis-Hastings</li>
</ol>
<p>Keep in mind that the point behind doing MC sampling is that it provides a mechanism for sampling from the posterior distribution. (Remember, the posterior distribution may have [in fact, usually has] no straightforward analytical form.) Last week we covered a class of non-iterative samplers that require us to have a candidate distribution that we could sample from. (They are non-iterative, because we could have drawn all N candidate values simultaneously. The accept-reject criteria for each sample was independent of the accept-reject decision made on the others.)</p>
<p>The problem with these methods is that it is often very hard to come up with a good candidate distribution that is a good approximation to the target distribution (the closer the better – otherwise, we have a very inefficient algorithm) and which we can sample from. (In one-dimension, this is usually not an issue, the problems arise when you have multi-dimensional problems because candidate distributions for the joint PDF are harder to generate.) Another class of samplers, used more often in Bayesian statistics, is iterative samplers that (for the most part) rely on the generation of a “Markov chain”.</p>
<p>Before we get into MCMC, its worth thinking a bit more about what a Markov Chain is. A Markov Chain is a series of observations, each one dependent only on the last (or, in more complex examples, or a finite number of previous observations). At the end of the day, a Markov Chain is simply a string of numbers (or, equivalently, states), which we will denote <span class="math inline">\(X^{t}\)</span>. So the state of the chain at <span class="math inline">\(t=0\)</span> is <span class="math inline">\(X^{0}\)</span>, etc.</p>
<p>The easiest way to represent the allowed transitions in the chain is through a transition matrix <span class="math display">\[
P = \begin{bmatrix}
p_{11} &amp; p_{12} &amp; p_{13}\\
p_{21} &amp; p_{22} &amp; p_{23}\\
p_{31} &amp; p_{32} &amp; p_{33}
\end{bmatrix}
\]</span> where <span class="math inline">\(p_{12}\)</span>, for example, is the probability of making the transition between state 1 and state 2 in the chain.</p>
<p>Let’s say that we start the Markov Chain with an initial value of <span class="math inline">\(X^{0} = 1\)</span>. What is the probability that we end up at state 3 after two time steps (i.e. that <span class="math inline">\(X^{2} = 3\)</span>)?</p>
<p>To figure this out, we have to account for all possible Markov Chains that could have occurred between <span class="math inline">\(t=0\)</span> and <span class="math inline">\(t=2\)</span>:</p>
<table>
<thead>
<tr class="header">
<th>Path</th>
<th align="center"><span class="math inline">\(X^{0}\)</span></th>
<th align="center"><span class="math inline">\(X^{1}\)</span></th>
<th align="center"><span class="math inline">\(X^{2}\)</span></th>
<th align="center">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>#1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">$p_{11}p_{13}</td>
</tr>
<tr class="even">
<td>#2</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">$p_{12}p_{23}</td>
</tr>
<tr class="odd">
<td>#3</td>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">$p_{13}p_{33}</td>
</tr>
</tbody>
</table>
<p>Therefore, the probability of being in state 3 by <span class="math inline">\(t=2\)</span> is the sum of these three probabilities: <span class="math inline">\(p_{11}p_{13}+p_{12}p_{23}+p_{13}p_{33}\)</span>.</p>
<p>Note that the transition probabilities are independent (that’s why the total probability is just the product of the two individual transition probabilities). The “Markov”ness comes about because the probability of transitioning from State 1 to State 3 is independent of the path that got you to State 1 in the first place, and so the probability of going to State 3 depends only on the fact that you are currently in State 1.</p>
<p>If we let the chain run long enough, you can see that we will eventually reach all the states. (None of the states are absorbing states, so you have a finite probability of moving from each state to every other state – such chains are called “ergodic”. I won’t get into any proofs or formal math here, but some vocabulary will be helpful in reading the literature on MCMC.) Not only will you reach all of the states, but the long-run probability of being in each state will eventually converge to a distribution which is governed by the transition matrix and is independent of the starting point (<span class="math inline">\(X^{0}\)</span>).</p>
<p>(Note that I am presenting Markov Chains in the context of discrete states, but the same logic holds when we extend this to continuous states whose transitions are described by a probability density.)</p>
<p>Mathematically, we would say that the  probability of state <span class="math inline">\(X^{t+1}\)</span> is given by the following</p>
<p><span class="math display">\[
X^{t+1} \sim p_{trans}(x|X^{t} = x^{t})
\]</span> that is, that the probability is conditional on the value of the state at time <span class="math inline">\(t\)</span>. The important element of this is that even though the individual states of the chain are conditional on the preceding states, the marginal (unconditional) distribution of the states will converge to a unique stationary distribution that is independent of the starting values.</p>
<p>From the Bayesian perspective, our goal is to choose a transition distribution that will generate a sequence of distributions for the parameters of our model whose unique stationary distribution is the joint posterior distribution of interest. (Some of this will be clearer as we go through more applied examples…)</p>
<p>Now that we have the basic idea, let’s discuss Gibbs sampling, which is one of the most widely used algorithms for generating a Markov Chain.</p>
<div id="gibbs-sampling" class="section level2">
<h2><span class="header-section-number">8.1</span> Gibbs Sampling</h2>
<p>Gibbs sampling is a method for obtaining a multidimensional Markov Chain by splitting a vector of random variables into univariate draws from conditional distributions. A simple example would be a three-dimensional Markov Chain for parameters <span class="math inline">\((\theta_{1},\theta_{2},\theta_{3})\)</span>. We may have no easy way to sample from this joint probability density, but we might know all the conditional probability distributions:</p>
<p><span class="math display">\[
p(\theta_{1}|\theta_{2},\theta_{3},y) \\ 
p(\theta_{2}|\theta_{1},\theta_{3},y) \\ 
p(\theta_{3}|\theta_{1},\theta_{2},y)
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the data.</p>
<p>The basic strategy is as follows:</p>
<ol style="list-style-type: decimal">
<li>Choose arbitrary starting values for <span class="math inline">\((\theta_{1},\theta_{2},\theta_{3})\)</span>, which we will call <span class="math inline">\((\theta_{1}^{(0)},\theta_{2}^{(0)},\theta_{3}^{(0)})\)</span>.</li>
<li>Iterate through each parameter, drawing from its conditional distribution for the next iteration of the chain, conditioning on the most recent values for each parameter</li>
</ol>
<p><span class="math display">\[
\theta_{1}^{(1)} \sim p(\theta_{1}|\theta_{2}^{(0)},\theta_{3}^{(0)},y) \\
\theta_{2}^{(1)} \sim p(\theta_{2}|\theta_{1}^{(1)},\theta_{3}^{(0)},y) \\
\theta_{3}^{(1)} \sim p(\theta_{3}|\theta_{1}^{(1)},\theta_{2}^{(1)},y) \\
\theta_{1}^{(2)} \sim p(\theta_{1}|\theta_{2}^{(1)},\theta_{3}^{(1)},y)
\]</span> and so forth.</p>
<ol start="3" style="list-style-type: decimal">
<li>Continue iterating through the chain a large number of times, until the chain has converged (we haven’t yet discussed how we would test convergence yet).</li>
</ol>
<p>EXERCISE: We’re going to practice this a bit using a simple example inspired by our earlier work on the uniform triangle distribution. Earlier we drew samples from a uniform triangle distribution using Rejection Sampling. Here we are going to use Gibbs Sampling. First, you will need to work out the two conditional probabilities <span class="math inline">\(p(x|y)\)</span> and <span class="math inline">\(p(y|x)\)</span>. You could of course use Bayes Theorem to calculate this</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="UniformTriangle.png" alt="Uniform triangle distribution" width="25%" />
<p class="caption">
Figure 1.1: Uniform triangle distribution
</p>
</div>
<p><span class="math display">\[
p(x|y) = p(x,y)/p(y)
\]</span> and</p>
<p><span class="math display">\[
p(y|x) = p(x,y)/p(x)
\]</span></p>
<p>since you already worked out the marginal probabilities <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(p(y)\)</span> but in this case its easier just to reason through it. In other words, if you knew <span class="math inline">\(x\)</span>, what would be the possible range of <span class="math inline">\(y\)</span>, and vice versa? (Here we are restricting the range to <span class="math inline">\(x \in (0,1)\)</span> and <span class="math inline">\(y \in (0,1)\)</span>, which is slightly different from our original example, but it should be straightforward to work out the two marginal probabilities involved here.)</p>
<p>Once you have the conditional probabilities, Gibbs sampling proceeds as follows (<span class="math inline">\(k\)</span> is the variable to track the number of iterations):</p>
<ol style="list-style-type: decimal">
<li>Choose legitimate starting values <span class="math inline">\((x_{0},y_{0})\)</span>.</li>
<li>Sample <span class="math inline">\(x_{1}\)</span> from <span class="math inline">\(p(x_{1}|y_{0})\)</span>.</li>
<li>Sample <span class="math inline">\(y_{1}\)</span> from <span class="math inline">\(p(y_{1}|x_{1})\)</span>.</li>
<li><span class="math inline">\(k \rightarrow k+1\)</span>, go back to step 2</li>
</ol>
<p>When you have tried a large number of samples <span class="math inline">\(k\)</span>, plot your bivariate samples - how do they look? (In other words, make a scatterplot of the sampled <span class="math inline">\((x,y)\)</span> pairs.)</p>
<p>Now histogram just the <span class="math inline">\(x\)</span> values. This is the <strong>marginal probability density</strong> <span class="math inline">\(p(x)\)</span>. Does it make sense why? Think back to the exercise we did with graph paper. If we have bivariate samples from <span class="math inline">\((x,y)\)</span> and we simply tally up all the samples for a certain x-value (irrespective of the y values involved), we have “marginalized out” <span class="math inline">\(y\)</span> to create <span class="math inline">\(p(x)\)</span>.</p>
<p>When we do modelling in JAGS, we will do this automatically. In fact, the posterior summaries provided by JAGS and the related R packages automatically provide marginal posterior summaries, and they do that by summarizing the draws for one variables ignoring the values for all the other variables. (I think of this as taking one column from the sims.matrix.)</p>
</div>
<div id="metropolis-algorithm" class="section level2">
<h2><span class="header-section-number">8.2</span> Metropolis algorithm</h2>
<p>The Metropolis algorithm is widely used, and can be described by the following steps, which describe the transition from one state <span class="math inline">\(x\)</span> to another state <span class="math inline">\(x^{\prime}\)</span> as follows:</p>
<ol style="list-style-type: decimal">
<li>Assuming we are currently in a state <span class="math inline">\(x\)</span>, a “candidate” <span class="math inline">\(x^*\)</span> is proposed according to some symmetric probability <span class="math inline">\(S(x,x^*)\)</span> (symmetric means that <span class="math inline">\(S(x,x^*)=S(x^*,x)\)</span>).</li>
</ol>
<p>(Note that Metropolis-Hastings is the same as Metropolis with the added flexibility of assymetric jumps. We won’t cover M-H specifically, except to to note that the terms are often used interchangeably because symmetric jumps are so common.)</p>
<ol start="2" style="list-style-type: decimal">
<li>This candidate, <span class="math inline">\(x^*\)</span>, is accepted as the next state with probability</li>
</ol>
<p><span class="math display">\[
\mbox{min}[1,\pi(x^*)/\pi(x)]
\]</span></p>
<p>where <span class="math inline">\(\pi(x)\)</span> is the probability density associated with <span class="math inline">\(x\)</span>, and likewise, <span class="math inline">\(\pi(x^*)\)</span> is the probability density associated with <span class="math inline">\(x^*\)</span>. If <span class="math inline">\(x^*\)</span> is accepted, then <span class="math inline">\(x^{t+1}=x^{*}\)</span>. If <span class="math inline">\(x^*\)</span> is rejected, then <span class="math inline">\(x^{t+1} = x^{t}\)</span>. (In other words, if you reject the candidate state, then you stay put.)</p>
<p>Note that you only need to be able to evaluate the ratio <span class="math inline">\(\pi(x^*)/\pi(x)\)</span>, so you don’t need to worry about, or even know, the normalizing constants associated with the function <span class="math inline">\(\pi\)</span>. The proposal distribution <span class="math inline">\(S(x,x^*)\)</span> is critical here because it controls how quickly or slowly the chain samples the space, and how quickly the chain converges. One popular proposal distribution is that which would be associated with an unbiased random walk.</p>
<p>Some terminology here to navigate if you are applying the Metropolis algorithm to a multivariate problem:</p>
<p>: Proposals only change one component of <span class="math inline">\(x\)</span>, and updates to each component are applied in sequence.</p>
<p>: Proposals change all components of <span class="math inline">\(x\)</span> simultaneously. For example, you might generate proposals of the form</p>
<p><span class="math display">\[
\begin{pmatrix}
\theta_{1}^{proposed}\\
\theta_{2}^{proposed}\\
\end{pmatrix} \sim N \left(
\begin{pmatrix}
0\\
0\\
\end{pmatrix},
\begin{pmatrix}
1 &amp; 0.5\\
0.5 &amp; 1\\
\end{pmatrix}\right)
\]</span> A nice visual of M-H (courtesy of Paul Lewis) is given here</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="MHdiagram.png" alt="Visualizing Metropolis-Hastings movement rules. Source: Paul Lewis" width="100%" />
<p class="caption">
Figure 3.1: Visualizing Metropolis-Hastings movement rules. Source: Paul Lewis
</p>
</div>
<p>You can see that, under the movement “rules” of M-H, the robot tends to spend the most time near the top of the hill.</p>
<p>(FYI: Gibbs sampling is a special case of M-H in which the proposed value of the parameter (the one drawn from the conditional) is always accepted.)</p>
</div>
<div id="the-messy-reality-hybrid-of-m-h-and-gibbs" class="section level2">
<h2><span class="header-section-number">8.3</span> The Messy reality = Hybrid of M-H and Gibbs</h2>
<p>JAGS and related software use a messy combination strategy that combines Gibbs sampling and M-H sampling. The essential idea is that you may find yourself with a suite of parameters, some of which have known conditional relationships, and others which do not. In these cases, you can use Gibbs where you can, and M-H where you can’t use Gibbs. (Why bother with Gibbs at all? Remember that Gibbs has an acceptance ratio of 1, so its more efficient than M-H.) This is what is hidden under the hood of JAGS. Note that JAGS always starts off with “adaptive sampling” (in the GUI, you can see this happening, but from R this is hidden…). This is the period of time when JAGS is trying to tune the M-H proposals to get a target acceptance ratio.</p>
</div>
<div id="convergence" class="section level2">
<h2><span class="header-section-number">8.4</span> Convergence</h2>
<p>One of the big challenges in doing MCMC is assessing whether your chains have converged to their unique stationary distribution.</p>
<p>How do we assess convergence?</p>
<p>Its often easy to see that chains have  converged – we usually do this visually by starting with three “overdispersed” sets of starting values and simply plotting the chains to see whether they are indistinguishable after some period of time. Since we are only interested in the stationary behavior and not the initial transient behavior, we discard the beginning of each chain so our posterior distribution is only derived from the stationary portion of the chain. We call the portion of the chain discarded the “burn in”. (Papers will report this number, as in “we used a burn in of 1,000 iterations…” and so forth.)</p>
<p>Another aspect to MCMC chain assessment is looking at how well they are “mixing”, which is just to ask how well they are sampling the posterior distribution. Warning signs that your chains are not mixing well is when the chains get stuck in certain regions of parameter space (in these cases, the posterior histograms tends to be multimodal as well.)</p>
<p>(You want the chains to look like white noise…)</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="WhiteNoiseChain.png" alt="Samples from an MCMC sampler that has converged." width="100%" />
<p class="caption">
Figure 8.1: Samples from an MCMC sampler that has converged.
</p>
</div>
<p>The autocorrelation function can provide additional information.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="WhiteNoiseAutocorrelation.png" alt="Autocorrelation function from MCMC samples." width="100%" />
<p class="caption">
Figure 8.2: Autocorrelation function from MCMC samples.
</p>
</div>
<p>Sometimes, to save storage, you may decide to subsample, or “thin” the chain and store only every other draw (or every 3rd, or 10th, etc.). The autocorrelation function can help you decide how spaced the draws would have to be in order to be independent. However, note that there is no need to thin the chains unless memory is an issue. Its better to keep all the draws if you can.</p>
<p>However, when chains converge slowly, or meander over parameter space, its often hard to prove that the chains have converged (or, more precisely, to say that you cannot reject the null hypothesis that the three chains come from the same distribution). (Meandering over parameter space is annoying, but it doesn’t necessarily mean that your script doesn’t work or the problem cannot be solved. Sometimes the chains just mix slowly, and while there are tricks to try and make them mix more quickly, sometimes you just have to run really long chains to get a good picture for the posterior distribution.</p>
<p>There are many ways to formally test for convergence; we will discuss this in Week 9. For now, we’ll just visually assess the chains for convergence, which is what people actually do right up until a manuscript reviewer asks for something more rigorous. (But much better not to make assumptions that might cause for an embarrassing retraction following review…)</p>
</div>
<div id="bayesian-change-point-example" class="section level2">
<h2><span class="header-section-number">8.5</span> Bayesian change point example</h2>
<p>The following is based on the Bayesian change point example by Murali Haran.</p>
<p>Consider the following simple changepoint example: You have Poisson draws and you expect that the Poisson parameter has changed at some point in the time series, so that</p>
<p><span class="math display">\[
Y_{i}|k,\theta,\lambda \sim Pois(\theta) \mbox{for i=1,...,k}
\]</span></p>
<p><span class="math display">\[
Y_{i}|k,\theta,\lambda \sim Pois(\lambda) \mbox{for i=k+1,...,n}
\]</span></p>
<p>We assume the following prior distributions for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span></p>
<p><span class="math display">\[
\theta \sim Gamma(0.5,3)
\]</span> <span class="math display">\[
\lambda \sim Gamma(0.5,3)
\]</span> <span class="math display">\[
k \sim Unif(1,n)
\]</span></p>
<p>We will assume the following parameterization for Gamma</p>
<p><span class="math display">\[
Gamma(\alpha,\beta) = \frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1}e^{-x/\beta}
\]</span></p>
<p>We want to use the data <span class="math inline">\(Y\)</span> to make inference about the following three dimensional posterior</p>
<p><span class="math display">\[
f(\theta,\lambda,k|Y) \propto \left(\prod_{i=1}^{k}\frac{\theta^{Y_{i}}e^{-\theta}}{Y_{i}!}\right) \times \left(\prod_{i=k+1}^{n}\frac{\lambda^{Y_{i}}e^{-\lambda}}{Y_{i}!}\right) \times \frac{1}{\Gamma(0.5)3^{0.5}}\theta^{-0.5}e^{-\theta/3} \times \frac{1}{\Gamma(0.5)3^{0.5}}\lambda^{-0.5}e^{-\lambda/3} \times \frac{1}{n}
\]</span></p>
<p>To do Gibbs sampling we need to sample from the posterior for each parameter conditional on the other parameters (and the data). Let’s start with the conditional posterior for k. We use the following fact</p>
<p><span class="math display">\[
f(k|\theta,\lambda,Y)=\frac{f(\theta,\lambda,k|Y)}{f(\theta,\lambda|Y)}
\]</span></p>
<p>which means that</p>
<p><span class="math display">\[
f(k|\theta,\lambda,Y) \propto f(\theta,\lambda,k|Y)
\]</span></p>
<p>where on the right hand side, we only need to worry about the terms that actually involve k. In this case</p>
<p><span class="math display">\[
f(k|\theta,\lambda,Y) \propto \left(\prod_{i=1}^{k}\frac{\theta^{Y_{i}}e^{-\theta}}{Y_{i}!}\right) \left( \prod_{i=k+1}^{n}\frac{\lambda^{Y_{i}}e^{-\lambda}}{Y_{i}!}\right)
\]</span></p>
<p>In fact, we can simplify this a bit further</p>
<p><span class="math display">\[
f(k|\theta,\lambda,Y) \propto \left(\prod_{i=1}^{k}\theta^{Y_{i}}e^{-\theta}\right) \left(\prod_{i=k+1}^{n}\lambda^{Y_{i}}e^{-\lambda}\right)
\]</span></p>
<p>because the denominators of the two products equal <span class="math inline">\(\prod_{i=1}^{n}Y_{i}!\)</span> which does not involve k.</p>
<p>What about <span class="math inline">\(\theta\)</span>? Using the same idea as above</p>
<p><span class="math display">\[
f(\theta|\lambda,k,Y)=\frac{f(\theta,\lambda,k|Y)}{f(\lambda,k|Y)}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
f(\theta|\lambda,k,Y) \propto f(\theta,\lambda,k|Y)
\]</span></p>
<p>where again we only need the parts of the right hand side that involve <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
f(\theta|\lambda,k,Y) \propto \left(\prod_{i=1}^{k}\theta^{Y_{i}}e^{-\theta}\right) \times \theta^{-0.5}e^{-\theta/3} 
\]</span></p>
<p>What about <span class="math inline">\(\lambda\)</span>? Using the same idea as above</p>
<p><span class="math display">\[
f(\lambda|\theta,k,Y)=\frac{f(\lambda,\theta,k|Y)}{f(\theta,k|Y)}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
f(\lambda|\theta,k,Y) \propto f(\lambda,\theta,k|Y)
\]</span></p>
<p>where again we only need the parts of the right hand side that involve <span class="math inline">\(\lambda\)</span></p>
<p><span class="math display">\[
f(\lambda|\theta,k,Y) \propto \left(\prod_{i=k+1}^{n}\lambda^{Y_{i}}e^{-\lambda}\right) \times \lambda^{-0.5}e^{-\lambda/3} 
\]</span></p>
<p>So, how does Gibb’s sampling actually work in this case?</p>
<p>First we pick starting values (at random, but within the support of the prior) for <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(k\)</span>. We call these <span class="math inline">\(\lambda^{(0)}\)</span>, <span class="math inline">\(\theta^{(0)}\)</span>, and <span class="math inline">\(k^{(0)}\)</span>. We then sample from each conditional posterior at a time:</p>
<p><span class="math display">\[
k^{(1)} \sim \left(\prod_{i=1}^{k^{(0)}}(\theta^{(0)})^{Y_{i}}e^{-\theta^{(0)}}\right) \left(\prod_{i=k^{(0)}+1}^{n}(\lambda^{(0)})^{Y_{i}}e^{-\lambda^{(0)}}\right)
\]</span></p>
<p>then</p>
<p><span class="math display">\[
\theta^{(1)} \sim \left(\prod_{i=1}^{k^{(1)}}(\theta^{(0)})^{Y_{i}}e^{-\theta^{(0)}}\right) \times (\theta^{(0)})^{-0.5}e^{-\theta^{(0)}/3} 
\]</span></p>
<p>Which can simplify to</p>
<p><span class="math display">\[
\theta^{(1)} \sim Gamma(\sum_{i=1}^{k^{(1)}}Y_{i}+0.5,\frac{3}{3k^{(1)}+1})
\]</span></p>
<p>because the Gamma is conjugate to the Poisson.</p>
<p>Finally,</p>
<p><span class="math display">\[
\lambda^{(1)} \sim \left(\prod_{i=k^{(1)}+1}^{n}(\lambda^{(0)})^{Y_{i}}e^{-\lambda^{(0)}}\right) \times (\lambda^{(0)})^{-0.5}e^{-\lambda^{(0)}/3} 
\]</span></p>
<p>Likewise…</p>
<p><span class="math display">\[
\lambda^{(1)} \sim Gamma(\sum_{i=k^{(1)}+1}^{n}Y_{i}+0.5,\frac{3}{3(n-k^{(1)})+1})
\]</span></p>
<p>Note that while the posteriors for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span> have well-known forms because of conjugacy, the conditional posterior for k does not have a standard PDF and we only know its PDF up to a proportion. That’s OK, though, because we can use any of the tools have have learned to sample from PDFs that are not “built-into-R”. What we have done is break apart a complex 3-dimensional posterior into three one-dimensional posteriors. Often, these conditional posteriors are easy to write down because you’ve used conjugate priors that have an analytically tractable posterior (as with <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span>). In this case, the conditional posterior for k cannot be written in an easy form, so we have to use some other method of sampling <em>inside</em> the Gibbs loop. See Gelfand 2000 Section 2.4. Notice that when the conditional posterior take a non-standard form, we can employ any of the tools we have learned, including rejection sampling, importance sampling, or Metropolis-Hastings. The last of these is quite a common approach, and it is called “Metropolis-within-Gibbs”. However, there are no cookbook recipes for doing this, you (as the modeller) have to choose among the tools to find one that works for your case.) The next round of draws would look like</p>
<p><span class="math display">\[
k^{(2)} \sim \left(\prod_{i=1}^{k^{(1)}}(\theta^{(1)})^{Y_{i}}e^{-\theta^{(1)}}\right) \left(\prod_{i=k^{(1)}+1}^{n}(\lambda^{(1)})^{Y_{i}}e^{-\lambda^{(1)}}\right)
\]</span></p>
<p><span class="math display">\[
\theta^{(2)} \sim \left(\prod_{i=1}^{k^{(2)}}(\theta^{(1)})^{Y_{i}}e^{-\theta^{(1)}}\right) \times (\theta^{(1)})^{-0.5}e^{-\theta^{(1)}/3} 
\]</span></p>
<p><span class="math display">\[
\lambda^{(2)} \sim \left(\prod_{i=k^{(2)}+1}^{n}(\lambda^{(1)})^{Y_{i}}e^{-\lambda^{(1)}}\right) \times (\lambda^{(1)})^{-0.5}e^{-\lambda^{(1)}/3} 
\]</span></p>
<p>We then repeat this process the draw <span class="math inline">\(\lambda^{(3)}\)</span>, <span class="math inline">\(\theta^{(3)}\)</span>, and <span class="math inline">\(k^{(3)}\)</span> etc.</p>
</div>
<div id="hierarchical-model" class="section level2">
<h2><span class="header-section-number">8.6</span> Hierarchical model</h2>
<p>The full hierachical model originally posed by Haran includes a prior on one of the parameters for the Gamma prior on <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span>. In other words, we postulate a prior distribution for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span>, but we say that we don’t know what the prior distribution should be exactly, and so we put a prior distribution on one of its parameters. Specifically,</p>
<p><span class="math display">\[
\theta|b_{1} \sim Gamma(0.5, b_{1})
\]</span> <span class="math display">\[
\lambda|b_{2} \sim Gamma(0.5, b_{2})
\]</span> <span class="math display">\[
b_{1} \sim Gamma(5,5)
\]</span> <span class="math display">\[
b_{2} \sim Gamma(5,5)
\]</span> <span class="math display">\[
k \sim Unif(1,...,n)
\]</span></p>
<p>These “priors of priors” (<span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>) are called hyperpriors. Note that I have replaced <span class="math inline">\(c_{1}\)</span>, <span class="math inline">\(c_{2}\)</span>, <span class="math inline">\(d_{1}\)</span>, and <span class="math inline">\(d_{2}\)</span> from Haran with the number 5, which is arbitrary but I wanted to make it clear what was a fixed number and what was a parameter. We will come back to these when we discuss hierarchical models in Week #14. For now, I’ll simply say that as each step of the Gibbs sampling, you need to sample from all the terms that include the parameter of interest. Whereas in the non-hierarchical example, we had three parameters, now we have five parameters that we need to iteratively sample from.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-4-lab.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-5-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
