[
["index.html", "Bayesian Data Analysis and Computation Lecture and Lab Notes Preface", " Bayesian Data Analysis and Computation Lecture and Lab Notes Heather Lynch 2020-08-08 Preface This eBook contains all of the lecture notes and lab exercises that we will do this semester in Bayesian Data Analysis and Computation. While I have made every effort to cite sources where I’ve used “found” material, this eBook reflects my own personal notes drawn up over the last seven years and some material may not properly identify the original sources used in drawing up my initial lecture notes. As I have moved this material into an online eBook, I have tried to better document material inspired by or drawn from other sources. If you find anything in these notes that is not properly cited or sourced, please let me know so it can be amended. Any mistakes are mine and mine alone. "],
["week-1-lecture.html", "1 Week 1 Lecture 1.1 Introduction to this course 1.2 Some probability vocabulary 1.3 Statistical philosophy and the foundations of Bayesian analysis 1.4 Testing JAGS installation 1.5 For more information about this week’s topic", " 1 Week 1 Lecture Papers to read this week: Christensen 2005: This paper is a little dense, but I’m more interested in your understanding the high level points being made than getting to bogged down in the mathematical details of rejection regions etc. Clark and Gelfand 2006 Stephens et al. 2007 1.1 Introduction to this course Before launching into Bayesian stats, take a few minutes and go over the syllabus. The structure of this course is very similar to Biometry, with one “lecture” and one “lab” a week and one problem set each week. The problem sets will be due before class on Monday, starting in Week 2. As always, you are encouraged to work in groups, but you are responsible for the final content of your assignment and identical (or virtually identical) problem sets or scripts will be considered plagiarism. Break to look over the syllabus. Any questions? Send me an e-mail… Note that the readings will come primarily from two textbooks (McCarthy, and Hobbs and Hooten). McCarthy presents the code in terms of the language/software WinBUGS. Instead of using WinBUGS, we will use JAGS. The good news is that WinBUGS and JAGS syntax is nearly identically the same, so don’t let this confuse you when reading McCarthy. (McCarthy’s presentation is very brief and to the point, and side steps a lot of the statistical theory. Hobbs and Hooten does a more complete job with respect to theory, though neither books gets into some of the details of sampling algorithms that we will cover towards the middle of the course.) Today we will go over some basics, including a review of null hypothesis significance testing and the differences between NHST and Bayesian approaches. On Wednesday, you will work through a simple lab to make sure that everyone has JAGS/RJAGS up and running. 1.2 Some probability vocabulary In Biometry we didn’t spend a lot of time on multivariate distributions (distributions that describe the joint probability of more than one stochastic variable), but these are critical to Bayesian analyses and so we will spend some time playing catch up this week. We will spend some time playing around with joint distributions in a second, but for now we’ll just introduce some vocabulary and some mathematical identities. Let’s say we have a bivariate distribution for discrete quantities such as hair color and eye color, and we survey a number (n=20 in this case) of students. Brown Blond Red Blue eyes 3 4 0 Brown eyes 7 2 0 Green eyes 2 1 1 This table summarizes the joint distribution for hair color and eye color, which we would write as \\[ P(hair,eye) \\] Remember that for any two traits A and B that are independent, \\[ P(A,B) = P(A) \\times P(B) \\] However, in this case, we don’t have any reason to believe that hair and eye color are independent traits. People with blue eyes have a different probability of having brown hair than people with brown eyes. These are called conditional probabilities. For example, the probability of having blue eyes conditional on having blond hair is given by 4/7. We write this as follows \\[ P(eyes=blue|hair=blond) \\] The | symbol represents the “conditional on” statement. We might also be interested in the marginal probabilities, which are those probabilities representing hair color irrespective of eye color, or eye color irrespective of hair color. In the example given, the marginal probability of having blue eyes is 7/20. The marginal probability of having blond hair is also 7/20. These are univariate probabilities, and are written as \\[ P(eye) \\] or \\[ P(hair) \\] The relationship between joint, marginal, and conditional distributions can be seen in the following statement \\[ P(\\mbox{eyes}=\\mbox{blue},\\mbox{hair}=\\mbox{blond})=P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{blond})P(\\mbox{hair}=\\mbox{blond}) \\] We can see that this works out as it should \\[ \\frac{4}{20}=\\frac{4}{7} \\times \\frac{7}{20} \\] If we want to know the probability of having blue eyes (and didn’t care about hair color) than we would want to add up all the possibilities: \\[ P(\\mbox{eyes}=\\mbox{blue})=P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{blond})P(\\mbox{hair}=\\mbox{blond}) \\\\ +P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{brown})P(\\mbox{hair}=\\mbox{brown}) \\\\ +P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\mbox{red})P(\\mbox{hair}=\\mbox{red}) \\] We can state this more simply as a sum: \\[ P(\\mbox{eyes}=\\mbox{blue})=\\sum_{\\mbox{all hair colors}}P(\\mbox{eyes}=\\mbox{blue}|\\mbox{hair}=\\square)P(\\mbox{hair}=\\square) \\] For continuous distributions, the same principles apply. Let’s say we have a continuous bivariate distribution \\(p(A,B)\\). The marginal distribution for A can be calculated by integrating over B (we call this “marginalizing out” or “marginalizing over” B) \\[ p(A=a)=\\int_{b=-\\infty}^{b=\\infty}p(A=a|B=b)p(B=b)db = \\int_{b=-\\infty}^{b=\\infty}p(A=a,B=b)db \\] Notice that, in all cases (discrete or continuous), the relationships among marginal, conditional, and joint distribution can written as \\[ p(A|B)\\times p(B)=p(A,B) \\] or as \\[ p(B|A)\\times p(A)=p(A,B) \\] Therefore, \\[ p(A|B)=\\frac{p(B|A)p(A)}{p(B)} = \\frac{p(A,B)}{p(B)} \\] Ta da! We’ve arrived at Bayes Theorem, using nothing more than some basic definitions of probability. (You’d think we’d be done for the semester, but it will take us the better part of the next 15 weeks to figure out how to actually use Bayes Theorem to do anything interesting.) Now that we’ve covered some of the basic terminology, let’s go back and look at a bit of the history about how we learn from data, since this history influences some of the current day debates about Bayesian inference and its role in science. 1.3 Statistical philosophy and the foundations of Bayesian analysis A quick review of Fisher, Neyman-Pearson, Bayes, and Popper The current statistical methodology common in the fields of ecology, evolution, anthropology etc. is a hybrid mash-up of several different paradigms developed in the early 20th century. Christensen (2005) provides an excellent summary, which you have already read; I will only sketch the main ideas here. Bayesian thinking is not a new idea – it was originally introduced by Thomas Bayes in 1763, and thus predates by a couple of centuries the developments of Fisher and Neyman and Pearson and Popper. Go ahead and read Aho Chapter 1 again for more of the historical background. There are multiple ways to frame the hypothesis testing / statistical inference endeavour. But going through them will make more sense if we take a minute to think of the kinds of hypotheses we are actually interested in testing. EXERCISE: Lets spend a few minutes writing down a hypothesis you will actually be trying to test in your thesis research. This involves two stages. (1) You need to think about a scientific hypothesis (biological, physical, political, etc.) that you would like to test. This is something you could say in words. (2) You need to find a way to translate that into a statistical hypothesis. In other words, at some level, your hypothesis has to boil down to one (or a few) parameters that can be measured and compared to some null hypothesis. What is that parameter? What is its value under the ‘null’ hypothesis? Alright, now that we have something in mind, lets plow ahead with the four primary ways of testing a statistical hypothesis: ‘Fisherian testing’ involves a single hypothesis (the null hypothesis) which can be rejected by contradiction. In other words, if your data would be unlikely to arise from your null model, you’d be inclined to reject your null model. In fact, the threshold for “unlikely” includes not only the result you got but also anything more extreme than the result obtained. How unlikely it would have to be to get something as or more extreme that the observed data is the cutoff , typically but rather arbitrarily set at 0.05. Note that rejecting the null hypothesis under Fisherian logic does not tell you why the null hypothesis was rejected. As noted by Christensen (2005), it could be because one of the parameter values was wrong, or because the data were not independent, or because the form of the distribution was wrong, and any of a long list of things. Also, note that in Fisherian testing, there is no concern about Type I error rates (that is, the probability of failing to reject the null hypothesis when the null hypothesis was false). Neyman-Pearson Tests explicitly involve both a null hypothesis and an alternative hypothesis. We have some intuition about how to choose a null hypothesis (‘null hypothesis is the dull hypothesis’) but how would we go about defining an explicit alternative hypothesis? This is where expected effect size comes in. If you have a theory about the expected size of the effect, this would be a logical alternative hypothesis. In Neyman-Pearson testing, we define a rejection region for the test statistic based (in some form or another) on its likelihood ratio, and starting from the largest likelihood ratio \\(Lik_{HA}/Lik_{H0}\\), selecting outcomes until the total probability of rejecting the null when it is true reaches some threshold \\(\\alpha\\). Those outcomes are now part of the rejection region, because they have a total Type I error rate of \\(\\alpha\\) and contain all those values in which \\(H_{A}\\) is much more likely than \\(H_{0}\\). The main difference between Fisherian testing and NP-testing is that Fisher may reject both hypotheses as inconsistent with the data, whereas NP will decide the better of two alternatives (without any real indication of the fact that both are poor representations of the data). Fisher and NP answer different questions: The former asks “Is this model any good”, whereas the latter asks “Of these two models, which is better”. Karl Popper and the use of hypothetico-deductive reasoning represents a mash-up of Fisherian and N-P testing. The basic idea is that you generate a hypothesis, you then say “if that hypothesis is true then we make the following predictions”, and you then do an experiment to test those predictions. If the experiment yields an outcome consistent with the prediction, than the model “lives to die another day”. However, if the experiment yields an outcome that is inconsistent with the prediction, than you can reject the original hypothesis and start back at the beginning (generating a new hypothesis). Popper’s hypothetico-deductive logic combines deductive and inductive reasoning by stating, in essence, that “if it predicts (deductive), than it explains (inductive)”. Bayesian approaches do not involve hypotheses at all (at least not explicitly; you may interpret the results in the context of a hypothesis). Bayesian methods provide a way to define a probability distribution for the parameter of interest. The probability distribution is called the posterior distribution, and it is a combination of your prior expectation for the parameter and the likelihood of the data. Since we will spend the whole semester focusing on Bayesian models, I won’t get into any more detail here. EXERCISE: Now that we’ve had some time to go through these, can you reframe your hypothesis in each of these paradigms? Spend a few minutes to rewrite your key question these four different ways. How does this relate to maximum likelihood estimation? The important take-home message is that Bayesian thinking involves an additional element of randomness that we don’t have with frequentist (a.k.a. traditional) methods. To really understand why this is, we need to review the idea of a likelihood, and how in frequentist statistics we use that likelihood to make inference about a parameter. Specifically, lets say we want to model the mean length of fish in a certain pond. \\[ Y \\sim N(\\mu, \\sigma^{2}) \\] where \\(\\mu\\) is the expected length of these fish. Frequentist methods assume \\(\\mu\\) and \\(\\sigma\\) are fixed but unknown. Our job is to reverse engineer what \\(\\mu\\) and \\(\\sigma\\) must have been in order for our data to be “a typical outcome”. As a reminder, the probability density of the normal distribution is given by \\[ P(X|\\mu,\\sigma) \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}} \\] Remember that for variables that are i.i.d., the joint probability (\\(X_{1}\\),\\(X_{2}\\),\\(X_{3}\\)) is simply the product of the three p.d.f.s \\[ P(X_{1}\\cap X_{2} \\cap X_{3})=P(X_{1})\\times P(X_{2})\\times P(X_{3}) \\] Therefore, by extension, for \\(n\\) data points \\[ P(X_{1},X_{2},...,X_{n}|\\mu,\\sigma)= \\prod^{n}_{i=1}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{\\sigma^{2}}\\right)} \\] Taken as a probability density, this equation denotes the probability of getting unknown data \\({X_{1},X_{2},...,X_{n}}\\) given (|) the known distribution parameters \\(\\mu\\) and \\(\\sigma\\). However, it can be rewritten as a likelihood simply by reversing the conditionality: \\[ L(\\mu,\\sigma|X_{1},X_{2},...,X_{n}) = \\prod^{n}_{i=1}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp{\\left(-\\frac{1}{2}\\frac{(X_{i}-\\mu)^{2}}{\\sigma^{2}}\\right)} \\] The likelihood specifies the probability of obtaining the known data \\({X_{1},X_{2},...,X_{n}}\\) by a certain combination of the unknown parameters \\(\\mu\\) and \\(\\sigma\\). pdf: parameters known, data varies likelihood: data known, parameters vary In this way, the relationship between the joint probability density and the likelihood function is a bit like the relationship between the young woman and the old maid in this famous optical illusion: Figure 1.1: Optical illusion known as “My Wife and my Mother-in-Law”. Source: Wikimedia Commons Parameter estimates may be found by maximum likelihood simply by finding those parameters that make your data most likely (among all possible data sets). [Time for an analogy: Anyone seen CSI: Miami? Where they find the weapon for the murder by shooting test rounds and finding the gun that leaves the marks on the bullet? That’s what we’re doing here in a way.] Hobbs and Hooten do a really nice job in Sections 4.1 and 4.2 walking through the relationship between the probability density function and the likelihood function. Remember the verbal gymnastics we went through in Biometry when we talked about confidence intervals? For example, we emphasized that a 95th percentile confidence interval represented an interval (whose upper and lower limits were statistical quantities generated from the empirical data) that we were 95\\(\\%\\) certain contained the true value of \\(\\mu\\). The statistical probability here was placed entirely on the interval itself, something we created, because the parameter was considered fixed and therefore could not possibly have some element of chance associated with it. [STOP: Let’s make sure we really are on the same page here. This is important stuff!] As frequentists, we assume a single “true” value for the parameters \\(\\mu\\) and \\(\\sigma^{2}\\). Bayesians would argue that \\(\\mu\\) and \\(\\sigma^{2}\\) themselves have a probability distribution, so we have some finite probability that \\(\\mu\\) is, say, 1.2 and another probability that \\(\\mu\\) is 1.4. THEN, conditional on that value of \\(\\mu\\), we have some probability of getting the data (the fish lengths) that were actually obtained. Bayesian thinking, which explicitly defines a statistical distribution for the unknown parameters, frees us from the horrible frequentist awkwardness on confidence intervals. We actually get what we always wanted all along but could never actually get with frequentist logic: a probability distribution for the parameters of interest. 1.4 Testing JAGS installation While there are several options for fitting Bayesian models, including writing your own custom samplers, in this class we will use JAGS, which stands for Just Another Gibbs Sampler. We’re not really going to get into using JAGS for fitting models until lab, I want to make sure everyone has JAGS installed properly. Once you have installed JAGS, we can make sure its working by running the R code in ‘Week 1 JAGS test.R’. This “wrapper” code gets the model variables set up and organized, and passes it to the actual JAGS code in the file ‘Week1_TestModel.jags’. Make sure that the .jags file is in the Working Directory so the wrapper file can find it. 1.5 For more information about this week’s topic Flam NTY article Aho’s Chapter 1 "],
["week-1-lab.html", "2 Week 1 Lab", " 2 Week 1 Lab First, a warm up: You were given some graph paper, which we are going to use to practice some of the ideas from this week in cases where we don’t have any parametric equations to deal with. I want you to put 100 points randomly on the grid. Now that you have your dataset (which will be unique to you), I want you to answer the following questions about your empirical dataset (not the average dataset you might have obtained). What is P(X=3)? What is P(Y=1)? What is P(X=3,Y=1)? What is P(X=3|Y=1)? Which of these questions are asking about the marginal probability? Joint probability? Conditional probability? Why might it be that \\(P(X=3,Y=1) \\neq P(X=3) \\times P(Y=1)\\)? Exercise #1: OK, now that we’re limbered up in the ways of joint, marginal, and conditional probabilities, let’s say you have two independent variables X and Y with the following probability distributions \\[ g(X)=\\frac{2}{X^{3}} \\space \\space \\space \\mbox{ for } 1\\leq X \\leq \\infty \\] and \\[ h(Y)=\\frac{3}{Y^{4}} \\space \\space \\space \\mbox{ for } 1\\leq Y \\leq \\infty \\] What is the joint probability distribution of (X,Y)? Show that \\(P(X\\leq Y) = 2/5\\). Exercise #2: Next we’ll work through some more complex cases, all in the name of trying to give you some intuition on how these multivariate distributions work. In each of the following three cases, find the (1) joint distribution \\(P(X,Y)\\), the two marginal distributions \\(P(X)\\) and \\(P(Y)\\) (2 and 3), and (4) determine if X and Y are independent. Case #1: (X,Y) are uniformly distributed on the square \\[ -6 \\leq X \\leq 6 \\\\ -6 \\leq Y \\leq 6 \\] Case #2: (X,Y) are uniformly distributed on the triangle \\[ Y \\leq X \\leq 6 \\\\ -6 \\leq Y \\leq X \\] Case #3: (X,Y) are uniformly distributed on the circle \\[ X^2+Y^2 \\leq 36 \\] Once you have worked this out analytically, we’ll try sampling from these distributions, and plotting the marginal probabilities. How you might sample from these distributions? Why would it be wrong to sample from the uniform for X and then sample from the conditional for Y? Write a function to sample from these three joint distributions, and draw 1000 values from the joint distribution. Plot the values, and put the marginal histograms on the X and Y axes. There are many ways to include the marginal histograms. I did a quick google search and found some code that would work (which uses the R package ‘ggExtra’). There is probably a better way; I’ll be interested to see what you all come up with. df1 &lt;- data.frame(x = rnorm(500, 50, 10), y = runif(500, 0, 50)) p1 &lt;- ggplot(df1, aes(x, y)) + geom_point() + theme_bw() ggMarginal(p1, type = &quot;histogram&quot;) "],
["week-2-lecture.html", "3 Week 2 Lecture 3.1 Bayes Theorem and all that follows from it 3.2 How do we interpret the posteriors? 3.3 A slight detour, to get us thinking about the basic philosophy behind Bayesian stats 3.4 Getting some more practice with JAGS 3.5 For more information about this week’s topic", " 3 Week 2 Lecture Papers to read this week: Ellison 1996 Ellison 2004: More advanced material that may take a few weeks to sink in Berger and Berry 1988: This is a repeat from Biometry but worth refreshing even if you took Biometry recently and required reading for those that did not take Biometry. This week we will review the basic elements of Bayesian inference, building on our introduction to Bayes Theorem last week. Everyone was asked to watch this video before lecture today. I think this video does a particularly nice job explaining the basic ideas underlying Bayesian inference, and having that intuition under your belt will help when we get into the math. At the end of this video, you got a brief introduction to Approximate Bayesian Computation. I’ll introduce a few highlights here, since doing so makes Bayesian analysis very intuitive, but we’ll reserve a more complete discussion for Week #15. 3.1 Bayes Theorem and all that follows from it We start with Bayes theorem (here I am following the notation of Ellison [1996]): \\[ P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)} \\] or, as it is more commonly written, \\[ P(\\theta|x) \\propto P(x|\\theta)P(\\theta) \\] In other words, the posterior probability distribution for the parameter \\(\\theta\\) conditional on the data \\(x\\) is proportional to the likelihood P(x|) times the prior distribution \\(P(\\theta)\\). We can think about this (vis a vis the video we just saw) as the likelihood “filtering out” the prior distribution. In other words, we have some distribution that describes our prior understanding of the parameter, and we use the data to sift through which values make more or less sense. Values that make the data more likely are themselves more likely, and vice versa. Next week we will focus on prior distributions and their interpretation, so we won’t say much more about prior distributions at this stage. We will however work through the algebra of Bayes Theorem to see how we obtain a posterior distribution. Let’s say we have one observation \\(y\\) from a Normal distribution of unknown mean \\(\\mu\\) and known variance \\(\\sigma^{2}\\). The likelihood of obtaining that observation is simply: \\[ P(y|\\mu) \\propto exp\\left(\\frac{-(y-\\mu)^{2}}{2\\sigma^2}\\right) \\] where I have included only the terms involving the unknown parameter \\(\\mu\\). (Side note on notation. By tradition, both probability densities and likelihoods have the data on the left side and the parameters on the right side on the conditional “|”. This is not how I originally introduced them last week, because in Week #1’s lecture I was trying to highlight the differences between PDFs and likelihoods, which involves what is known and what is unknown. However, to be consistent with the book and papers on the subject, here I revert back to the traditional notation where the data “y” is on the left hand side. Also, I use the letters \\(x\\) and \\(y\\) interchangeably so my choice of one or the other to represent data is arbitrary.) Let’s assume a fairly broad (i.e. large variance) Normal distribution prior for \\(\\mu\\), \\(N(\\mu_{0},\\tau^{2})\\), so that: \\[ P(\\mu) \\propto exp\\left(\\frac{-(\\mu-\\mu_{0})^{2}}{2\\tau^2}\\right) \\] Make sure this distribution makes sense. In this case the distribution is on \\(\\mu\\) and its parameters are \\(\\mu_{0}\\) and \\(\\tau\\). The posterior distribution is now given by the product: \\[ P(\\mu|y) \\propto exp\\left(\\frac{-(y-\\mu)^{2}}{2\\sigma^2}\\right)exp\\left(\\frac{-(\\mu-\\mu_{0})^{2}}{2\\tau^2}\\right) \\] \\[ P(\\mu|y) \\propto exp\\left(-\\frac{1}{2}\\left(\\frac{(y-\\mu)^{2}}{\\sigma^2}+\\frac{(\\mu-\\mu_{0})^{2}}{\\tau^2}\\right)\\right) \\] I’ll spare you the algebra, but this can be simplified as: \\[ P(\\mu|y) \\propto exp\\left(-\\frac{1}{2}\\frac{(\\mu-V\\nu)^{2}}{V}\\right) \\] where, \\[ \\frac{1}{V} = \\frac{1}{\\sigma^2}+\\frac{1}{\\tau^2} \\] and, \\[ \\nu = \\frac{y}{\\sigma^2}+\\frac{\\mu_{0}}{\\tau^2} \\] While this expression looks kind of messy, in the limits it makes a lot of sense: \\[ lim_{\\tau \\rightarrow \\infty} V\\nu \\rightarrow \\frac{y/\\sigma^2}{1/\\sigma^2} = y \\] In other words, the mean of the posterior distribution is just the value of the single data point if the prior is so broad as to contribute no information to the posterior. Question: What is \\(lim_{\\tau \\rightarrow 0} V\\nu\\)? Click for Answer In this case, \\(V \\rightarrow \\tau^2\\) \\(\\nu \\rightarrow \\frac{\\mu_{0}}{\\tau^2}\\) Therefore, \\(V\\nu \\rightarrow \\mu_{0}\\). Likewise, in the \\(lim_{\\tau \\rightarrow \\infty} V \\rightarrow \\frac{1}{1/\\sigma^2} = \\sigma^2\\) which is just the known variance for the data’s distribution. (This should make some sense keeping in mind that the posterior variance represents the uncertainty on the posterior mean. If we consider the central limit theorem (CLT), we know that the standard error of the mean is given by \\(\\sigma/\\sqrt{n}\\) but in this case \\(n=1\\) so this is exactly what we would expect.) What happens if we have \\(n &gt; 1\\) independent observations? In this case the likelihood is a product: \\[ P(y|\\mu) \\propto \\prod_{i=1}^{N}exp\\left(-\\frac{(y_{i}-\\mu)^2}{2\\sigma^2}\\right) = exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_{i}-\\mu)^2\\right) \\] Working through the same algebra, we have the same basic form for the posterior distribution: \\[ P(\\mu|y) \\propto exp\\left(-\\frac{1}{2}\\frac{(\\mu-V\\nu)^2}{V}\\right) \\] except now, \\[ \\frac{1}{V} = \\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2} \\] and, \\[ \\nu = \\frac{n\\bar{y}}{\\sigma^2}+\\frac{\\mu_{0}}{\\tau^2} \\] (Why do we only keep track of the posterior up to a proportion? Since we know the posterior has to integrate to 1 [not withstanding the use of an “improper” prior, more on that next week], we can always work out the constant if we needed. In practice, however, we usually use Monte Carlo methods to draw samples from the posterior and, for reasons we will discuss in a few weeks, this only requires knowledge of the posterior up to within a proportionality constant.) You can see that as \\(n\\) gets large, the data will start to dominate the prior. In fact: \\[ lim_{n \\rightarrow \\infty} V\\nu \\rightarrow \\frac{\\frac{n\\bar{y}}{\\sigma^2}}{\\frac{n}{\\sigma^2}} = \\bar{y} \\] Once again, this is precisely what we would expect under the CLT. In general, we can see from this fairly simply case with Normal distributions that the posterior is a weighted average of the prior and the likelihood, with the weights determined by the variances \\(\\sigma^2\\) and \\(\\tau^2\\) and the sample size. Larger sample size weights the likelihood more strongly than the prior. What happens if \\(\\tau^2\\) is very very small? In this case, the prior dominates the posterior entirely, irrespective of the data. This is a good reminder that you want the prior to have reasonable support (non-negligible density) for all values that you think might be supported by the likelihood, because this will allow the posterior to shift and bend towards the likelihood as more data is used. What if the variance \\(\\sigma^2\\) was not assumed known? Then we would have to include a prior for \\(\\sigma^2\\) as well. In this case, the joint posterior distribution would be given as: \\[ P(\\mu,\\sigma|y) \\propto P(y|\\mu,\\sigma)P(\\mu)P(\\sigma) \\] If we wanted the marginal posterior distribution for \\(\\mu\\), then we would need to integrate out \\(\\sigma\\): \\[ P(\\mu|y) = \\int P(\\mu,\\sigma|y)d\\sigma \\] 3.2 How do we interpret the posteriors? Whereas frequentist methods are focused on rejecting a null hypothesis, Bayesian analyses are focused on the posterior distribution as a statement of the probability of the parameter lying within a certain interval (for a continuous parameter). Bayesian methods often (but not always) end with a statement about the mean (sometimes median) of the posterior distribution and some interval around that measure of central tendency. Its worth reminding ourselves that there are three measures of central tendancy that we might want to use to describe the posterior distribution. While the mean is perhaps most common, it is not unusual in Bayesian analyses to report instead the median or the mode; it often depends on which measure of central tendency you think is the most biologically relevant. We define a 100(1-\\(\\alpha\\))\\(\\%\\) credible interval for a parameter \\(\\theta\\) as: \\[ P(\\theta \\in B_{X}|X) = 1 - \\alpha \\] where \\(B_{X}\\) is some interval. In the discussion to follow I will assume 100(1-\\(\\alpha\\))\\(\\%\\) equals 95\\(\\%\\), but this is only because it makes the discussion more intuitive and not because there is anything magical about \\(\\alpha\\)=0.05. There are an infinite number of intervals that would contain 95\\(\\%\\) of the area under the curve (i.e. the pdf of the posterior), and so there are an infinite number of 95th percentile credible intervals (heretofore, CI). One possibility is to make sure there is 2.5\\(\\%\\) in each tail (the “central” CI). Another is to find the shortest interval that contains 95\\(\\%\\) of the probability. This latter interval is called the highest posterior density interval or HDPI. Calculation of the HDPI can be quite difficult, especially because we often lack an analytical form for the posterior, so often the central CI is used instead. Figure 1.1: Source: Doug Schluter link Note that while summary statistics are useful, it is often the case that authors will publish a histogram of the entire posterior distribution. This can be particularly valuable if the posterior is highly skew (or even multimodal). 3.3 A slight detour, to get us thinking about the basic philosophy behind Bayesian stats For lecture today you read the article by Fenton and Neil (2012) and the accompanying article from the Guardian. The basic story at hand: Convicted killer “T” appealed his conviction, which was based in part on a Bayesian analysis that found strong evidence linking his sneakers (found in his home) to a shoeprint at the crime scene. A judge threw out “T”s conviction on the basis that Bayesian statistics (specifically, expert opinion that had been used to generate a prior distribution for the number of sneakers of that type in the UK at the time) was not “firm” statistics. The court went further and basically rejected the use of Bayesian statistics in all cases except (somewhat arbitrarily, in my opinion) in the analysis of DNA evidence. Fortunately, Bayesian statisticians have come to its defense noting, for example, “The fact that variables cannot be precisely expressed does not affect the validity of the relationships described by [Bayes] formula”. A few questions that were raised in Fenton and Neil (2012): 1) When is it OK to multiply likelihoods? Why was the practice of multiplying likelihoods (or likelihood ratios, equivalently in this case) criticized in the original court case? 2) Why is a 1 in 10,000 fingerprint match not necessarily strong evidence that the accused was at the scene? (Hint: Can you ever accept the null hypothesis? What is the null hypothesis in this case? Why does it matter?) Click for Answer Figure 3.1: Source: Doug Schluter link Note that if p=\\(10^{-6}\\), then \\(P(\\mbox{guilt}|\\mbox{match})\\)=0.5. If p=\\(0.5\\), then \\(P(\\mbox{guilt}|\\mbox{match})\\)=0.999999. So, your decision about the guilt or innocence of the defendent depends a lot on your prior expectations for their guilt of innocence. Fenton and Neil raise the idea of a Bayesian network, which we will return to briefly next week. A Bayesian network is a graphical model that shows the conditional relations among variables, and is just a more visual way of understanding a Bayesian analysis with many conditional relationships. 3.4 Getting some more practice with JAGS We don’t have a formal lab this week, but we can use McCarthy’s example in Boxes 3.4 and 3.5 to practice running models in JAGS. This also gives us an opportunity to see how Bayesian models align with frequentist approaches we may be more familiar with. EXERCISE #1: Run the code from McCarthy Box 3.4, which fits the following model: \\[ \\#Trees \\sim Pois(\\mbox{mean density}) \\] for the number of trees recorded in equal sized quadrats. First, use the prior and starting values suggested by McCarthy. Then try and break JAGS – use other prior distributions and wildly different starting values. Do the starting values influence the time to convergence? (Look at the posterior draws…) EXERCISE #2: Compare the result obtained using JAGS to that obtained in a frequentist analysis using the R function ‘glm’. Are they the same? Why or why not? Does it depend on the prior? EXERCISE #3: Add the extra variation as described in Box 3.5. 10 pt BONUS (turn in with problem set): Compare the result obtained using JAGS (from Exercise #3) to that obtained in a frequentist random effects analysis using the R function ‘glmer’. (Nominally, the Poisson model with extra variation is just a random effects model.) Is your glmer model actually equivalent to the JAGS model? If its different, how so? How do you compare the output of glmer with that of JAGS? How do we even know whether they yield the same inference? How do you calculate the variance of the random effect as output by glmer? 3.5 For more information about this week’s topic Both of the articles below discuss the use of Bayesian methods in the criminal justice system: Fenton and Neil 2012 A formula for justice in \\(\\textit{The Guardian}\\) "],
["week-3-lecture.html", "4 Week 3 Lecture 4.1 How do we obtain priors? 4.2 Conjugacy 4.3 Sensitivity analysis 4.4 Expert elicitation 4.5 For more information about this week’s topic", " 4 Week 3 Lecture Papers to read this week: Kuhnert et al. 2010 Lambert et al. 2005: More advanced material Senn 2007: More advanced material Lambert et al. 2008: A response to Senn 2007 The use of prior information is one of the major distinguishing features of Bayesian analysis, and also the issue that generates the most controversy about Bayesian methods. How do we think about the relationship between the prior distribution, the data, and the posterior? We took an aside last week to discuss the ways in which Bayesian statistics is used for criminal justice proceedings, so here we will continue using criminal justice (a system we have some intuition about) to think about the role of priors. Our criminal justice system assumes all defendants are “innocent until proven guilty”. This is our prior expectation. The evidence has to overwhelmingly suggest guilt for a jury to reach a guilty verdict. (“Innocent until proven guilty” refers to the prior distribution, “proof beyond a reasonable doubt” refers to what the posterior distribution would have to look like to determine that a defendant is actually guilty.) Question: What does the prior distribution actually look like in a criminal case? (Hint: Can’t be a delta function at “innocent” – why not?) 4.1 How do we obtain priors? The vast majority of ecological analyses use non-informative priors bounded only by biological feasibility. (In other words, you might constrain lifespan to the maximum known lifespan for that species, but use a uniform distribution over that range as the prior.) In these cases, Bayesian statistics are being employed because they are simply more flexible than frequentist methods; where both analyses are possible, they will yield similar results. Technical Note: It is possible to use a prior distribution that is “improper”, i.e. doesn’t integrate to 1, and still obtain a proper posterior distribution. However, JAGS generally forces the use of proper prior distributions. 4.2 Conjugacy When the prior and posterior distributions have the same distributional form, the prior is said to be “conjugate” to the distribution for the data. For example, if you have data that are binomially distributed, and you use a beta distribution to represent your prior knowledge of the binomial probability \\(p\\), then you will get a posterior distribution for \\(p\\) that is also described by a beta distribution. We say in this case that the beta and binomial form a “conjugate pair”. NB: Note that the beta distribution is a prior distribution on the continuous (but bounded) parameter \\(p\\), and the binomial distribution is a discrete distribution representing the discrete outcome of the binomial process. In other words, the distribution used to model the data may be discrete, and the distribution to describe the parameter may be continuous. There is no conflict combining these two different types of distribution in this context. Why do we care? In the old days, before MCMC methods were made feasible by fast computers, the only way to do Bayesian analyses was to exploit the algebraic simplicity of conjugacy, because these were the only cases in which you could calculate, using pencil and paper, the posterior distribution. Now with the advent of MCMC and fast computers, we don’t need to limit ourselves to prior distributions that are conjugate to the model for the data. Even now, however, WinBUGS and JAGS will run much faster if you do use a conjugate prior, and due to the historical legacy, conjugate pairs are often the default choice even if they are no longer required. (In fact, you should use a prior distribution that most honestly reflects your prior knowledge, whatever form that might be, unless computation efficiency because a major concern for your analysis.) Sometimes, however, informative priors are used, either because they reflect genuine prior knowledge of the system that should be reflected in the analysis, or because there is so little data to work with that you need some kind of informative prior to get reasonable, useful posterior distributions. Suppose we are told that we have a fair coin, and it has come up heads \\(y=10\\) times. We do not know how many times the coin was flipped. The likelihood for the data (\\(y=10\\)) can be written as: \\[ p(y|N) = \\mbox{Binomial}(0.5,N) = \\frac{N!}{(N-y)!y!}0.5^{N-y}0.5^{y} \\propto \\frac{N!}{(N-y)!y!}0.5^{N} \\] We need a prior for N, and since N is discrete we need a discrete distribution for this prior. Jeffrey’s prior in this case would be \\(p(N) \\propto 1/N\\). (See text box for more information about Jeffrey’s prior; we will not go into much detail and for the example here, you just need to know that the prior is \\(\\propto 1/N\\).) The posterior distribution is then given by: \\[ p(N) \\propto \\frac{N!}{(N-y)!y!}0.5^{N} \\times \\frac{1}{N} = \\frac{(N-1)!}{(N-y)!}0.5^{N} \\] This is in fact (modulo constants) the pdf of the negative binomial (you can convince yourself this is true at home), which is what you would expect if the experimental design had been “keep flipping until you get y heads”. That is precisely the scenario that generates a negative binomial distribution. But, in this case, we do not need to assume anything about the manner in which the data were collected, and in fact we do not know whether the 10th head was the last coin flip which ended the “experiment”. In could have been 10 heads followed by 10 tails – in this case the experimental design does not influence the posterior distribution for N. I assigned a re-reading of Berger and Berry (1988) for lecture today because this example gets to the heart of that earlier discussion from Biometry. Remember from Berger and Berry that a frequentist approach to this same problem would require some assumption about the nature of the experiment. This comes about because frequentist p values depend not only on the likelihood of the outcome, but the likelihood of all outcomes more extreme than the one obtained. This dependence on more extreme outcomes that were not observed require that you know (or assume) something about the range of outcomes possible under the experimental design. Bayesian statistics do not hinge on unobserved outcomes, which is why it does not require any assumptions about the manner in which the data were obtained. 4.3 Sensitivity analysis At the end of every Bayesian analysis, you should do a sensitivity analysis to see how sensitive your posterior distributions are to priors. In some cases, some parameters may be highly sensitive to the choice of prior, but other parameters of more interest are not. This would be perfectly fine if you are not going to be making any biological inference on the parameters that are highly sensitive to the priors. They may be tangential to the parameters you are really focused on, which themselves may be fairly robust to various prior assumptions. However, you may have cases where the prior makes a big difference. While many data analysts fear this possibility, this is in fact, a confirmation that a Bayesian approach is worthwhile in the first place. In these cases, you simply need to justify the choice of prior and be transparent about the range of outcomes that might have been obtained with different prior assumptions. Our first exercise here will be to emphasize a caution noted in the handout from the book by Lunn et al.: that a uniform distribution is not always a “vague” or uninformative prior. You have to be very cautious when dealing with transformation of parameters. If you want a vague prior for a parameter, make sure the prior is actually what you think it is. In particular, transformations of a uniform are not uniform. This often comes up when putting a vague prior on the variance, since JAGS deals only with the precision (the inverse-variance). A uniform distribution for the precision is not what you actually want – since does not reflect the uncertainty on the scale intended (i.e. on the variance, or possibly on the standard deviation). EXERCISE #1: Convince yourself of this by looking at the distribution for \\(\\theta^2\\) if \\(\\theta \\sim \\mbox{Unif}(0,1)\\). Lunn et al. tells us that this new distribution is actually a Beta(0.5,1) – do you agree? Note that while the above example seems contrived, this exact same issue can come up quite easily in ecology. Take the following scenario: You want to model the state of a bird, and there are three possible states (1) not present, (2) present but not breeding, (3) present and breeding. We can think of this as two independent processes: \\[ \\mbox{Present} \\sim Bern(\\theta) \\] and, \\[ \\mbox{Breeding|Present} \\sim Bern(\\pi) \\] If you didn’t know anything about this bird, you would be tempted to use the following “uninformative” priors: \\[ \\theta \\sim \\mbox{Unif}(0,1) \\\\ \\pi \\sim \\mbox{Unif}(0,1) \\] However, when we put these two processes together, the prior distribution for these three states is not uninformative. Next, we are going to explore the various methods one could employ to generate a vague prior for the Binomial parameter p. This exercise is based on the discussion in Lunn et al. Section 5.2.5 but I want to convince ourselves of this in R since R is easier to work with and we can focus on the core ideas involved. Lunn et al. presents us with the following situation. You are doing a logistic regression analysis to understand the probability \\(\\theta\\) of a binomial event occurring, and you want to put a flat prior on the parameter \\(\\theta\\). But here we have several options, because we can use a vague prior for the probability p or we could use a vague prior for the logit(\\(\\theta\\)). In addition, there are several vague priors we might choose. Lunn’s notation is as follows (the “whatever” could be any linear function, its not relevant here): \\[ Y \\sim \\mbox{Binomial}(\\theta) \\\\ \\phi \\sim \\mbox{logit}(\\theta) = \\mbox{whatever} \\] Lunn et al. lays out the options as follows (I have crossed out #3 since we won’t get into Jeffrey’s priors but have left it so that the numbering is consistent with Lunn’s figure): Use a uniform on \\(\\theta\\), let \\(\\phi\\) be determined accordingly Use a uniform on \\(\\phi\\), let \\(\\theta\\) be determined accordingly (3) Use a Jeffrey’s prior on \\(\\theta\\), let \\(\\phi\\) be determined accordingly Use a N(mean=0,precision=0.5)=N(mean=0,var=2) prior on \\(\\phi\\), let \\(\\theta\\) be determined accordingly Use a N(mean=0,precision=0.368)=N(mean=0,var=2.71) prior on \\(\\phi\\), let \\(\\theta\\) be determined accordingly EXERCISE #2: Plot these alternatives in R to recreate Lunn et al. Figure 5.1. What are the pros and cons of using each of these options. Is there any practical benefit to Option #1 over #5 (or vice versa)? 4.4 Expert elicitation One method of obtaining prior distributions is through expert elicitation. The Delphi method is one way of eliciting expert judgment. Delphi method: 1) Elicit information from each expert independently Collect results and share with the group Experts reconsider their responses in light of the responses of others Elicitation and feedback cycle continues We will walk through an example of this in lab. 4.5 For more information about this week’s topic We do not go into detail on Jeffrey’s prior in this course because we have limited time and I don’t see Jeffrey’s priors used very often in practical ecological analyses. However, here are some references that may be useful if you would like more information. On Jeffrey’s Priors: Notes from lectures at Berkeley and Duke. Other papers: Daniels 1999 A chapter from Lunn et al. 2012 \\(\\textit{A BUGS Book: A Practical Introduction to Bayesian Analysis}\\) Lunn et al. Chapter 5 "],
["week-3-lab.html", "5 Week 3 Lab 5.1 Congugacy 5.2 Moment Matching two distributions 5.3 From Prior to Posterior to Prior 5.4 Adding data: One at a time or all at once? 5.5 What impact did the choice of prior have?", " 5 Week 3 Lab This lab has three objectives: To teach you the basics of the Delphi method of expert elicitation To provide some practice using Moment Matching To provide some intuition for how a prior distribution is combined with data to create a posterior distribution, and how that posterior distribution can be used as a prior distribution for the analysis of additional data You should all have a bag of Skittles. In this lab, we’ll walk through the Delphi method to establish a priori probability on the number of Skittles per bag. (Or, more precisely, a prior probability on the Poisson parameter that governs the expected number of Skittles per bag.) The details of the Delphi method aren’t important, but whether you are asking a group of experts or just yourself, prior distributions are often derived using informed but ultimately subjective a priori knowledge about a system. Whether this is a “feature”&quot; or a “bug” of Bayesian analysis is something we can discuss. Before we get started, let’s think about why we might use the Poisson distribution to describe the number of Skittles in a bag. Is the Poisson distribution the best one to use for Skittles? Do we expect they will be under- or overdispersed? What are our other options? In this case, we’ll use the Poisson for convenience, but keep in mind that in an actual analysis, the choice of distribution requires careful consideration. EXERCISE: Draw on a piece of paper a “uninformative” probability distribution for the parameter \\(\\lambda\\) associated with the Poisson distribution. Is this prior distribution a proper probability distribution? (What are the requirements for a proper probability distribution function?) What distributions might look “close enough” to your “ideal” uninformative prior? Using R, play around with the Gamma distribution to get a distribution you think is “close enough” and write down the parameter values a and b. We will need these later in our lab. 5.1 Congugacy With modern computers, we often don’t worry too much about using a prior distribution that is “congugate” to the likelihood distribution. However, for this lab, we’ll use the congugate distribution because it is convenient and it will help us visualize the effect of adding more data. As you will prove for the problem set, the Gamma distribution is conjugate to the Poisson distribution. In other words, ff the prior distribution is \\[ \\theta \\sim Gamma(a,b) \\] and you have data with \\(n\\) counts with an average of \\(\\bar{y}\\), then the posterior distribution will be \\[ \\theta|\\bar{y} \\sim Gamma(a+n\\bar{y},b+n) \\] Keep in mind that this is equivalent to \\[ \\theta|\\bar{y} \\sim Gamma(a+\\sum_{i=1}^{n} y_{i},b+n) \\] Since a Gamma prior will be easy to work with, we would like to elicit “expert” opinion on \\(\\lambda\\) in the form of the gamma distribution. There are multiple methods of this (Lunn et al. introduces the idea of a “pre-prior”) but I like the moment-matching method best. In other words, you will each decide what you think is the expected value of \\(\\lambda\\) with a measure of the standard error of your estimate. We can then ask “What would the Gamma distribution parameters a and b have to be to get a distribution (for \\(\\lambda\\)) with that mean and that variance”? (This is the essence behind matching moments of distributions.) 5.2 Moment Matching two distributions We’ll illustrate the process of moment matching with an example. Look at the handout DistributionCheatSheet.pdf kindly provided by the instructor of the SESYNC Bayesian course and look down to the row on the Gamma distribution. The mean and variance of the Gamma distribution is related to the two parameters a and b as follows \\[ E[X] = \\frac{a}{b} \\] \\[ Var[X] = \\frac{a}{b^{2}} \\] We can use these two expressions to solve for the a and b associated with a distribution that has mean \\(\\mu\\) and variance \\(\\sigma^{2}\\) \\[ a = \\frac{\\mu^{2}}{\\sigma^{2}} \\] \\[ b = \\frac{\\mu}{\\sigma^{2}} \\] That’s it! So when you think of a distribution that you think describes the number of Skittles in a bag, you can take the mean and variance of that distribution and work out the associated Gamma parameters a and b. 5.3 From Prior to Posterior to Prior Since we have some intuition for the moments of the distribution, and not the parameters of the gamma distribution, we will use our expert opinion to get a prior distribution for \\(\\lambda\\) (and our uncertainty about the estimate). When we have arrived at a consensus about this, we will work out the parameters of the corresponding gamma distribution using moment matching. To make this a little more manageable, we’ll break into groups of 4 or 5 for the Delphi Method. Step 1: Without discussing it with the others in your group, write down on a piece of paper how many skittles you think are in a bag of skittles. This is your \\(\\mu\\). Now write down what you think your uncertainty about this number is. In other words, if you think there are 50 skittles in a bag, but it could be anywhere from 30 to 70, than \\(\\mu=50\\), and \\(2\\sigma = 20\\), so your estimate of the variance \\(\\sigma^{2}=100\\). Make sure this makes sense. (The 2 is \\(\\sim\\) 1.96, this is all sort of “back of the envelope”.) Step 2: Discuss your guesses with each other, explain why you think the number is what you think it is, etc. Step 3: Now, having shared your estimates and discussed them, write down your independent assessment of \\(\\mu\\) and \\(\\sigma^{2}\\). Step 4: Discuss again! We could continue this cycle indefinitely, but in the interest of time, come to some concensus for \\(\\mu\\) and \\(\\sigma^{2}\\). You can always just average for estimates together. This will be your prior for \\(\\lambda\\). But, remember, we need to use moment matching to get this into a prior estimate for a and b. Work through the math so your group has a single prior estimate for a and b in the Gamma distribution. 5.4 Adding data: One at a time or all at once? In frequentist analyses, we worry about “looking at the data” before the data has been fully collected for fear that our preliminary analysis of the data may inform or change our pre-determined experiemental design. Doing so can cause issues with multiple comparisons and inflated Type I error rates, as discussed in Biometry. However, as we will see here, Bayesian analyses are insensitive to this issue, and the data can be added piecewise or all at once. Now everyone should open up their bags of Skittles and count the number inside. Decide the order in which these counts will be “used” in the analysis. Using one data point, calculate the posterior distribution for \\(\\lambda\\). Because the Gamma is congugate to the Poisson, this is easy to do (see above). This posterior distribution will be our new prior distribution. Now include the number of skittle in the second bag as a new data point, turning this new prior (one data point) into a new posterior (2 data points). Working our way through the bags, we update each prior into a posterior, which is then used as the next prior. What is your final posterior distribution? Now redo the calculation treating all the counts as a single dataset (n=4 if you have 4 people in your group). What is your final posterior distribution now? The following code (an example from last year) can be modified and pasted into R to plot your prior and your slowly changing posterior as data are added a &lt;- 28.4 # prior on a b &lt;- 1.78 #prior on b prior=dgamma(x = 8:22,shape = a,rate = b) plot(prior, x=8:22,ylim=c(0,0.42),pch=16,typ=&quot;b&quot;) #plots your prior data=c(14,13,14,16,16,15,14,14,15,15,14,15,17,14) # put your data in here for(i in 1:length(data)){ n= i y = data[1:i] posterior = dgamma(x = 8:22,shape = a+n*mean(y),rate = b + n) lines(posterior,x=8:22,col=rainbow(15)[i]) } 5.5 What impact did the choice of prior have? The choice of a prior distribution can impact your posterior distribution, especially when you have small datasets. This is precisely what gives many frequentists heartburn about Bayesian analysis. Compare the posterior you got using your Delphi method to derive a prior disytribution with the posterior distribution you would get had you used your “close enough to uninformative” prior you generated earlier in the lab. Are these posteriors different? By a little? By a lot? Which is the better method? Do the different varieties of Skittles have different numbers in each bag? (a.k.a. dipping our toe in the water of Bayesian inference) Now we will have a posterior distribution for the number of Skittles for each variety. We can now ask whether these are “statistically significantly” different? Is that a question we even really ask in a Bayesian analysis? The place to get started here is to create a derived quantity representing the difference in number between two varieties. Usually, we will do this in the JAGS model itself, so that we calculate the difference immediately after the sample for each variety. This is one of the nicest features of Bayesian modeling (made easy with JAGS), which is that anything downstream of the model we might be interested in, we can compute along the way. If we are interested in differences, than we should store the differences in a new variable and when the sampling is complete we have a posterior for that quantity of interest. In this case, we are not doing any MCMC sampling because we have used conjugate priors and therefore have an analytical expression for the posterior. In this case we can simply sample from each of the two posteriors and subtract them to create a posterior for the difference. In class we will discuss the ways in which a Bayesian might intepret that posterior for the difference, i.e. how do we actually do Bayesian inference once the sampling is all done. "],
["week-4-lecture.html", "6 Week 4 Lecture 6.1 Rejection Sampling 6.2 Adaptive Rejection Sampling 6.3 Monte Carlo Integration 6.4 Sometimes you just want the integral… 6.5 Importance Sampling 6.6 Sampling Importance Resampling", " 6 Week 4 Lecture Papers to read this week: Lunn et al. Section 1.4 Robert and Casella Lecture Notes: For the purpose of this week, focus on pages 51-79 but these lecture notes are excellent and cover a lot of material and are worth skimming through in their entirety in case there are other sections of interest. Smith and Gelfand 1992: Important: This paper has a typo, an important one. See if you can find it. (The paper is such an important one, and so nicely explains the underlying principles, so we read this paper anyways…) In any case, we’ll discuss it in class. In Week #4 and #5, we take a detour from Bayesian stats itself to discuss a more general concept, which is Monte Carlo methods. Monte Carlo methods are named for the famous gambling city, which remind us that stochasticity is really the only requirement for something to be considered “Monte Carlo”. Monte Carlo methods, and the idea behind Markov Chain Monte Carlo (MCMC), are much more general than just Bayesian statistics and, in fact, arise in frequentist statistics as well. I’ll introduce Monte Carlo methods using three common applications: Rejection sampling Monte Carlo Integration Importance Sampling Next week, we will tackle Markov Chain Monte Carlo Methods, such as Gibbs Sampling and Metropolis-Hastings. (Markov Chain Monte Carlo is a special case of Monte Carlo in which the next sample from the distribution depends only on the last sample [or, in more complex cases, on a finite number of previous samples]. I will tend to use the term “MCMC” when talking about these methods as applied to Bayesian statistics, because most of the methods used in Bayesian statistics have the Markov property, but keep in mind that MCMC is really a special case of a much more general concept of “Monte Carlo”.) Why are we talking about MC methods? Why do we need them in Bayesian statistics? - because the posterior distribution may have no straightforward analytical form! In most cases, we have no way of either writing down the posterior PDF or of sampling from it. MC methods (of which there are many) provide a mechanism to sample from the posterior distribution even if we cannot write it down. There are several methods we will discuss. Which ones you actually use in each situation depend on your needs for efficiency vs. simplicity. JAGS will automatically choose the best method, but a proper understanding of Bayesian statistics requires a thorough understanding of MC (even though, sadly, most ecologists use JAGS without this background…) 6.1 Rejection Sampling The idea behind rejection sampling is pretty straightforward and best illustrated with a geometric example. Assume for a moment that you want to sample points within the unit circle (at right) but you only have a machine that can sample from the Uniform(0,1) distribution. What do you do? You could sample X and Y from the Unif(0,1), which will give you a random assortment of locations (X,Y) within the white square. For each pair of points (defining a location \\([x_i, y_i]\\)) you can test whether it falls inside the unit circle (by checking that ) or outside. If you simply reject all pairs \\((x,y)\\) that fall outside the unit circle, you are left with a random sample of points within the unit circle. (This is probably what you did to solve the Week #1 Problem Set.) This is just as good as having a function to sample from the unit circle directly except now it is less efficient because you have had to draw more points than you needed (because some were rejected). In this case, the loss of efficiency was rather trivial, but you can imagine having drawn X and Y from Unif(-1000,1000) and using rejection sampling to get points within the unit circle. In this case, the loss of efficiency would be rather extreme. Figure 1.1: Probability is uniform within the unit circle. Now that we have the basic picture, we can see how this might apply to a real problem. Rejection sampling is based on the idea that you may not be able to draw from the distribution you really want, but you can sample from a distribution that includes (in a statistical sense) the distribution you want and reject samples accordingly. Assume \\(p(x)\\) is the distribution you would like to draw from – we call this the “target distribution”. The basic idea is that you take a distribution \\(q(x)\\) which you can sample from (we call this the “candidate distribution” because it generate candidates for the accept-reject part), and you scale it by some number M so that you guarantee that \\(M*q(x)\\) is always greater than or equal to \\(p(x)\\). What is M? You want M to be only as large as it needs to be (Why?), so we calculate M as \\[ M = sup_{x}\\left(\\frac{p(x)}{q(x)}\\right) \\] or, to look at it another way \\[ 1 = sup_{x}\\left(\\frac{p(x)}{M*q(x)}\\right) \\] In words, this simply says that the largest you would want \\(p(x)/M*q(x)\\) to be is 1. OK, so now let’s assume that you have figured out what \\(M\\) needs to be. The pseudocode lays out the basic algorithm to draw N samples from the target distribution: Note that the draw from the uniform is just a mechanism for accepting values from the target distribution with probability \\(p(x)/M*q(x)\\). If it makes more sense, you could use a draw from the Bernoulli instead, i.e. \\[ x^{(i)} \\sim q(x) \\\\ \\mbox{if rBinom} \\left(1,\\frac{p(x)}{M*q(x)}\\right) \\] The analogy I might use is that of carving out a sandcastle from a pile of sand. The first task is to pile up enough sand that the pile is higher than the tallest part of the castle, and then the second task is to carve away at the sand until you get the shape you want. Rejection sampling is just carving away at the big shapeless pile of sand to get the distribution you wanted in the first place. We will write some code in lab to actually practice doing this. 6.2 Adaptive Rejection Sampling I won’t say much about adaptive rejections sampling, except to say that it tunes the candidate distribution over time to increase the acceptance ratio and speed up the sampling. 6.3 Monte Carlo Integration The idea behind Monte Carlo integration is very simple. Let’s say you have a probability distribution f(x) and you want to know the E[X]. If you knew the pdf analytically, you could simply calculate the expectation as follows: \\[ E[X] = \\int_{-\\infty}^{\\infty} xf(x)dx \\] But what do you do if you don’t know the equation for \\(f(x)\\) but you do have some way of sampling from \\(f(x)\\)? (in other words, some black box method for generating random draws \\({x_{1},x_{2},x_{3},...,x_{T}}\\), but no idea what’s in the black box…) In this case, you can estimate the expectation by \\[ E[X] \\approx \\frac{1}{T}\\sum_{t=1}^{T}x_{t} \\] Remember that the expected value E[X] is simply the value you would expect if you sampled from \\(f(x)\\). The expected value is just the mean of all values from \\(f(x)\\), in which case you can simply use the draws that you have in lieu of having the full pdf describing \\(f(x)\\). This can be extended for an arbitrarily complex function \\(g(x)\\), so that \\[ E[g(X)] = \\int_{-\\infty}^{\\infty}g(x)f(x)dx \\] is approximated by \\[ E[X] = \\int_{-\\infty}^{\\infty} xf(x)dx \\] \\[ E[g(X)] \\approx \\frac{1}{T}\\sum_{t=1}^{T}g(x_{t}) \\] Notice that this is really no more complicated, its simply saying that you draw from \\(f(x)\\), plug those values into \\(g()\\) and then average all those values of \\(g(x)\\)! How good is this estimate? \\[ SE_{E[g(x)]} = \\sqrt{\\frac{s^{2}_{g(x)}}{T}} \\] where \\(s^{2}_{g(x)}\\) is the sample variance of \\(g(X)\\) \\[ s^{2}_{g(x)} = \\frac{1}{T-1}\\sum^{T}_{1}(g(x_{t})-E[g(x)])^2 \\] (This is closely tied to some of the ideas we discussed in Biometry regarding bootstrap sampling. The basic idea is the same: Samples from \\(f(x)\\) can be used in lieu of \\(f(x)\\) for approximations of quantities involving \\(f(x)\\). The quality of those approximations increases as the number of samples used increases.) 6.4 Sometimes you just want the integral… So far, we’ve been focused on using MC integration to calculate an expected value, but really it is a more general strategy for calculating an integral. Let’s say we want to know the integral of some function over the interval \\([a,b)\\). We can use the \\(Unif(a,b)\\) distribution to help us, by using it for \\(f(x)\\) in the equation above, i.e. as the distribution we can draw easily from. To see that, lets re-write the initial integral as \\[ \\int^{b}_{a}g(x)\\frac{(b-a)}{(b-a)}dx = (b-a)\\int^{b}_{a}g(x)\\frac{1}{(b-a)}dx = (b-a)\\int^{b}_{a}g(x)f(x)dx \\] The last version here looks like what we had up above. So we now draw from \\(f(x)=Unif(a,b)\\) and plug those draws into our function \\(g(x)\\) \\[ (b-a)\\left[\\frac{1}{N}\\sum^{N}_{t=1}g(x_{t})\\right] = \\sum^{N}_{t=1}g(x_{t}) \\times \\frac{(b-a)}{N} \\] I’ve re-written this on the right hand side because it connects it to the geometric interpretation illustrated here (from Jarosz 2008; note that the figure indexes the samples by \\(i\\), whereas we are using \\(t\\)). Figure 3.1: The left hand figure is just the Riemann sum version of integration. The right hand side is what we are essentially doing with Monte Carlo integration. Instead of drawing equal spaced boxes along the x-axis, we are sampling values along the x axes from a uniform distribution and then using those values to calculate the function \\(g(x)\\). Note that the term Monte Carlo Integration is sometimes replaced by, or used synonymously with the phrase Monte Carlo simulation. Don’t let this confuse you. The idea behind both of these terms is simply that you can replace a probability distribution function (which may be a conditional probability distribution) with samples from that probability distribution function. In one-dimension, this all seems rather too simple to be of any use, but in multi-dimensional problems, these methods are essential. The reason is that if you have \\(T\\) multidimensional draws from \\(f(\\vec{X})\\) (where I am using vector notation explicitly to denote the fact that each draw contains \\(&gt;1\\) element), then you can make inference about any particular component by using the draws for that component completely ignoring the other components. Why does this work? Because the draws from the multidimensional distribution “average out” (heuristically speaking) the other components which might be related. In other words, to the extent that the pdf involves correlations among components, the draws from the multidimensional distribution reflect those underlying correlations already, and you can use the marginal distributions directly without concern for the multidimensionality of it. (Why this is so exciting will become clearer as we get into more detailed Bayesian examples…) We will play around this this in lab as well. 6.5 Importance Sampling Importance sampling is similar to MC integration, and uses a bit of a trick to get from a distribution you can’t easily sample from, to one you can. Let’s say that \\(f(x)\\) above is a distribution that you cannot easily sample from. You can get around this problem by finding a similar distribution that you can sample from, using \\[ f(x) = f^{*}(x)\\frac{f(x)}{f^{*}(x)} \\] What have we gained? Well, what we can do is sample from \\(f^{*}(x)\\) and weight these draws by the ratio \\(\\frac{f(x)}{f^{*}(x)}\\). Now we can get E[X] (or, similarly, the E[g(x)]), by drawing from \\(f^{*}(x)\\) to get a chain of values \\(x_{i}\\) and calculating \\[ \\frac{1}{n}\\sum^{n}_{i=1}x_{i}\\frac{f(x_{i})}{f^{*}(x_{i})} \\] How useful is this method? The challenge here is in finding a good distribution \\(f^{*}(x)\\) that has sufficient probability over the range that is important for f(x), but you don’t want something so “flat” that you end up sampling a lot of x values that don’t really contribute to the expected value of interest. 6.6 Sampling Importance Resampling Notice that in the above discussion of Importance Sampling, I only showed you how to use the Importance Ratios to calculate expectations, but we didn’t actually discuss how to use this method to get samples from the distribution itself. This procedure is called Sampling Importance Resampling, and we will go over it and the discussion by Smith and Gelfand (1992) in lab. "],
["week-4-lab.html", "7 Week 4 Lab 7.1 Smith and Gelfand (1992)", " 7 Week 4 Lab In lab this week, we are going to play around with writing R code to actually do some sampling. The goal of this week was to introduce you to various alternative methods of sampling from a distribution, with the ultimate aim of being able to sample from the posterior distribution in a Bayesian analysis. We have two methods in hand to make draws from an unknown distribution: Rejection Sampling, and Sampling Importance Resampling. Let’s take the following PDF, which is not one of the distributions built-into R and therefore not one we have an easy means of drawing samples from: \\[ f(x|\\sigma) = \\frac{1}{2\\sigma}e^{-|x|/\\sigma} \\] Our choice of \\(\\sigma\\) here is arbitrary, so lets work with \\(\\sigma=2\\). Exercise 1: Write a script to generate samples from this distribution using Rejection Sampling (RS). Keep in mind that this distribution is valid (i.e. ‘has support’) for all \\(x \\in (-\\infty,+\\infty)\\) and therefore your candidate distribution has to also have support over that range. Exercise 2: Write a script to generate samples from this distribution using Sampling Importance Resampling (SIR). This is quite similar to rejection sampling except that it does not require you to find a constant M that ensures that your candidate function is always larger than your target function. (This can be handy when your target function is unknown.) Pseudo code for SIR: * Sample a large number of random values from a candidate distribution with support over the same range of x values as the target distribution. * Find the probability of obtaining those values from the target distribution (i.e. the probability density at each \\(x\\) value drawn). * Normalize these probabilities from Step 2 so they sum to 1. * Use the (now normalized) probabilities from Step 3 as weights in a resampling of the random values from Step 1. In other words, use the ‘sample’ function in R to sample with replacement from the values drawn in Step 1, and use the probabilities from Step 3 as weights for that bootstrap sampling. * The samples from Step 4 are the draws from the unknown distribution! Exercise 3: Calculate the E[X] of this distribution using either the samples from IS or those from SIR. (If you did everything correctly, they should be roughly the same.) (Stop and work out what E[X] should be mathematically.) Look back at the Week #4 Lecture and make sure you see why this procedure is closely related to the idea of Monte Carlo Integration. What is the function \\(g(x)\\) in this case? Exercise 4: How good is your estimate from Exercise 3? (i.e., calculate the standard error on E[X]. How do we do this? We can either bootstrap from our samples OR [perhaps even better] calculate the SE by actually drawing new sets of samples altogether.) 7.1 Smith and Gelfand (1992) Key points: * Bayesian statistics is all about using the data to go from a prior distribution for model parameters to a posterior distribution for model parameters. In some cases, this can be done directly (e.g., when we have conjugate priors). More often than not, this cannot be done directly. In these cases, we have to settle for a somewhat indirect approach focused on using the data to go from samples from the prior distribution to samples from the posterior distribution. We have replaced manipulations of the pdfs, with stochastic samples from those pdfs. * Rejection methods require that you can calculate some number M such that the ratio of the candidate distribution to the target distribution is always greater than or equal to one. Sometimes this isn’t possible. In these cases, Smith and Gelfand (1992) introduce another approach called the weighted bootstrap approach. What is it, or in what way is it related to a simple bootstrap? Note that in other places, this approach is called SIR=Sampling – Importance Resampling. We will now work through the binomial example discussed by Smith and Gelfand (1992). The basic premise is this: Let’s say you have two Binomial variables \\[ X_{i1} \\sim Binomial(n_{i1},\\theta_{1}) \\\\ X_{i2} \\sim Binomial(n_{i2},\\theta_{2}) \\] conditionally independent given \\(n_{i1}\\), \\(n_{i2}\\),\\(\\theta_{1}\\), and \\(\\theta_{2}\\). Let’s say that you don’t observe \\(X_{i1}\\) and \\(X_{i2}\\) directly, but only their sum \\[ Y_{i} = X_{i1}+X_{i2} \\] For simplicity sake, let’s assume you have three data points, so \\(i=1,2,3\\). I replicate the table from Smith and Gelfand here: i=1 i=2 i=3 \\(n_{i1}\\) 5 6 4 \\(n_{i2}\\) 5 4 6 \\(y_{i}\\) 7 5 6 So, in words, the situation is this. You have two people flipping coins, and in each trial you know how many each person flipped but you only how many heads they had in total. You want to estimate the probability of each person getting a head. Before launching into a Bayesian analysis of this, let’s just work out what we would expect using plain-old maximum likelihood estimation. The likelihood from Smith and Gelfand has two typos, so I am reproducing the correct likelihood below: \\[ \\prod^{3}_{i=1}\\left[\\sum_{ji} {n_{i1} \\choose j_i}{n_{i2} \\choose y_i-j_i}\\theta_{1}^{j_{i}}(1-\\theta_{1})^{n_{i1}-j_{i}}\\theta_{2}^{y_{i}-j_{i}}(1-\\theta_{2})^{n_{i2}-y_{i}+j_{i}}\\right] \\] Note that the bounds on \\(j_{i}\\) are as follows: \\(max{0,y_{i}-n_{i2}} \\leq j_{i} \\leq min{n_{i1},y_{i}}\\). Important! They don’t say in the paper, but Smith and Gelfand drop the factorials from their likelihood \\({n_{i1} \\choose j_{i}}{n_{i2} \\choose y_{i}-j_{i}}\\). To match with their figure, you should do so as well. The constant does not change the \\((\\theta_{1},\\theta_{2})\\) location of the peaks of the posterior distribution. However, it does change the posterior and strictly speaking, Smith and Gelfand are incorrect. (In other words, the peaks are still in the same place but the shape of the likelihood is changed.) To get the correct posterior likelihood, the factorials should be left in. But for the purposes of the problem set, we will also drop these terms because doing so will allow us to reproduce what Smith and Gelfand have done. Exercise 5: Use a grid search to find the MLEs. Plot the two-dimensional likelihood (since we left off the constants in step 1, this will be an unscaled likelihood, but it doesn’t matter for our purposes) To apply a Bayesian analysis, we need prior distributions for \\(\\theta_{1}\\) and \\(\\theta_{2}\\). We will follow Smith and Gelfand’s lead and just use the unit square (so Unif(0,1) for both). What other prior distributions might we have used? We will use sampling importance resampling to sample from the posterior distribution. To do this, first generate a sample from the prior distributions to get prior samples of \\((\\theta_{1},\\theta_{2})\\). Then calculate the likelihood of the data for each sample from the prior \\((\\theta_{1},\\theta_{2})\\). The probability of resampling each sample from the prior is \\[ q = \\frac{\\mbox{Likelihood}(\\theta_{1},\\theta_{2};y)}{\\sum \\mbox{Likelihood}(\\theta_{1},\\theta_{2};y)} \\] Use this procedure to generate a posterior distribution for \\(\\theta_{1}\\) and \\(\\theta_{2}\\). Exercise 6: Plot the posterior distribution in the manner of Smith and Gelfand (1992) Figure 2 on top of the likelihood surface from question 2. (It might be easier to visualize if you plot the surface as a contour plot, rather than as a color image.) Are they similar? Different? Exercise 7: What does this procedure tell us about how the support for the prior (the range of values “permitted” under the prior for which the pdf&gt;0) affects the support for the posterior? In light of this, why does Figure 2 from Smith and Gelfand (1992) look funny? Exercise 8: What would the posterior samples look like if the priors for \\(\\theta_{1}\\) and \\(\\theta_{2}\\) were drawn from Unif(0,0.5) instead of Unif(0,1)? "],
["week-5-lecture.html", "8 Week 5 Lecture 8.1 Gibbs Sampling 8.2 Metropolis algorithm 8.3 The Messy reality = Hybrid of M-H and Gibbs 8.4 Convergence 8.5 Bayesian change point example 8.6 Hierarchical model 8.7 For more information about this week’s topic", " 8 Week 5 Lecture Papers to read this week: Lunn et al. Chapter 4 Robert and Casella Lecture Notes: For the purpose of this week, focus on pages 126-167. ClarkLab5 Last week we talked about several applications of MC methods. This week, we are going to expand on these ideas to cover two additional, very important, applications: Gibbs sampling Metropolis / Metropolis-Hastings Keep in mind that the point behind doing MC sampling is that it provides a mechanism for sampling from the posterior distribution. (Remember, the posterior distribution may have [in fact, usually has] no straightforward analytical form.) Last week we covered a class of non-iterative samplers that require us to have a candidate distribution that we could sample from. (They are non-iterative, because we could have drawn all N candidate values simultaneously. The accept-reject criteria for each sample was independent of the accept-reject decision made on the others.) The problem with these methods is that it is often very hard to come up with a good candidate distribution that is a good approximation to the target distribution (the closer the better – otherwise, we have a very inefficient algorithm) and which we can sample from. (In one-dimension, this is usually not an issue, the problems arise when you have multi-dimensional problems because candidate distributions for the joint PDF are harder to generate.) Another class of samplers, used more often in Bayesian statistics, is iterative samplers that (for the most part) rely on the generation of a “Markov chain”. Before we get into MCMC, its worth thinking a bit more about what a Markov Chain is. A Markov Chain is a series of observations, each one dependent only on the last (or, in more complex examples, or a finite number of previous observations). At the end of the day, a Markov Chain is simply a string of numbers (or, equivalently, states), which we will denote \\(X^{t}\\). So the state of the chain at \\(t=0\\) is \\(X^{0}\\), etc. The easiest way to represent the allowed transitions in the chain is through a transition matrix \\[ P = \\begin{bmatrix} p_{11} &amp; p_{12} &amp; p_{13}\\\\ p_{21} &amp; p_{22} &amp; p_{23}\\\\ p_{31} &amp; p_{32} &amp; p_{33} \\end{bmatrix} \\] where \\(p_{12}\\), for example, is the probability of making the transition between state 1 and state 2 in the chain. Let’s say that we start the Markov Chain with an initial value of \\(X^{0} = 1\\). What is the probability that we end up at state 3 after two time steps (i.e. that \\(X^{2} = 3\\))? To figure this out, we have to account for all possible Markov Chains that could have occurred between \\(t=0\\) and \\(t=2\\): Path \\(X^{0}\\) \\(X^{1}\\) \\(X^{2}\\) Probability #1 1 1 3 $p_{11}p_{13} #2 1 2 3 $p_{12}p_{23} #3 1 3 3 $p_{13}p_{33} Therefore, the probability of being in state 3 by \\(t=2\\) is the sum of these three probabilities: \\(p_{11}p_{13}+p_{12}p_{23}+p_{13}p_{33}\\). Note that the transition probabilities are independent (that’s why the total probability is just the product of the two individual transition probabilities). The “Markov”ness comes about because the probability of transitioning from State 1 to State 3 is independent of the path that got you to State 1 in the first place, and so the probability of going to State 3 depends only on the fact that you are currently in State 1. If we let the chain run long enough, you can see that we will eventually reach all the states. (None of the states are absorbing states, so you have a finite probability of moving from each state to every other state – such chains are called “ergodic”. I won’t get into any proofs or formal math here, but some vocabulary will be helpful in reading the literature on MCMC.) Not only will you reach all of the states, but the long-run probability of being in each state will eventually converge to a distribution which is governed by the transition matrix and is independent of the starting point (\\(X^{0}\\)). (Note that I am presenting Markov Chains in the context of discrete states, but the same logic holds when we extend this to continuous states whose transitions are described by a probability density.) Mathematically, we would say that the probability of state \\(X^{t+1}\\) is given by the following \\[ X^{t+1} \\sim p_{trans}(x|X^{t} = x^{t}) \\] that is, that the probability is conditional on the value of the state at time \\(t\\). The important element of this is that even though the individual states of the chain are conditional on the preceding states, the marginal (unconditional) distribution of the states will converge to a unique stationary distribution that is independent of the starting values. From the Bayesian perspective, our goal is to choose a transition distribution that will generate a sequence of distributions for the parameters of our model whose unique stationary distribution is the joint posterior distribution of interest. (Some of this will be clearer as we go through more applied examples…) Now that we have the basic idea, let’s discuss Gibbs sampling, which is one of the most widely used algorithms for generating a Markov Chain. 8.1 Gibbs Sampling Gibbs sampling is a method for obtaining a multidimensional Markov Chain by splitting a vector of random variables into univariate draws from conditional distributions. A simple example would be a three-dimensional Markov Chain for parameters \\((\\theta_{1},\\theta_{2},\\theta_{3})\\). We may have no easy way to sample from this joint probability density, but we might know all the conditional probability distributions: \\[ p(\\theta_{1}|\\theta_{2},\\theta_{3},y) \\\\ p(\\theta_{2}|\\theta_{1},\\theta_{3},y) \\\\ p(\\theta_{3}|\\theta_{1},\\theta_{2},y) \\] where \\(y\\) is the data. The basic strategy is as follows: Choose arbitrary starting values for \\((\\theta_{1},\\theta_{2},\\theta_{3})\\), which we will call \\((\\theta_{1}^{(0)},\\theta_{2}^{(0)},\\theta_{3}^{(0)})\\). Iterate through each parameter, drawing from its conditional distribution for the next iteration of the chain, conditioning on the most recent values for each parameter \\[ \\theta_{1}^{(1)} \\sim p(\\theta_{1}|\\theta_{2}^{(0)},\\theta_{3}^{(0)},y) \\\\ \\theta_{2}^{(1)} \\sim p(\\theta_{2}|\\theta_{1}^{(1)},\\theta_{3}^{(0)},y) \\\\ \\theta_{3}^{(1)} \\sim p(\\theta_{3}|\\theta_{1}^{(1)},\\theta_{2}^{(1)},y) \\\\ \\theta_{1}^{(2)} \\sim p(\\theta_{1}|\\theta_{2}^{(1)},\\theta_{3}^{(1)},y) \\] and so forth. Continue iterating through the chain a large number of times, until the chain has converged (we haven’t yet discussed how we would test convergence yet). EXERCISE: We’re going to practice this a bit using a simple example inspired by our earlier work on the uniform triangle distribution. Earlier we drew samples from a uniform triangle distribution using Rejection Sampling. Here we are going to use Gibbs Sampling. First, you will need to work out the two conditional probabilities \\(p(x|y)\\) and \\(p(y|x)\\). You could of course use Bayes Theorem to calculate this Figure 1.1: Uniform triangle distribution \\[ p(x|y) = p(x,y)/p(y) \\] and \\[ p(y|x) = p(x,y)/p(x) \\] since you already worked out the marginal probabilities \\(p(x)\\) and \\(p(y)\\) but in this case its easier just to reason through it. In other words, if you knew \\(x\\), what would be the possible range of \\(y\\), and vice versa? (Here we are restricting the range to \\(x \\in (0,1)\\) and \\(y \\in (0,1)\\), which is slightly different from our original example, but it should be straightforward to work out the two marginal probabilities involved here.) Once you have the conditional probabilities, Gibbs sampling proceeds as follows (\\(k\\) is the variable to track the number of iterations): Choose legitimate starting values \\((x_{0},y_{0})\\). Sample \\(x_{1}\\) from \\(p(x_{1}|y_{0})\\). Sample \\(y_{1}\\) from \\(p(y_{1}|x_{1})\\). \\(k \\rightarrow k+1\\), go back to step 2 When you have tried a large number of samples \\(k\\), plot your bivariate samples - how do they look? (In other words, make a scatterplot of the sampled \\((x,y)\\) pairs.) Now histogram just the \\(x\\) values. This is the marginal probability density \\(p(x)\\). Does it make sense why? Think back to the exercise we did with graph paper. If we have bivariate samples from \\((x,y)\\) and we simply tally up all the samples for a certain x-value (irrespective of the y values involved), we have “marginalized out” \\(y\\) to create \\(p(x)\\). When we do modelling in JAGS, we will do this automatically. In fact, the posterior summaries provided by JAGS and the related R packages automatically provide marginal posterior summaries, and they do that by summarizing the draws for one variables ignoring the values for all the other variables. (I think of this as taking one column from the sims.matrix.) 8.2 Metropolis algorithm The Metropolis algorithm is widely used, and can be described by the following steps, which describe the transition from one state \\(x\\) to another state \\(x^{\\prime}\\) as follows: Assuming we are currently in a state \\(x\\), a “candidate” \\(x^*\\) is proposed according to some symmetric probability \\(S(x,x^*)\\) (symmetric means that \\(S(x,x^*)=S(x^*,x)\\)). (Note that Metropolis-Hastings is the same as Metropolis with the added flexibility of assymetric jumps. We won’t cover M-H specifically, except to to note that the terms are often used interchangeably because symmetric jumps are so common.) This candidate, \\(x^*\\), is accepted as the next state with probability \\[ \\mbox{min}[1,\\pi(x^*)/\\pi(x)] \\] where \\(\\pi(x)\\) is the probability density associated with \\(x\\), and likewise, \\(\\pi(x^*)\\) is the probability density associated with \\(x^*\\). If \\(x^*\\) is accepted, then \\(x^{t+1}=x^{*}\\). If \\(x^*\\) is rejected, then \\(x^{t+1} = x^{t}\\). (In other words, if you reject the candidate state, then you stay put.) Note that you only need to be able to evaluate the ratio \\(\\pi(x^*)/\\pi(x)\\), so you don’t need to worry about, or even know, the normalizing constants associated with the function \\(\\pi\\). The proposal distribution \\(S(x,x^*)\\) is critical here because it controls how quickly or slowly the chain samples the space, and how quickly the chain converges. One popular proposal distribution is that which would be associated with an unbiased random walk. Some terminology here to navigate if you are applying the Metropolis algorithm to a multivariate problem: : Proposals only change one component of \\(x\\), and updates to each component are applied in sequence. : Proposals change all components of \\(x\\) simultaneously. For example, you might generate proposals of the form \\[ \\begin{pmatrix} \\theta_{1}^{proposed}\\\\ \\theta_{2}^{proposed}\\\\ \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} 0\\\\ 0\\\\ \\end{pmatrix}, \\begin{pmatrix} 1 &amp; 0.5\\\\ 0.5 &amp; 1\\\\ \\end{pmatrix}\\right) \\] A nice visual of M-H (courtesy of Paul Lewis) is given here Figure 3.1: Visualizing Metropolis-Hastings movement rules. Source: Paul Lewis You can see that, under the movement “rules” of M-H, the robot tends to spend the most time near the top of the hill. (FYI: Gibbs sampling is a special case of M-H in which the proposed value of the parameter (the one drawn from the conditional) is always accepted.) 8.3 The Messy reality = Hybrid of M-H and Gibbs JAGS and related software use a messy combination strategy that combines Gibbs sampling and M-H sampling. The essential idea is that you may find yourself with a suite of parameters, some of which have known conditional relationships, and others which do not. In these cases, you can use Gibbs where you can, and M-H where you can’t use Gibbs. (Why bother with Gibbs at all? Remember that Gibbs has an acceptance ratio of 1, so its more efficient than M-H.) This is what is hidden under the hood of JAGS. Note that JAGS always starts off with “adaptive sampling” (in the GUI, you can see this happening, but from R this is hidden…). This is the period of time when JAGS is trying to tune the M-H proposals to get a target acceptance ratio. 8.4 Convergence One of the big challenges in doing MCMC is assessing whether your chains have converged to their unique stationary distribution. How do we assess convergence? Its often easy to see that chains have converged – we usually do this visually by starting with three “overdispersed” sets of starting values and simply plotting the chains to see whether they are indistinguishable after some period of time. Since we are only interested in the stationary behavior and not the initial transient behavior, we discard the beginning of each chain so our posterior distribution is only derived from the stationary portion of the chain. We call the portion of the chain discarded the “burn in”. (Papers will report this number, as in “we used a burn in of 1,000 iterations…” and so forth.) Another aspect to MCMC chain assessment is looking at how well they are “mixing”, which is just to ask how well they are sampling the posterior distribution. Warning signs that your chains are not mixing well is when the chains get stuck in certain regions of parameter space (in these cases, the posterior histograms tends to be multimodal as well.) (You want the chains to look like white noise…) Figure 8.1: Samples from an MCMC sampler that has converged. The autocorrelation function can provide additional information. Figure 8.2: Autocorrelation function from MCMC samples. Sometimes, to save storage, you may decide to subsample, or “thin” the chain and store only every other draw (or every 3rd, or 10th, etc.). The autocorrelation function can help you decide how spaced the draws would have to be in order to be independent. However, note that there is no need to thin the chains unless memory is an issue. Its better to keep all the draws if you can. However, when chains converge slowly, or meander over parameter space, its often hard to prove that the chains have converged (or, more precisely, to say that you cannot reject the null hypothesis that the three chains come from the same distribution). (Meandering over parameter space is annoying, but it doesn’t necessarily mean that your script doesn’t work or the problem cannot be solved. Sometimes the chains just mix slowly, and while there are tricks to try and make them mix more quickly, sometimes you just have to run really long chains to get a good picture for the posterior distribution. There are many ways to formally test for convergence; we will discuss this in Week 9. For now, we’ll just visually assess the chains for convergence, which is what people actually do right up until a manuscript reviewer asks for something more rigorous. (But much better not to make assumptions that might cause for an embarrassing retraction following review…) 8.5 Bayesian change point example The following is based on the Bayesian change point example by Murali Haran. Consider the following simple changepoint example: You have Poisson draws and you expect that the Poisson parameter has changed at some point in the time series, so that \\[ Y_{i}|k,\\theta,\\lambda \\sim Pois(\\theta) \\mbox{for i=1,...,k} \\] \\[ Y_{i}|k,\\theta,\\lambda \\sim Pois(\\lambda) \\mbox{for i=k+1,...,n} \\] We assume the following prior distributions for \\(\\theta\\) and \\(\\lambda\\) \\[ \\theta \\sim Gamma(0.5,3) \\] \\[ \\lambda \\sim Gamma(0.5,3) \\] \\[ k \\sim Unif(1,n) \\] We will assume the following parameterization for Gamma \\[ Gamma(\\alpha,\\beta) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha-1}e^{-x/\\beta} \\] We want to use the data \\(Y\\) to make inference about the following three dimensional posterior \\[ f(\\theta,\\lambda,k|Y) \\propto \\left(\\prod_{i=1}^{k}\\frac{\\theta^{Y_{i}}e^{-\\theta}}{Y_{i}!}\\right) \\times \\left(\\prod_{i=k+1}^{n}\\frac{\\lambda^{Y_{i}}e^{-\\lambda}}{Y_{i}!}\\right) \\times \\frac{1}{\\Gamma(0.5)3^{0.5}}\\theta^{-0.5}e^{-\\theta/3} \\times \\frac{1}{\\Gamma(0.5)3^{0.5}}\\lambda^{-0.5}e^{-\\lambda/3} \\times \\frac{1}{n} \\] To do Gibbs sampling we need to sample from the posterior for each parameter conditional on the other parameters (and the data). Let’s start with the conditional posterior for k. We use the following fact \\[ f(k|\\theta,\\lambda,Y)=\\frac{f(\\theta,\\lambda,k|Y)}{f(\\theta,\\lambda|Y)} \\] which means that \\[ f(k|\\theta,\\lambda,Y) \\propto f(\\theta,\\lambda,k|Y) \\] where on the right hand side, we only need to worry about the terms that actually involve k. In this case \\[ f(k|\\theta,\\lambda,Y) \\propto \\left(\\prod_{i=1}^{k}\\frac{\\theta^{Y_{i}}e^{-\\theta}}{Y_{i}!}\\right) \\left( \\prod_{i=k+1}^{n}\\frac{\\lambda^{Y_{i}}e^{-\\lambda}}{Y_{i}!}\\right) \\] In fact, we can simplify this a bit further \\[ f(k|\\theta,\\lambda,Y) \\propto \\left(\\prod_{i=1}^{k}\\theta^{Y_{i}}e^{-\\theta}\\right) \\left(\\prod_{i=k+1}^{n}\\lambda^{Y_{i}}e^{-\\lambda}\\right) \\] because the denominators of the two products equal \\(\\prod_{i=1}^{n}Y_{i}!\\) which does not involve k. What about \\(\\theta\\)? Using the same idea as above \\[ f(\\theta|\\lambda,k,Y)=\\frac{f(\\theta,\\lambda,k|Y)}{f(\\lambda,k|Y)} \\] and \\[ f(\\theta|\\lambda,k,Y) \\propto f(\\theta,\\lambda,k|Y) \\] where again we only need the parts of the right hand side that involve \\(\\theta\\) \\[ f(\\theta|\\lambda,k,Y) \\propto \\left(\\prod_{i=1}^{k}\\theta^{Y_{i}}e^{-\\theta}\\right) \\times \\theta^{-0.5}e^{-\\theta/3} \\] What about \\(\\lambda\\)? Using the same idea as above \\[ f(\\lambda|\\theta,k,Y)=\\frac{f(\\lambda,\\theta,k|Y)}{f(\\theta,k|Y)} \\] and \\[ f(\\lambda|\\theta,k,Y) \\propto f(\\lambda,\\theta,k|Y) \\] where again we only need the parts of the right hand side that involve \\(\\lambda\\) \\[ f(\\lambda|\\theta,k,Y) \\propto \\left(\\prod_{i=k+1}^{n}\\lambda^{Y_{i}}e^{-\\lambda}\\right) \\times \\lambda^{-0.5}e^{-\\lambda/3} \\] So, how does Gibb’s sampling actually work in this case? First we pick starting values (at random, but within the support of the prior) for \\(\\lambda\\), \\(\\theta\\), and \\(k\\). We call these \\(\\lambda^{(0)}\\), \\(\\theta^{(0)}\\), and \\(k^{(0)}\\). We then sample from each conditional posterior at a time: \\[ k^{(1)} \\sim \\left(\\prod_{i=1}^{k^{(0)}}(\\theta^{(0)})^{Y_{i}}e^{-\\theta^{(0)}}\\right) \\left(\\prod_{i=k^{(0)}+1}^{n}(\\lambda^{(0)})^{Y_{i}}e^{-\\lambda^{(0)}}\\right) \\] then \\[ \\theta^{(1)} \\sim \\left(\\prod_{i=1}^{k^{(1)}}(\\theta^{(0)})^{Y_{i}}e^{-\\theta^{(0)}}\\right) \\times (\\theta^{(0)})^{-0.5}e^{-\\theta^{(0)}/3} \\] Which can simplify to \\[ \\theta^{(1)} \\sim Gamma(\\sum_{i=1}^{k^{(1)}}Y_{i}+0.5,\\frac{3}{3k^{(1)}+1}) \\] because the Gamma is conjugate to the Poisson. Finally, \\[ \\lambda^{(1)} \\sim \\left(\\prod_{i=k^{(1)}+1}^{n}(\\lambda^{(0)})^{Y_{i}}e^{-\\lambda^{(0)}}\\right) \\times (\\lambda^{(0)})^{-0.5}e^{-\\lambda^{(0)}/3} \\] Likewise… \\[ \\lambda^{(1)} \\sim Gamma(\\sum_{i=k^{(1)}+1}^{n}Y_{i}+0.5,\\frac{3}{3(n-k^{(1)})+1}) \\] Note that while the posteriors for \\(\\theta\\) and \\(\\lambda\\) have well-known forms because of conjugacy, the conditional posterior for k does not have a standard PDF and we only know its PDF up to a proportion. That’s OK, though, because we can use any of the tools have have learned to sample from PDFs that are not “built-into-R”. What we have done is break apart a complex 3-dimensional posterior into three one-dimensional posteriors. Often, these conditional posteriors are easy to write down because you’ve used conjugate priors that have an analytically tractable posterior (as with \\(\\theta\\) and \\(\\lambda\\)). In this case, the conditional posterior for k cannot be written in an easy form, so we have to use some other method of sampling inside the Gibbs loop. See Gelfand 2000 Section 2.4. Notice that when the conditional posterior take a non-standard form, we can employ any of the tools we have learned, including rejection sampling, importance sampling, or Metropolis-Hastings. The last of these is quite a common approach, and it is called “Metropolis-within-Gibbs”. However, there are no cookbook recipes for doing this, you (as the modeller) have to choose among the tools to find one that works for your case.) The next round of draws would look like \\[ k^{(2)} \\sim \\left(\\prod_{i=1}^{k^{(1)}}(\\theta^{(1)})^{Y_{i}}e^{-\\theta^{(1)}}\\right) \\left(\\prod_{i=k^{(1)}+1}^{n}(\\lambda^{(1)})^{Y_{i}}e^{-\\lambda^{(1)}}\\right) \\] \\[ \\theta^{(2)} \\sim \\left(\\prod_{i=1}^{k^{(2)}}(\\theta^{(1)})^{Y_{i}}e^{-\\theta^{(1)}}\\right) \\times (\\theta^{(1)})^{-0.5}e^{-\\theta^{(1)}/3} \\] \\[ \\lambda^{(2)} \\sim \\left(\\prod_{i=k^{(2)}+1}^{n}(\\lambda^{(1)})^{Y_{i}}e^{-\\lambda^{(1)}}\\right) \\times (\\lambda^{(1)})^{-0.5}e^{-\\lambda^{(1)}/3} \\] We then repeat this process the draw \\(\\lambda^{(3)}\\), \\(\\theta^{(3)}\\), and \\(k^{(3)}\\) etc. 8.6 Hierarchical model The full hierachical model originally posed by Haran includes a prior on one of the parameters for the Gamma prior on \\(\\theta\\) and \\(\\lambda\\). In other words, we postulate a prior distribution for \\(\\theta\\) and \\(\\lambda\\), but we say that we don’t know what the prior distribution should be exactly, and so we put a prior distribution on one of its parameters. Specifically, \\[ \\theta|b_{1} \\sim Gamma(0.5, b_{1}) \\] \\[ \\lambda|b_{2} \\sim Gamma(0.5, b_{2}) \\] \\[ b_{1} \\sim Gamma(5,5) \\] \\[ b_{2} \\sim Gamma(5,5) \\] \\[ k \\sim Unif(1,...,n) \\] These “priors of priors” (\\(b_{1}\\) and \\(b_{2}\\)) are called hyperpriors. Note that I have replaced \\(c_{1}\\), \\(c_{2}\\), \\(d_{1}\\), and \\(d_{2}\\) from Haran with the number 5, which is arbitrary but I wanted to make it clear what was a fixed number and what was a parameter. We will come back to these when we discuss hierarchical models in Week #14. For now, I’ll simply say that as each step of the Gibbs sampling, you need to sample from all the terms that include the parameter of interest. Whereas in the non-hierarchical example, we had three parameters, now we have five parameters that we need to iteratively sample from. 8.7 For more information about this week’s topic Some more detailed statistical papers on various aspects of sampling: Gelfand 2000 Casella and George 1992 Gelfand and Smith 1990 Chib and Greenberg 1995 A fairly non-technical paper about using MCMC in understanding landslides from \\(\\textit{Physics Today}\\). For papers involving Bayesian analysis in phylogenetics: Altekar et al. 2004 Brown 2003 "],
["week-5-lab.html", "9 Week 5 Lab 9.1 Gibbs Sampler", " 9 Week 5 Lab We won’t get into Bayesian regression until Week #7 and #8, but we will use an example of a logistic regression to illustrate the basic method of Metropolis sampling. We will start with an ecological data set that comes from M.P. Johnson and P.H. Raven (1973). Species number and endemism: The Galapagos Archipelago revisited. Science 179, 893-895. and which can be downloaded here. This is really a subset of the original dataset; I have included only the columns relevant for lab: Island: Name of Island Plants: Number of plant species PlantEnd: Number of endemic plant species Elevation: Maximum elevation (m) Let’s focus on endemism as the trait of interest, so for island \\(i\\), \\(y_{i}=1\\) if the plant is endemic and \\(y_{i}=0\\) if the plant is not endemic. We will treat Elevation as the covariate \\(x_{i}\\) of interest, so \\[ Y \\sim Binom(n,p) \\] where \\(n\\) is the total number of plants on the island, and \\(p\\) is the probability that a plant is endemic. \\[ logit(p_{i}) = \\beta_{0}+\\beta_{1}x_{i} \\] We might put the following vague priors on the parameters \\[ \\beta_{0} \\sim N(0,1) \\\\ \\beta_{1} \\sim N(0,1) \\\\ \\] We will use Metropolis to sample from the posterior distribution. We will use the following proposal distribution \\[ S(\\beta,\\beta^*) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(\\beta^*-\\beta)^2/2\\sigma^2} \\] (I am letting you find the best \\(\\sigma\\).) This proposal distribution is a random-walk proposal distribution. The pseudocode goes as follows: Choose starting values. Propose new value of \\(\\beta_{0}\\) (a.k.a. \\(\\beta_{0}^*\\)) Accept \\(\\beta_{0}^*\\) with probability \\(\\mbox{min}[1,\\pi(x^*)/\\pi(x)]\\) Repeat Steps 2 and 3 with \\(\\beta_{1}\\) Repeat Steps 2-4 many times (1000? 10000?) You can also make multivariate draws using the ‘mvrnorm’ function from the MASS package. Question: Why might multivariate draws be preferrable? Keep in mind the following: \\[ P(\\beta_{0}^*,\\beta_{1}^*|Y) = \\frac{P(Y|\\beta_{0}^*,\\beta_{1}^*)P(\\beta_{0}^*,\\beta_{1}^*)}{P(Y)} \\] and \\[ P(\\beta_{0},\\beta_{1}|Y) = \\frac{P(Y|\\beta_{0},\\beta_{1})P(\\beta_{0},\\beta_{1})}{P(Y)} \\] Therefore \\[ \\frac{P(\\beta_{0}^*,\\beta_{1}^*|Y)}{P(\\beta_{0},\\beta_{1}|Y)} = \\frac{\\pi(x^*)}{\\pi(x)} = \\frac{P(Y|\\beta_{0}^*,\\beta_{1}^*)P(\\beta_{0}^*,\\beta_{1}^*)}{P(Y|\\beta_{0},\\beta_{1})P(\\beta_{0},\\beta_{1})} \\] thus eliminating the pesky denominator in both expressions. We will make a minor modification to step 3 to head off problems with numerical errors. The original formulation would require that we draw a value from Unif(0,1) and accept if this value if it is less than \\(\\pi(x^*)/\\pi(x)\\). Since probabilities (esp. joint probabilities) are always very small problematic numbers, we would prefer to log both sides of this, and to log the draw from the Unif(0,1) and compare this value to \\(\\pi(x^*)/\\pi(x)=log(\\pi(x^*))-log(\\pi(x))\\). \\[ log(\\pi(x^*))-log(\\pi(x)) = log(P(Y|\\beta_{0}^*,\\beta_{1}^*))+log(P(\\beta_{0}^*))+log(\\beta_{1}^*)-log(P(Y|\\beta_{0},\\beta_{1}))-log(P(\\beta_{0}))-log(P(\\beta_{1})) \\] In other words, we calculate the difference in log probabilities and use this as the acceptance threshold. Note – this still may not prevent numerical underflow – what are other possible solutions? Click for Answer Standardizing the covariate might be needed! (e.g., \\((elev-mean(elev))/sd(elev)\\)). Doing so makes it much easier to get your sampler working efficiently. Make sure your code works by comparing it to the results obtained using the glm function. Things to play around with: Once you get the code working, plot the chains for both \\(\\beta_{0}\\) and \\(\\beta_{1}\\) - have they converged? How would you check? Also, plot the histograms – these represent your posterior distributions. What do you get if you plot a scatterplot of the posteriors for \\(\\beta_{0}\\) and \\(\\beta_{1}\\)? Make your script calculate the acceptance rate. What happens to the acceptance rate when you vary ? (We are generally looking for acceptance rates in the vicinity of 0.5. How would you write your script to adaptively sample to get an acceptance rate $$0.5?) What happens if you use other proposal distributions? Does it converge faster or slower? 9.1 Gibbs Sampler (This example is basically Lunn et al. Example 4.2.2) For this example, I have generated some random values from a Normal distribution with unknown (to you) mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). The data are posted on Bb under “gibbsdata.txt”. Since JAGS works with the precision, we will define the precision \\(\\tau=1/\\sigma^{2}\\). We will use the following priors \\[ \\mu \\sim N(\\gamma, \\omega^{2}) \\\\ \\tau \\sim Gamma(\\alpha,\\beta) \\] Note that these are the conjugate priors where precision and mean are know, respectively, but that the joint distribution \\[ p(\\mu,\\tau) \\sim N(\\gamma, \\omega^{2}) \\times Gamma(\\alpha,\\beta) \\] is not the conjugate prior for the case in which both \\(\\mu\\) and \\(\\tau\\) are unknown. In this case we have to appeal to some other scheme for sampling from the posterior distribution, like a Gibbs sampler. What we will need in hand to make the Gibbs sampler work is the conditional distribution for \\(\\mu\\) assuming \\(\\tau\\) is known, and the conditional distribution for \\(\\tau\\) assuming \\(\\mu\\) is known. Keep in mind that \\[ \\mbox{posterior} = \\mbox{prior} \\times \\mbox{likelihood} \\] and the likelihood in this case is simply the pdf for a Normal, since the data are Normally distributed. (With all the focus on priors and posteriors, the likelihood sometimes gets lost in the shuffle, so don’t forget to make sure you have the right likelihood!) \\[ p(y_{1},y_{2},...,y_{n}|\\mu,\\tau) \\propto e^{-\\frac{\\tau}{2}\\sum_{i=1}^{n}(y_{i}-\\mu)^{2}} \\] Question: Why is there a sum in the exponent? OK, back to the question at hand – what is the distribution for \\(\\mu^{(t)}\\) if we have the data y and \\(\\tau^{(t-1)}\\)? (The beauty of Gibbs sampling is that each draw is conditional on the most recent draw for the other parameters, so the other parameters are assuming “known” at their most recent value.) In other words, what is the conditional posterior for \\(\\mu\\) assuming \\(\\tau\\) is known? \\[ p(\\mu|y,\\tau) \\propto e^{-\\frac{1}{2\\omega^{2}}(\\mu-\\gamma)^{2}}e^{-\\frac{\\tau}{2}\\sum_{i=1}^{n}(y_{i}-\\mu)^{2}} \\] (This is just the prior for \\(\\mu\\) times the likelihood.) But, because we assume \\(\\tau\\) is known here, the prior for \\(\\mu\\) is conjugate to the likelihood, and we can simply write down the posterior distribution! \\[ p(\\mu^{(t)}|y,\\tau^{(t-1)}) \\sim N\\left(\\frac{\\tau^{(t-1)}\\sum y_{i}+\\omega^{-2}\\gamma}{n\\tau^{(t-1)}+\\omega^{-2}},\\frac{1}{n\\tau^{(t-1)}+\\omega^{-2}}\\right) \\] Likewise, if we know \\(\\mu\\), then the prior for \\(\\tau\\) is conjugate to the likelihood, and its conditional pdf is given by \\[ p(\\tau^{(t+1)}|y,\\mu^{(t)}) \\sim Gamma\\left(\\alpha+\\frac{n}{2},\\beta+\\frac{1}{2}\\sum_{i=1}^{n}(y_{i}-\\mu^{(t)})^{2}\\right) \\] EXERCISE: Write a script to use Gibbs sampling to get posterior distributions for \\(\\mu\\) and \\(\\tau\\). Use three chains to check convergence, and provide plots of the chains and the posterior histograms for both \\(\\mu\\) and \\(\\tau\\). What is the posterior mean and 95th percentile credible intervals? "],
["week-6-lab.html", "10 Week 6 Lab 10.1 Fitting a distribution 10.2 One-way ANOVA", " 10 Week 6 Lab This week we’re going to get our feet wet with JAGS by building some simple models familiar from Biometry. We’ll start by using the template you were given in Week #1 when we were testing your JAGS installation (Week 1 JAGS test.R). I suggest copying that file over to a new folder for this week’s lab. (Side note: For the next few weeks, we’ll be sort of sloppy with our initial values. You can set the initial values to just about anything that’s in the range of the prior and the model will work fine. However, once we get into more complex models, the choice of initial values is more important. We’ll discuss this again later in the semester. Just keep in mind that our “set it and forget it” approach early in the semester is not best practice.) The first thing we’re going to do is demonstrate how Bayesian models, implement in JAGS, can be used to fit statistical distributions. In this case, we have the benefit of ‘fitdistr’ and all the other tools learned in Biometry, so we can check whether JAGS is actually working correctly. 10.1 Fitting a distribution Use R to draw 100 values from a Beta(2,5) distribution, save those values as a .csv, and modify the template JAGS file to estimate the parameters of this distribution. (We’ll assume that we know its from a beta distribution, we just don’t know the parameters.) You’ll need to think carefully about what the prior should be for the two shape parameters. What is their range? Question #1: Does your Bayesian model agree with ‘fitdistr’? Question #2: What happens to the posterior distribution as you reduce the number of simulated data points to 50, to 10, to 2? (You can just sub-sample from the initial set of values.) To get some intuition for what the burn-in is doing, you might try setting burn in to something very small and making your initial values very incorrect. If you plot the chains, you should be able to see the model moving from the initial values to the correct values. It is that portion of the chain (the transients) that we aim to eliminate by setting a burn-in period. 10.2 One-way ANOVA Here we are going to use a small dataset found [here] (http://www.stat.ucla.edu/projects/datasets/seaslug-explanation.html). From the data source: GENERAL EXPLANATION OF THE STUDY Sea slugs, which live along the Southern California coast, produce thousands of microscopic larvae each year. These larvae locate and settle onto a patch of vaucherian seaweed, before developing into sea slugs. The accompanying data file is from a pilot study on the ability of sea slug larvae to detect this kind of seaweed at different tide heights. Instead of randomly swimming until they find this seaweed (as was previously believed), these larvae actually “smell” the seaweed when they are passing over it in the water. They do this, it is believed, by detecting chemicals that slowly leach out of the seaweed. This study attempts to support this theory by analyzing the ability of larvae to distinguish the smell of this seaweed just as the tide is coming in – when the chemicals are most concentrated – and at high tide – when the chemicals are more dilute due to the rising tide. This study has been published: Krug, P.J. and R.K. Zimmer. 2000b. Larval settlement: chemical markers for tracing production, transport, and distribution of a waterborne cue. Marine Ecology Progress Series, vol. 207: 283-296. BRIEF DESCRIPTION OF THE DATA Just before the tide came in, one water sample containing filtered sea water was collected away from the patch of seaweed. This sample is the control (it is coded 99). Once the tide washed in, water samples were collected above a patch of vaucherian seaweed every five minutes, for a total of thirty minutes. Each of these samples was then divided into six, so that there are six replicates for each time point. There are seven time points (0-30 minutes) so there are a total of 42 observations, excluding the control. The control was divided into five replicates. Fifteen slug larvae were then injected into each of the replicates, and the percentage of larvae that metamorphosed was recorded. This percentage is a function of the ability of the larvae to detect the chemicals from the seaweed. I’ve added a column for “Group” to the original dataset and put it in the folder for this week’s lab (seaslug.csv). Its worth noting that in the original experiment, there were not exactly 15 seaslugs in each treatment (some would die before undergoing metamorphosis, etc.). While this experiment nominally lends itself to a Binomial model, the data provided are just percentages (we have no data on final sample sizes in each treatment group). While the beta distribution would be the most appropriate distribution, we will keep things simple and model the percentages with a Normal distribution. There are many ways to fit an ANOVA model. Let’s start by writing a script to fit this model \\[ Y_{i} \\sim N(\\mu_{i},\\sigma^{2}) \\] where \\(Y_{i}\\) is the percentage of sea slugs that metamorphosed in Group \\(i\\). When you get the code working, you should end up with estimates of the mean percentage for each group of sea slug and the standard deviation \\(\\sigma\\). Question #3: Is the normal distribution really appropriate here? What other distribution might we use that would be better? Amend the model with a more appropriate distribution for the data. Question #4: In our original model set up, we estimated the average percentage for each group. There were no “contrasts” defined, so our posterior distributions for each group do not lend themselves to testing any particular hypothesis. What would be a more appropriate way to write this model? Amend your model from Question #1 with a more appropriate contrast. (Note that Bayesian models make it really easy to track variables of interest in the model rather than post hoc. Be sure to write your model so that you get a posterior distribution for the quantity [or contrast] of interest.) "],
["week-7-lecture.html", "11 Week 7 Lecture 11.1 Class projects", " 11 Week 7 Lecture Last week I hope I convinced you that Bayesian methods are preferred even for the very simplest problems you could do in your sleep using the skills learned in Biometry. In other words, Bayesian methods aren’t just for hard problems – they are for easy problems too. This week, we’ll continue working on relatively simple models, i.e. linear regression. By way of brief review, the basic problem to be addressed in simple linear regression is: \\[ Y \\sim N(\\mu,\\sigma^{2}) \\\\ \\mu = \\alpha+\\beta X \\] where Y is the response, and X is a (generally continuous) covariate that we think influences the value of the mean \\(\\mu\\). There are three parameters to be estimated here, \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^{2}\\), although the last is usually not of direct interest. Using JAGS to do simple regression is straightforward. Using the example from McCarthy, in which Tree Density was used as a covariate influencing the response of Course Woody Debris, the model part of the code looks like CWD[i] ~ dnorm(mu[i],prec) mu[i]&lt;-alpha+beta*TreeDens[i] That’s it! Since we are already familiar with the basics of regression from Biometry, and will get some practice with the models themselves on Wednesday. I’ll just go over a few points that differentiate Bayesian regression modeling from what you already know from Biometry. One such difference is in the process of predicting a value from the fit model or, in the same vein, getting a prediction interval from a regression model. Remember the horrible complicated formulas from Biometry? (If not, McCarthy provides a reminder in Box 5.2.) We don’t have to fuss with any of this using Bayesian methods because the posterior distributions for the parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^{2}\\) provide everything we need. What is the predicted value of CWD at TreeDens=1500? There are really two ways to do this. The first way is to plug in the posterior means for \\(\\alpha\\) and \\(\\beta\\) (which I’ll call \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)) as follows: \\[ \\widehat{CDW}_{\\mbox{mean predicted at 1500}} = \\hat{\\alpha} + \\hat{\\beta}*1500 \\] This approach is perhaps the most intuitive, but it is also wrong. Why? By collapsing the information in the posteriors for and to their posterior means, we ignore any correlations between the samples for and . Its better to sample from the posteriors directly to build up a posterior distribution for . In other words, just sample from the posterior of \\(\\alpha\\), sample from the posterior from \\(\\beta\\), and plug those samples into \\[ CDW_{\\mbox{mean predicted at 1500}} = \\alpha_{\\mbox{sample}} + \\beta_{\\mbox{sample}}*1500 \\] If you put this in a loop and do this, say, 1000 times, then you create a posterior distribution for predicted at TreeDens=1500. The posterior mean is the better estimate of _{}$. Not only that, but now you have more information, because you have the entire posterior distribution. Using this we can directly extract the 95th percentile credible interval. This is analogous to the from Biometry. In other words, the uncertainty here stems directly and only from the uncertainty in the parameters. To get a , we need to go one step further and draw samples from \\[ CDW_{\\mbox{predicted at 1500}} \\sim N(CDW_{\\mbox{mean predicted at 1500}},\\widehat{\\sigma^{2}}) \\] Just as before, we should sample from the posterior for \\(\\sigma^{2}\\), and not just use the posterior mean. We can do this in JAGS by just adding the calculation to the code; for each sample from the posteriors for \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^{2}\\), we can sample from this distribution. Since this is done at each iteration of the chain, we end up with a posterior distribution describing the prediction interval. It is generally far better to anticipate what you will want out of the model and have the model calculate this as it samples than to try and propagate all the uncertainities. Note that there is yet a third way of getting predictions, and that is to include those X values of interest as missing data in the original model fit. In other words, if you want a prediction interval for X=1500, add the data point (1500,NA) to the data. JAGS will use the model to fill in the missing data, which is precisely what a prediction interval is anyways. 11.1 Class projects For the rest of lecture, we will go over your project proposal. Keep in mind that your proposal is just that, a proposal. You are free to make changes as needed, just as you would with any research project. We’ll get into groups of three. Send your proposal to your two classmates so everyone has a copy of each proposal. Discuss each proposal in turn, using the following as guiding questions: Is this potentially publishable? (Hopefully, yes! Many of the projects from the last two Bayesian classes were published, and the goal should be to finish a publication-worthy analysis.) What is the model? (Specifically, with equations.) Does everyone understand the model? What data are available? What are the parameters of interest? Is there a specific hypothesis to be tested? What is the most appropriate distribution for the response data? Are there going to be missing data? Data can be imputed, so missing response data can be addressed. Is the focus on understanding past phenomena or predicting future behavior? Are there other papers in the literature (in your field) that have used similar methods? If not, why not? "],
["week-7-lab.html", "12 Week 7 Lab", " 12 Week 7 Lab This week we’ll be using a dataset on flower phenology that was published in PLOS ONE. The paper was assigned as reading to give you a brief overview of the data. (NB: The temperature data for 1894 was missing, and I have replaced it with the mean temperature from the other years just for the purposes of doing the lab.) This is as good a time as any to introduce some of the R packages designed to help you plot and evaluate MCMC chains produces by R2WinBUGS and RJAGS. Download the ‘coda’ package in R and read through the manual. Read through the blog post that I have posted on Bb which provides a quick overview. There is another newer package called ‘boa’ which is similar to ‘coda’ but I have less experience with it and will stick with the more conventional ‘coda’ package. Exercise: Write a script to use JAGS to fit a linear regression model to the flower data. In other words, write a JAGS script to fit the following model: \\[ Y_{i} \\sim N(\\alpha + \\beta*MAMTemp_{i},\\sigma^{2}) \\] where \\(i\\) is an index for a particular year. You can choose any flower you wish to focus on. (Copy and paste code, summary of parameters, and relevant plots.) Question #1: Is there a statistically significant relationship between mean temperatures and flowering date? (Note that, when doing a Bayesian analysis, we are not strictly involved in “hypothesis testing” (e.g., we have no null model, no p-values, etc.). However, in a loose mapping between Bayesian approaches and frequentists approaches, data analysts will often use overlap between the credible interval and zero [or other null value] as a measure of “statistical significance”.) Question #2: Plot the data, the best-fit regression line, and the confidence and prediction intervals. "],
["week-8-lecture.html", "13 Week 8 Lecture", " 13 Week 8 Lecture There is almost nothing special to say about Bayesian GLMs because the JAGS language makes it perfectly transparent what model is being used for the data and switching to something other than a normal is straightforward. Nevertheless, it’s a good opportunity to get some more practice with JAGS and review the basic principles of non-linear models/GLMs. First, we will start with a review of such models, a subject that many people find challenging the first time around (a.k.a. in Biometry). Let’s start with the model equation for a standard regression with normally distributed errors: \\[ Y_{i} \\sim N(\\mu_{i},\\sigma_{i}^{2}) \\\\ \\mu_{i} = \\alpha + \\beta X_{i} \\] There are two assumptions being made here. (1) a linear relationship between the covariate and the mean response and (2) normally distributed errors. You could imagine changing the first assumption without actually changing the second assumption. You could assume that there was some non-linear relationship between \\(\\mu_{i}\\) and \\(X_{i}\\) \\[ logit(\\mu_{i}) = \\alpha + \\beta X_{i} \\] or \\[ \\mu_{i} = exp(\\alpha + \\beta X_{i}) \\] or anything else for that matter. The point is that when we introduce GLMs, we tend to gloss over the fact that we are changing both the model for the errors and* the relationship between the model parameters and the covariates of interest. (Non-linear regression usually implies that we have kept a normal model but made the relationship between the mean and the covariate non-linear.) When we have a Bernoulli or Binomial response, we use a Bernoulli or Binomial model for the data \\[ Y_{i} \\sim \\mbox{Bernoulli}(p_{i}) \\\\ Y_{i} \\sim \\mbox{Binomial}(n_{i},p_{i}) \\] where \\(n_{i}\\) is usually assumed known in the latter case. Using either model, we need some way of relating the probability \\(p_{i}\\) to the covariates of interest. We are fairly constrained in ways to do this because \\(p_{i}\\) has to remain bounded (0,1). The logit transformation satisfies this requirement: \\[ logit(p_{i}) = \\alpha + \\beta X_{i} \\] We do this in JAGS with the following statement in the model Y[i] ~ dbern(p[i]) logit(p[i])&lt;-alpha+beta*X[i] Poisson regression works similarly. In this case, we have a response that is Poisson distributed with parameter \\(\\lambda\\), and we need some model that relates \\(\\lambda\\) to the covariates. In this case, \\(\\lambda \\ge 0\\) so we use the log link function \\[ log(\\lambda) = \\alpha + \\beta X \\] The JAGS code for this is analogous to before Y[i] ~ dpois(lambda[i]) log(lambda[i])&lt;-alpha+beta*X[i] Notice that, as pointed out in McCarthy, the MCMC samplers work better when the covariates are centered on their mean value. Its also true that the samplers are happiest when the numbers are smallish (1-10), so if you have large responses its better to divide your response Y by a constant, do the MCMC analysis, and multiply back by that constant at the end. While regular Poisson regression is fairly straightforward, there are many ways in which a dataset may be poorly fit by a Poisson. The two most common deviations from “poissonness” are overdispersion (Var&gt;Mean) and an excess of zeros. As discussed in Biometry, overdispersion and zero-inflation can be dealt with using “standard” likelihood (frequentist) methods. They can also be dealt with using a Bayesian approach. Note that zero-inflation of a Poisson can be thought of as a type of mixture model, whereby a Binomial controls whether there is presence (Abundance&gt;0) or absence (Abundance=0), and a Poisson is used to model Abundance conditional on presence. There are many ways to add variance to a Poisson regression. One option is to multiply \\(\\lambda\\) by a new parameter that varies among individuals or groups of individuals \\[ Y_{i} \\sim Pois(\\epsilon_{i}\\lambda_{i}) \\\\ log(\\lambda_{i}) = \\alpha + \\beta X_{i} \\\\ \\epsilon_{i} \\sim Gamma(\\theta,\\theta) \\] Another option is to add a random variable to the term for \\(log(\\lambda_{i})\\) \\[ Y_{i} \\sim Pois(\\lambda_{i}) \\\\ log(\\lambda_{i}) = \\alpha + \\beta X_{i} + \\epsilon_{i} \\\\ \\epsilon_{i} \\sim N(0,\\sigma^{2}) \\] A third option is to simply use a distribution that allows for Var&gt;Mean; the classic example here is the Negative Binomial, which is similar to the Poisson but with an extra parameter that controls dispersion (Var&gt;Mean only). Side note: The first and third options here are actually the same thing. A Poisson distribution with a parameter that varies as a Gamma distribution is actually just a Negative Binomial. The Negative Binomial is also known as the Gamma-Poisson mixture. A nice overview of this can be found here. Zero-inflation is another way in which a dataset may diverge from a simple Poisson (or Binomial, Gamma etc.). Zero-inflated Poissons can be dealt with as follows: \\[ \\omega_{i} \\sim \\mbox{Bernoulli}(\\theta_{i}) \\\\ Y_{i} \\sim \\mbox{Pois}(\\omega_{i}\\lambda_{i}) \\\\ log(\\lambda_{i}) = \\alpha + \\beta X_{i} \\] (NB: Strictly speaking, the Poisson parameter \\(\\lambda\\) has to be greater than 0, so you may need to code this as: \\(Y_{i} \\sim \\mbox{Pois}(\\omega_{i}\\lambda_{i} + 0.0001)\\). Note that you are often interpreting the two components of this differently. In an ecological context, you might say that the excess zeros represent sites that are unsuitable for occupancy and the Poisson represents the abundance at sites that are suitable (here I have represented abundance as having a covariate “X”). Keep in mind, however, that the Poisson also yields zeros, and so it is critical not to interpret all the zeros as coming from unsuitable habitat. Sometimes a habitat is suitable and there is no abundance, and the Poisson deals with this just fine. Side note: We can also have truncated Poisson distributions. For example, let’s say that you are studying litter size. We only have a litter size if there is a litter in the first place, so we don’t have litter sizes of zero. In these cases we have to truncate the Poisson so as to yield only Y&gt;0. Side note #2: Question #3 on the problem set asks you to model a parameter of a statistical model as coming from a distribution. This is a simple example of what is known as hierarchical modeling. It sounds super complicated and fancy but its really not. We are simply nesting statistical models within other statistical models. This is why many statisticians, particularly those using Bayesian approaches like this, avoid the terms “fixed effects” and “random effects” (see Gelman and Hill, posted on Bb). By “fixed” effects, all we are saying is that we are not going to model variation as any higher hierarchical level. “Random effects” simply means that we are going to model variation at another hierarchical level. Gellman and Hill argue for the terms “modeled” and “unmodelled” variation, which I prefer over fixed vs. random. It is only because frequentists methods are so awkward dealing with hierarchical variation that we make such a fuss over fixed vs random in Biometry. Using Bayesian methods its not a big deal at all. "],
["week-8-lab.html", "14 Week 8 Lab", " 14 Week 8 Lab In lab we are going to work through several examples of fitting GLMs using data from the breeding bird survey program. The data you have been given (BBS data for NY.csv) is, as the name implies, the summary of BBS data for the state of NY. For those not familiar with the BBS data, what we have are essentially counts of individuals recorded along transect surveys. The first two columns of the dataset are the Year and the number of Routes completed (which is related to effort). The other columns are for each species recorded in NY state. Exercise #1: We are going to do a traditional Poisson regression to look at the change in abundance of the Northern Bobwhite: \\[ Count_{i} \\sim \\mbox{Pois}(\\lambda_{i}) \\\\ log(\\lambda_{i}) = \\alpha + \\beta*Year_{i} \\] I suggest centering the Year covariate by subtracting off some middle value, like 1990. (Doesn’t matter, just have to back transform when you plot the results.) To illustrate the model fit, we will plot four things: The original data. The best fit line, which we find by taking the mean of \\[ exp(\\alpha[k]+\\beta[k]*Year_{i}) \\] where \\(k=1,2,\\dots,\\mbox{number of posterior samples}\\). The Bayesian credible interval, which we find by taking the (2.5,97.5) quantiles of \\[ exp(\\alpha[k]+\\beta[k]*Year_{i}) \\] The Bayesian prediction interval, which we find by taking the (2.5,97.5) quantiles of \\[ \\mbox{Pois}(exp(\\alpha[k]+\\beta[k]*Year_{i}) \\] If you plot these draws as little tiny dots (pch=”.”) than you can see the spread of predicted values and the data at the same time. Exercise #2: In the same spirit of last week’s problem set, we will now model a larger number of species. Since the entire dataset might take some time to fit, let’s go with all the species up to and including the Northern Bobwhite (so the first 17 columns). We will, for the time being, assume completely independent parameters for each species. \\[ Count_{ij} \\sim \\mbox{Pois}(\\lambda_{ij}) \\\\ log(\\lambda_{ij}) = \\alpha_{j} + \\beta_{j}*Year_{i} \\] (\\(i\\) is the Year, \\(j\\) is the species) Plot the data and a sample of the results (enough to convince yourself the code works). Question: What is the difference between this and doing 17 different regressions? Exercise #3: We will now do a hybrid between the first two approaches, by modeling the species-specific slopes as being drawn from a distribution. This is a classic hierarchical Bayesian problem. \\[ Count_{ij} \\sim \\mbox{Pois}(\\lambda_{ij}) \\\\ log(\\lambda_{ij}) = \\alpha_{j} +\\beta_{j}*Year_{i} \\\\ \\beta_{j} \\sim N(\\mu_{\\beta},\\sigma_{\\beta}^{2}) \\] Gelman and Hill’s description of such models is quite good. What we have done in going from the second model and the third is reduce the number of parameters from 34 (17 for \\(\\alpha\\) and 17 for \\(\\beta\\)) to 19 (17 for \\(\\alpha\\) and 2 for \\(\\beta\\)). Plot the data and a sample of the results (enough to convince yourself the code works). Exercise #4: So far we have been ignoring variation in effort across years. One way of accounting for this is to use an offset, as described in Kery Section 14.3. Using the number of Routes instead of Area (see Kery page 190), include log(Route.Count) as a covariate to your hierarchical model from Exercise #3. How are the coefficients to be interpreted when including this new term? Does including effort as a covariate change the biological interpretation of your results? "],
["week-9-lecture.html", "15 Week 9 Lecture 15.1 The probability of estimability 15.2 Multilevel modelling ala Gelman and Hill 15.3 In sum…. 15.4 Nice et al. (2014)", " 15 Week 9 Lecture Hierarchical models are simply models in which model parameters themselves have a statistical distribution. Another definition would be the one provided by Gelman and Hill “…multilevel models are extensions of regression in which data are structured in groups and coefficients can vary by group”. We were introduced to the simplest version of hierarchical models in Biometry when we discussed random effects models. Random effects models are hierarchical models, but the latter term encompasses much more complicated models than can be fit using ‘lmer’ or related functions in R. Let’s say that you are modeling survivorship of fish in five different watersheds \\((i = 1,2,3,4,5)\\). \\[ S_{i} \\sim \\mbox{Bern}(p_{i}) \\] You have three choices: Model each of the five watersheds independently. You have to estimate a survivorship probability \\(p_{i}\\) for each of the watersheds. This is analogous to treating ‘watershed’ as a ‘fixed effect’. (I use this terminology now but for reasons discussed in Gelman and Hill, I will drop these terms once you understand the basic idea behind hierarchical modeling.) Lump all the data together, ignore any differences between watersheds, and estimate just one survivorship probability \\(p\\) for the aggregated dataset. Model the parameters as belonging to some distribution, for example, maybe a normal distribution \\[ logit(p_{i}) \\sim N(\\mu,\\sigma^{2}) \\] Now you have to estimate two parameters, \\(\\mu\\) and \\(\\sigma^{2}\\), which is fewer than Option #1 but less than Option #2. There are many advantages to modelling parameters as being drawn from a common distribution, in addition to the issue of the number of model parameters just described. One such advantage is the idea of “borrowing strength” from groups with large sample sizes to inform estimates of groups with small sample sizes. Let’s say you have one watershed with only three fish data points. Using Option #1, we would have to estimate survivorship using only these three fish. Using Option #3, however, we can constrain the watershed-specific survivorship estimate by saying that it has to fall along some distribution; this hyperparameter distribution may be quite well parameterized by the other four watersheds, and this keeps the estimate for the poorly sampling watershed reasonable. 15.1 The probability of estimability Hierarchical models are often easy to write down, but sometimes they can be difficult to fit. Parameters in a hierarchical model may become entangled: likelihoods can become ridge-shaped or multimodal. Example: \\[ Y_{i} \\sim N(\\theta_{i},\\sigma^{2}) \\\\ \\theta_{i} \\sim N(\\mu,\\tau^{2}) \\] What’s wrong with this? This is equivalent to \\[ Y_{i} \\sim N(\\mu, \\tau^{2}+\\sigma^{2}) \\] No amount of data is ever going to allow you to separately estimate \\(\\tau\\) and \\(\\sigma\\). However, the fact that \\(\\tau\\) and \\(\\sigma\\) have prior distributions does mean that you will technically be able to run these models, and you will get posterior distributions for \\(\\tau\\) and \\(\\sigma\\) - you just have to be aware that the data are not contributing to the inference regarding the balance between \\(\\tau\\) and \\(\\sigma\\) in structuring the overall variability in the data. In more complex examples, you may not even be aware that parameters are non-estimable (a.k.a. that the data are not contributing to their estimates). One common way to check if this is happening is to compare the prior to the posterior. If the posterior looks just like the prior, then the data probably hasn’t provided much information. (Though this isn’t fail safe – sometimes the posterior “looks” different from the prior as a statistical artifact of marginalization over another parameter.) 15.2 Multilevel modelling ala Gelman and Hill No paper or book does a better job explaining the ins and outs of “multilevel” or “hierarchical” modelling than Gelman and Hill (2007). If you foresee doing any kind of serious hierarchical modelling for your research, I can’t recommend more highly buying (and reading) Gelman and Hill. Today we will focus on Chapters 11 and 16. I’m going to walk through the discussion presented in Gelman and Hill Chapter 11 but using a more ecological example. Let’s say we want to study flowering phenology (a concept we should all be fairly familiar with by now), and we do this by measuring the timing of flowering in 100 plants sampled in each of five New York ecosystems. (In other words, we measuring flowering in 100 plants in the Pine Barrens, another 100 plants in the Adirondacks, etc.). We may have some individual-level predictors such as latitude or spring temperature. In other words, these are predictors that are different for each and every individual (n=500) in the study. We have many ways that we could model these data. One way would be a “varying-intercept model”: \\[ y_{i} = \\alpha_{j[i]} + \\beta x_{i} + \\epsilon_{i} \\] (The notation \\(j[i]\\) indicates that this is the group \\(j\\) in which individual \\(i\\) is found.) How do we read this? This says that the flowering date of each plant (\\(i\\)) is modelled as a linear function of a group-level intercept and an individual-level covariate (\\(x_{i}\\)) with a shared coefficient \\(\\beta\\) (and an individual-level error term). In other words, the full model involves 5 intercepts (one for each ecosystem) and one slope term. Let’s assume for now we simply estimate these 5 intercept terms as “fixed effects”, and that we do not assume a higher level model such as \\(\\alpha_{j[i]} \\sim N(\\mu,\\sigma^{2})\\). [NB: There would be nothing wrong with doing this if we thought it was appropriate.] We could have used a “varying slope” model: \\[ y_{i} = \\alpha + \\beta_{j[i]}x_{i} + \\epsilon_{i} \\] Now we would have a single intercept and 5 slopes. The interpretation of this model would be that the relationship between phenology and latitude depends on ecosystem. Make sure this makes sense! We could have used a “varying-intercept, varying-slope” model: \\[ y_{i} = \\alpha_{j[i]} + \\beta_{j[i]}x_{i} + \\epsilon_{i} \\] Now we would have 5 intercepts and 5 slopes. Let’s consider for a moment a varying slope model like this \\[ y_{i} = \\alpha + \\beta_{j[i]}x_{i} + \\epsilon_{i} \\] where we want to further model the slopes as coming from a normal distribution: \\[ \\beta_{j[i]} \\sim N(\\mu, \\sigma^{2}) \\] Why might we want to do this? We may want to do this because we have some underlying theory that suggests that the slopes follow a normal distribution. We might want to protect ourselves from getting unreasonable slope estimates for groups that have small amounts of data. (In this case, we posited 100 samples from each ecosystem, so not explicitly a concern.) We might want to explicitly estimate some common “mean slope” that averages together the responses of each individual ecosystem. This allows us to estimate the slope for each ecosystem and get a measure of “average slope” at the same time, and even a measure of variance between ecosystems. Let’s say this is the model we wanted to fit. You might ask yourself, why not fit the first model \\[ y_{i} = \\alpha + \\beta_{j[i]}x_{i} + \\epsilon_{i} \\] and then afterwards simply fit a normal distribution to the resulting slope estimates. The reason this is NOT equivalent is that there is no way for the constraint on the slopes to “filter back up” to the slope estimates themselves. Make sure this makes sense! Doing a model like this in two stages is called “doing statistics on statistics” and it is generally frowned upon. Why do two models that don’t interact when you could do everything in one integrated model that captures all the information you want simultaneously. So far we have assumed that the covariates we have are specific to each individual, for example, the exact latitude of each plant. But it may be that covariates come in at the ecosystem level, like species diversity. (I’m not saying I know how species diversity might affect flowering phenology, its just an example of a covariate that comes in at the ecosystem level.) Why does this require special consideration? Let’s say that the first few rows of our data might look like FloweringDay | Ecosystem | Latitude | Ecosystem Species Diversity | | | (Individual level) | (Group level) | ——– —- |:—————:|:——————:|:———–:|:————–:| 97 | PineBarrens | 40.78 | 120 | | 95 | PineBarrens | 40.65 | 120 | | 103 | PineBarrens | 40.62 | 120 | | 107 | PineBarrens | 40.81 | 120 | | 92 | PineBarrens | 40.60 | 120 | | 101 | Adirondacks | 44.13 | 235 | | Notice that if we use the varying-slope model, we shouldn’t write the model like this: \\[ y_{i} = \\alpha + \\beta_{j[i]}Lat_{i} + \\gamma Diversity_{i} + \\epsilon_{i} \\] but like this: \\[ y_{i} = \\alpha + \\beta_{j[i]}Lat_{i} + \\gamma Diversity_{j[i]} + \\epsilon_{i} \\] Do you understand why? Because the former model involves a type of pseudoreplication, whereby the model assumes that each individual has a unique Diversity whereas this predictor comes in at the ecosystem level. The second way of the writing the model is correct. *Multilevel modelling simply refers to models in which covariates (a.k.a. predictors) enter the model at different levels. Some predictors will be individual-based predictors, some will be at the group level (there may be many “groups”). *Hierarchical modelling refers to models in which the coefficients (i.e. the things you are trying to estimate) are modelled as having additional distributional assumptions. In other words, there is a hierarchy of models, a model for the data, a model for the parameters, possible a model for the parameters of THAT model, and so forth. Multilevel modelling and Hierarchical modelling are not the same thing, nor are they unique to Bayesian modelling, but they often go hand in hand because multilevel models are often also hierarchical models. Bayesian methods provide a straightforward approach to these types of models, but by no means the only approach. Before moving on, we should be clear that multilevel models are not constrained to nested structures. Imagine for example that you want to add random effects for Ecosystem and for flower “Family” (as in Species, Genus, Family). Family will not be nested in Ecosystem, nor will Ecosystem be nested in Family. That’s OK! Random effects for Ecosystem and Family present no problem at all. \\[ y_{i} = \\alpha+\\beta_{j[i]}\\mbox{Lat}_{i} + \\gamma \\mbox{Diversity}_{j[i]} + \\mbox{Ecosystem}_{j[i]} + \\mbox{Family}_{k[i]} + \\epsilon_{i} \\] (Just to summarize, at this point we have a single intercept for all plants, a slope for Latitude that depends on which Ecosystem you are in, a random effect for Ecosystem, a random effect for Family, and a residual term.) 15.3 In sum…. We have a lot of freedom to build models in whatever way makes the most sense for the data. We can choose to pool coefficients across groups and have a single coefficient or we can allow each group to have its own coefficient. We can do something intermediate, and enforce a distribution for the coefficients so that each individual or group is allowed to have variation for that coefficient but that variation is modelled as following some distribution with its own parameters to estimate. We can and should model predictors on the scale at which they are measured (individual-level, group-level, etc.). There is no “correct” model. As a final ‘capstone’ application of Bayesian modeling, we are going to work through analyses by Nice et al. (2014). 15.4 Nice et al. (2014) The goal: To understand how weather affects the population dynamics of butterflies, both at the individual species level and also at the community level. Challenges: Traditional methods make it difficult to assess fluctuations at the species level and the community level simultaneously. Data: Observations on butterfly presence and absence over 38 year period at Donner Pass. (Response variable was the number of “Day Positives” – the number of days in which a species was recorded as present – treated as a Binomial variable.) GLM model for “Day Positives” \\[ DP_{ij} \\sim \\mbox{Binomial}(p_{ij},\\mbox{Visits}_{j}) \\] where \\(i\\) = species and \\(j\\) = site. \\[ logit(p_{ij}) \\sim \\mu_{\\mbox{species}_{i}} + \\beta_{1species_i} \\times \\mbox{covariate}_{1_ij} + \\beta_{2species_i} \\times \\mbox{covariate}_{2_ij} + \\dots \\] Species-specific intercepts and coefficients were modeled as \\[ \\mu_{\\mbox{species}_i} \\sim N(\\mu_{\\mu},\\tau_{\\mu}) \\\\ \\beta_{\\mbox{species}_i} \\sim N(\\mu_{\\beta},\\tau_{\\beta}) \\] The parameters \\(\\mu_{\\mu}\\),\\(\\mu_{\\beta}\\) represent the mean community-level response, and \\(\\tau_{\\mu}\\),\\(\\tau_{\\beta}\\) represent how much variation there is at the species level (i.e. between species variation \\(=1/\\tau\\)). What are the authors talking about in the paragraph starting with “We then asked if differences among species…” on page 2159? How do the authors represent their results? See Figures 1 and 2. Which approach do you like better (Figure 1 or 2)? Why? Note that in Table 1 and Figure 3 (and elsewhere), the authors have used the median and not the mean to represent the central posterior value. Why might they have done that? "],
["week-9-lab.html", "16 Week 9 Lab", " 16 Week 9 Lab This week, we’re going to spend our lab time together working on our projects, and thinking about how/if to use a hierarchical modeling structure. If spatial or phylogenetic structure is going to be important, now is a good time to work out how to do that. By the end of this week, you should be well on your way with your project, with the data sorted and an initial model running in JAGS. "],
["week-10-lecture.html", "17 Week 10 Lecture 17.1 Convergence 17.2 Testing for convergence 17.3 Gelman-Rubin statistic 17.4 The take away: What should we be checking after we run our models 17.5 Missing data 17.6 Initial values 17.7 Sample scripts and output for prior-posterior overlap", " 17 Week 10 Lecture This week we’ll take some time to discuss the messy, annoying computational details of doing Bayesian analyses. Before launching into this, its worth stepping back and reviewing a bit about Analysis of Variance, since the same ideas are going to be useful to us today. Let’s say I have Normally distributed data on heights and I have three groups of individuals; I may want to ask whether these groups are different in height (on average). At first blush, I could simply take the average height in each group and compare them to each other, but this isn’t a good test. Why? Because if there is any variability in heights, then I do not know whether these group means are different because of random sampling error (i.e. I took a sample from each group and my sample may not be representative of the underlying population because there is some variance and maybe my sample has a few smaller individuals, etc.), or whether they are different because the groups really are fundamentally different in height. This is what we have to decide using the data we have. We do this by computing within-group variance using two methods. Method #1: The simplest way of estimating within group variance is just to calculate it. In other words, calculate the variance for each group individually. If we have 3 groups, we now have 3 estimates of within group variance. To get an average estimate of within-group variance, we can just average those estimates together. More formally: \\[ \\sigma^{2} = \\frac{1}{m}\\sum_{j=1}^{m}(\\mbox{variance in group j}) \\] This works as an estimate of within-group variance even if the groups are different from one another. We could have a group of very tall individuals and a group of shorter individuals, but we could still estimate within-group variance by looking at the variation within each group and then averaging them together. If the groups are not different (i.e., under \\(H_{0}\\)), then all the individuals sampled really do come from the same underlying population, regardless of group, and this gives us a second method of estimating within-group standard deviation. Recall that the Central Limit Theorem states that the variance in the mean is related to the variance in the data as \\[ \\sigma^{2}_{\\bar{Y}} = \\frac{\\sigma^{2}}{n} \\] More samples (\\(n\\)) means we have a more precise (smaller \\(\\sigma_{\\bar{Y}}\\)) estimate of the mean \\(\\bar{Y}\\). We can flip this around to say \\[ \\sigma^{2} = n\\sigma^{2}_{\\bar{Y}} \\] This says that if we know the variance among the MEANS we can use this to estimate the within-group variance. This only works when all the comes come from the same distribution. When doing ANOVA, we compare these two measures of within-sample variance to TEST whether the samples in different groups come from the same population. But the basic underlying idea, that we have variation within each group, and variance between groups, and that variance between groups can be used to estimate variance within groups (under \\(H_{0}\\)) is an idea we will need to use again in the context of chain convergence. 17.1 Convergence When we talk about a Bayesian model converging, we usually mean that our posterior distributions have adequately sampled the (correct) posterior distribution, and that we have enough samples from that posterior distribution to precisely estimate the parameters of our model. There are many metrics that we might use to assess our MCMC chains but they generally fall into two categories. Estimates of Monte Carlo error: Monte Carlo error refers to the uncertainty in our parameter estimates that stems from having a finite number of samples from the posterior. Testing convergence: \\(\\hat{R}\\) and \\(n_{eff}\\) Before getting further, I want to make sure that everyone understands the distinction between two types of error. This is best illustrated with an example. Lets say that I have a simple binomial model \\[ Y_{i} \\sim \\mbox{Binom}(N,\\theta) \\] The posterior distribution \\(\\theta|Y_{i}\\) will have some “spread”, as is easily seen by a histogram of the posterior samples, and the standard deviation of the posterior samples is a measure of that uncertainty. As long as we have adequately sampled the posterior distribution, that standard deviation will be insensitive to sample size. In other words, we can run our chains forever and that spread of the posterior distribution won’t change, because that spread reflects the “true” uncertainty about the parameter based on the prior and the data available. The parameter estimate we will report in the paper (i.e. the quantity of interest) will usually be the mean or median of that distribution, and we will refer to this measure of central tendency as \\(\\hat{\\theta}\\). If we use the posterior mean as our measure of central tendency, then \\[ \\hat{\\theta} = E[\\theta|Y_{i}] \\] Unlike the standard deviation of the samples for \\(\\theta|Y_{i}\\), our uncertainty regarding \\(\\hat{\\theta}\\) (which we call the Monte Carlo standard error) is sensitive to sample size. As we collect more samples, we will more precisely estimate \\(\\hat{\\theta}\\), and our uncertainty about will decrease. This situation is analogous to the difference between the standard deviation of a distribution, and the standard deviation of the mean of a distribution, the latter of which we refer to as the standard error (of the mean). In most applied applications of Bayesian modelling, Monte Carlo standard errors are not of interest and will be far smaller than the standard deviation of the posterior. In other words, given the Binomial model just discussion, you would report in your paper “$=\\(3.5 (2.1,4.7)&quot;. You would not need to specify the posterior mean to more digits &quot;\\)=$3.547834 (2.1,4.7)” because the extra precision is made irrelevant because of the spread of the posterior. Technically, you should also report the uncertainty associated with the estimate of the posterior mean (in other words, how well do you know that the mean is 3.5 and not 3.4 or 3.6) but this is not usually done in the manuscript, rather the error of the mean is usually included in a table (such as in the summary table produced by JAGS). [Keep in mind that we can ask about the MCSE for any function of any parameter, such as a function \\(\\widehat{g(\\theta)}\\).] OK, now we know what Monte Carlo standard errors are, but how do we calculate them? If the N samples were all independent, then we could easily calculate the MCSE by just using the Central Limit Theorem. \\[ \\mbox{MCSE} = \\sqrt{\\frac{s^2}{N}} = \\sqrt{\\frac{\\frac{1}{N-1}\\sum_{i=1}^{N}(g(\\theta_{i})-\\overline{g(\\theta_{i})})^{2}}{N}} = \\frac{SD(g(\\theta_{i}))}{\\sqrt{N}} \\] However, the samples are not usually independent, and in fact, Markov Chain samples can be highly autocorrelated. Therefore, we use the way of calculating the MCSE, which is to divide up our chain into \\(m\\) batches of length \\(n\\) (\\(N=mn\\)); while the individual draws may be highly correlated, we hope that batches of draws will be independent. (There are many ways to estimate the Markov Chain standard error but here I am describing the batch means method.) Now we can write \\[ s^{2} = \\frac{n}{m-1}\\sum_{j=1}^{m}(\\overline{\\theta_{. j}}-\\overline{\\theta_{..}})^{2} \\] without concern for the autocorrelation of individual samples. 17.2 Testing for convergence Theory cannot tell us how long it will take to get our chains to converge. So far we’ve done simple examples that converge after only a few thousand iterations, and take only seconds to run. In practice, JAGS may take days or even weeks of processing, even with parallel chains, to converge. You may never get it to converge. If you are writing your own samplers from scratch, there are a lot of issues you have to deal with, but using JAGS, your problems are fewer. One of the few controls you have on the actual posterior is to decide how much to thin the chains. It is always better to keep everything. However, if you have to run millions of chains, you are usually better off only storing some fraction of those samples. You can thin out by selection one out of every two, one out of ten, etc. This keeps the size of the MCMC output reasonable even if the chains are very long, and because the samples are often highly correlated, it involves minimal information loss. How do we assess convergence? Many Bayesian practitioners would argue the best approach is simply to compare the performance of multiple chains. With parallel processing, this often doesn’t take any longer than running a single chain, especially so because even basic desktop computers have multiple cores. The idea is that if multiple chains converge on the same distribution, then you must have found the correct distribution. However, Geyer (in Brooks et al. 2011) cautions about pseudo-convergence. What is pseudo-convergence? Consider a situation in which the distribution you are trying to sample from (but don’t know) has a 99.9% probability of being Unif(0,1) and a 0.1% probability of being Unif(1000000,1000001). Under these circumstances, assuming a fairly typical jump proposal such as N(0,1), you might start all of your samplers so that they end up sampling only the portion that is Unif(0,1). For a chain of any reasonable length, you might never propose a jump to the second portion of the distribution Unif(1000000,1000001), and so your chains will appear to converge on Unif(0,1) and you would have no clue that there was in fact support along some other interval. Running multiple chains will not have helped identify convergence because, in essence, they were not ‘overdispersed’. The problem is that without knowing the distribution, you have no way of knowing what constitutes overdispersed, so you have no way of choosing starting values that are guaranteed to sample the entire space. As noted by Geyer, this is a problem with all “black box” samplers. Gelman and Shirley (in Brooks et al. 2011) are less nihilistic in their advice on convergence, and they offer some more practical suggestions. The most important suggestion in to simulate fake data and fit the model to convince yourself that a) the model works as expected and b) that the posterior intervals are sensible (i.e., 50% of the 50% posterior intervals should contain the true value). Whenever possible, its critical to use fake data to confirm that your code works. This is well worth the time invested. I also like Gelman and Shirley’s (slightly cheeky) advice to run the chains from the point of submission until the reviews come in. It’s actually kind of brilliant. 17.3 Gelman-Rubin statistic There are many ways to try and assess whether a chain has converged. Of course, we never know if this is pseudo-convergence, but this is an unknown unknown that non-statisticians don’t usually obsess over (unless, of course, the posterior is pathological or the chains are very slow to mix…). Here I will present just one formal test, the Gelman-Rubin diagnostic, which I like because it’s quite intuitive and is a direct analog to an analysis of variance. (It is also well-known, and likely to pass through peer review without a second glance…) The G-R statistic requires that you run multiple chains starting with “overdispersed” starting values (in practice, people just pick widely different starting values; more correctly, I would fit your model, draw overdispersed values from those posteriors, and run it again with those starting values). If you have chains, each of length , then the G-R statistic is \\[ B=\\frac{n}{m-1}\\sum_{j=1}^{m}(\\overline{\\theta_{. j}}-\\overline{\\theta_{..}})^{2} \\\\ W=\\frac{1}{m}\\sum_{j=1}^{m}\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}(\\overline{\\theta_{ij}}-\\overline{\\theta_{.j}})^{2}\\right] \\] where \\(\\theta_{ij}\\) is the ith sample from the jth chain, \\(\\overline{\\theta_{.j}}\\) is the average over the jth chain, and \\(\\theta_{..}\\) is the global average (over all samples in all chains). Using these values, we can estimate the marginal posterior variance of \\(\\theta\\) \\[ \\widehat{Var(\\theta|y)} = \\frac{n-1}{n}W + \\frac{1}{n}B \\] which can be used to calculate the Gelman-Rubin statistic \\[ \\hat{R} = \\sqrt{\\frac{\\widehat{Var(\\theta|y)}}{W}} \\] If \\(\\hat{R} \\approx 1\\), at least &lt;1.1, then we can be fairly confident that the chains have converged. To gain a little intuition for this statistic, consider the case of very large (which is always the case, since the chains are always long) \\[ \\hat{R}_{\\mbox{n large}} \\approx \\sqrt{B/n}{W}} \\] which is basically like comparing two estimates of the variance, one derived from the variance between chains (divided by n, think back to how we got SE from SD!) and one derived from the variance within a chain. The effective sample size \\(n_{eff}\\) is a related measure to \\(\\hat{R}\\). 17.4 The take away: What should we be checking after we run our models Step 1: The first step in fitting any kind of “mission-critical” model is to simulate data and to ensure that you can recover your parameter values. If some parameters are recoverable, but others are not, this might suggest a reworking of the model to eliminate non-identifiable parameters. Step 2: Once you’ve run your model (on simulated data, as in Step 1, or later with real data), you will want to check convergence. Visual inspection of the chains Look at \\(\\hat{R}\\) for all parameters, esp. those of key interest Step 3: Do a bivariate scatterplot of your key parameters. In other words, make scatterplots which plots the samples of each parameter against the samples from the other parameter. Doing so will highlight parameters that might be “trading off” (i.e. large values of one parameter are associated with small values of another parameters, etc.). If parameters are trading off, it may be because there is some combination of these that is identifiable (the product or the ratio) but that the parameters individually are not identifiable. If you have parameters that are trading off or whose scatterplots show non-independence between parameters, you will need to summarize your posterior distribution by a confidence envelop rather than through the two marginal posterior distributions, since the latter (while not strictly incorrect) hides the inherent correlations between the two parameters and may lead you to over- or underestimate uncertainty (similar to Bolker’s likelihood slices vs likelihood profiles in Biometry). Step 4: Plot prior-posterior overlap for each key parameter of interest. Too much overlap strongly suggests that your data are simply not that informative about a certain parameter, which is why your posterior looks very similar to the prior. Some code to do that and some example output are appended at the end of the lecture notes. 17.5 Missing data Bayesian analyses treat data as being sampled from a statistical distribution, so missing response data poses no problem. Missing covariate data, on the other hand, poses more of a problem, and our choices are to either discard rows of our dataset which contain missing covariates (very sad) or impute missing covariates. Unless your dataset is very large, I recommend against deleting data whenever possible. Bayesian methods are often used when datasets are small anyways, and it’s better to rescue elements of your dataset that make your dataset smaller than it has to be. First we need to distinguish between four types of missing: Missing completely at random (probability of being missing is the same for all units) Missing at random (probability of being missing depends only on available information) Missingness depends on unobserved predictors Missingness depends on the missing value itself It is important to determine what kind of missingness you have. Question: Any examples of missing data from your research? What kind of missingness are you dealing with? A sample of imputation methods: 1. Replace missing covariate value with the mean of that covariate across all units. 2. Last value carried forward (sensible if you have a time series or something with strong autocorrelation). 3. Sample with replacement from the other values for that covariate. 4. Fit a model to the observed cases and use that model to predict the missing covariate values. a) Can use the expected value of the model for imputation b) Can include prediction error (in this case, as with any situation in which there is some stochasticity to the imputation procedure, it is best to do this multiple times and assess sensitivity of the results to the imputed values) Some vocabulary: Hot-deck vs. cold deck imputation. Hot-deck imputation is when a missing value is imputed by finding one or more “similar” cases without missing data in the dataset under analysis. Cold-deck imputation uses a similar matching procedure but with data that were collected previous to the current analysis. 17.6 Initial values We tend to ignore the initial values, though we know that we should be starting our MCMC chains with three “random” widely dispersed values. How do we define widely dispersed? Usually with respect to the prior distribution. BUT in some cases we might be using a flat prior, and drawing random values from a flat prior might make it difficult for the model to converge, or it will take a very long time to converge. I suggest a two part approach for “the final run” of models that are nearing publication-readiness. First, try running the model by drawing randomly from the prior; hopefully this works OK. If not, you’ll have to hand select initial values from values that you think are more reasonable starting values. Run the model, see how it does. Even if it doesn’t reach full convergence, use draws from the posterior as new initial values for a second run. Constrain these starting values to ensure that at least one value is drawn from each side of the posterior mode. In other words, if the posterior is centered on zero, make sure you have at least one positive and at least one negative starting value. Now you’ll be starting values that are dispersed (by design) and shouldn’t be TOO far off reasonable even if you stick with the original flat prior. 17.7 Sample scripts and output for prior-posterior overlap plot.function=function(prior.vec,posterior.vec,overlap) { prior &lt;- data.frame(values=prior.vec) posterior &lt;- data.frame(values=posterior.vec) prior$name &lt;- &#39;prior&#39; posterior$name &lt;- &#39;posterior&#39; Results &lt;- rbind(prior, posterior) binsize&lt;-(max(Results$values)-min(Results$values))/50 ggplot(Results, aes(values, fill = name)) + geom_histogram(alpha = 0.5, binwidth=binsize, aes(y = ..density..), position = &#39;identity&#39;) + ggtitle(paste(as.character(round(overlap,digits=2)),&quot;% overlap&quot;,sep=&quot;&quot;)) } calc.overlap=function(prior.vec,posterior.vec) { prior &lt;- data.frame(values=prior.vec) posterior &lt;- data.frame(values=posterior.vec) prior$name &lt;- &#39;prior&#39; posterior$name &lt;- &#39;posterior&#39; Results &lt;- rbind(prior, posterior) binsize&lt;-(max(Results$values)-min(Results$values))/50 s&lt;-seq(min(Results$values),max(Results$values),by=binsize) overlap&lt;-vector(length=length(s)) for (i in 1:(length(s)-1)) { prior.num&lt;-sum(as.numeric((prior.vec&gt;s[i])&amp;(prior.vec&lt;s[i+1])))/length(prior.vec) posterior.num&lt;-sum(as.numeric((posterior.vec&gt;s[i])&amp;(posterior.vec&lt;s[i+1])))/length(posterior.vec) overlap[i]&lt;-min(prior.num,posterior.num) } percent.overlap=sum(overlap) return(percent.overlap) } In this case, the data were informative about the first of these parameters but not about the second. Figure 3.1: Histogram of prior distribution (blue) and posterior distribution (pink). Figure 8.1: Histogram of prior distribution (blue) and posterior distribution (pink). Here is an example of where the prior was centered on the posterior mean but the data were still clearly informative: Figure 8.2: Histogram of prior distribution (blue) and posterior distribution (pink). "],
["week-10-lab.html", "18 Week 10 Lab", " 18 Week 10 Lab On Monday, we introduced the idea of data imputation. As a reminder, the real issue here is when we have missing covariates. In a Bayesian context, missing responses are easy handled without any extra effort because the data are assumed to have been derived from a sampling distribution anyways, so missing data are simply sampled at each iteration. On the other hand, we need to think a bit more carefully about missing covariates, and in lecture we discussed several strategies for doing so. To gain some practice with various methods for data imputation, we are going to use the phenology dataset from Week #7. As it was, I imputed a missing value for 1894 for Week #7. Not only will we replace that missing data with an “NA” but we will further degrade the dataset by removing other Temperature values. *The 1894 value should be permanently removed for all exercises. We will create 4 degraded datasets. Degraded dataset #1 (Missing completely at random): Remove 9 values completely at random (and replace with “NA”). Degraded dataset #2 (Missing at random): Assume missingness declines over time. \\[ \\mbox{Missing} \\sim \\mbox{Bernoulli}(p) \\\\ \\mbox{logit}(p) = -0.09*year \\] You can ensure that you get 9 sampled values by passing your vector of years to the “sample()” function in R and use the probabilities calculated above as weights to that sample function. Note that the sample() function will automatically reweight the probabilities as needed, and using the sample() function you can specify that you want to select 9 Years. (In other words, sample() is just telling you 9 years to select.) Degraded dataset #3 (Depends on unobserved predictors): Assume all odd numbered years Blue Hill was run by an idiot, and those data have a 75% chance of missingness, and all even numbered years have a 0% chance of missingness. Degraded dataset #4 (Depends on itself): Assume the following model: \\[ \\mbox{Missing} \\sim \\mbox{Bernoulli}(p) \\\\ \\mbox{logit}(p) = -0.35*TempMAM \\] You can use the same basic procedure as above, using the sample() function. We will use four methods of imputation on each of the four degraded datasets. Method #1: Replace with the mean Method #2: Sample with replacement Method #3: Fit a model to the observed cases (you are free to choose the model, keeping in mind that the responses [flowering dates] can be used as predictors when imputing covariates; how might we use multivariate techniques here?) and use the expected value for imputation. If you were using Flowering Date to model MAMTemp, your model would look like \\[ \\mbox{MAMTemp} \\sim N(\\beta_{0} + \\beta_{1}*FD,\\sigma^{2}) \\] Note that you need not use the same flower to impute MAMTemp as the flower you are actually focused on. In other words, let’s say that Amelanchier canadensis is the focus of your phenology study. To impute missing MAMTemp covariates for this model, you may want to find that species that has the strongest correlation with MAMTemp, since this will provide the best predictions of MAMTemp. You don’t need to do that here, but hopefully you understand why you might do this in a real analysis. Here you can use any model for MAMTemp (including one just based on Year). Method #4: Same as #3 but using prediction error as well. In other words, if you used the model suggested above, here you are drawing an imputed value from \\[ \\mbox{MAMTemp} \\sim N(\\beta_{0}+\\beta_{1}*FD,\\sigma^{2}) \\] rather than just taking the expected value \\(\\beta_{0}+\\beta_{1}*FD\\). Once you have your four imputed datasets, run the model you created for the Week #7 lab (one species only). Create a 4x4 table of the 95th percentile posterior CIs (Degradation Method \\(\\times\\) Imputation Method). Do any of the imputation methods work better than any others? How might you define “better”? "],
["week-11-lecture.html", "19 Week 11 Lecture 19.1 Dynamical (time series) models 19.2 Process error 19.3 Observation error 19.4 Other kinds of state=space models 19.5 Missing data", " 19 Week 11 Lecture We have some more topics to cover in terms of model comparison and hypothesis testing, but first we will cover state-space models so that we can discuss model comparison in terms of various models for population dynamics. ‘State-space’ is a generic term used for any kind of modeling in which the observed variables pass between different states. These are often time series models, in which the states are the different time periods of measurement, or they may represent the transitions between different life stages of an animal (for existence). Bolker talks about “dynamical models”. In this case, the states are assumed to represent time. These would be considered a subset of state-space models. (You might further specify that a model is stage-structured or age-structured…regardless, the process of fitting such models is very similar.) We will discuss a tiny slice of state-space models, which is their application to abundance time series. This is where ecological theory meets statistical modeling. 19.1 Dynamical (time series) models I will follow Ben Bolker’s treatment of the subject quite closely, because it is well laid out and it covers both likelihood methods and Bayesian methods. The basic situation is as follows: \\[ N_{t+1,obs} = f(N_{t+1}) \\\\ N_{t+1} = g(N_{t}) \\] Observation error: Involves the actual observation or measurement process. Observation error does not feed back into the dynamics of the system because the dynamics involve only the true state. Process error: Involves variation in the dynamical process. I don’t really like the term “error” here, because there is no error involved. (For this reason, some authors call it process noise.) Process error simply refers to variation in the dynamics not otherwise explained by the model (and possibly unexplainable if there is some genuine stochasticity) Figure 1.1: Diagram of a state space model If you were to simulate 1000 time series from a process-error only model, the time series would diverge over time. By contrast, if you simulated 1000 time series from a model with only observation error, the time series occupy a “band” of constant width over time. Do you see why this is? The process errors accumulate over time because the error in one year affects the dynamics in the next year. Observation errors do not affect the real state of the system, and thus do not accumulate over time. Some vocabulary is in order here. If you are measuring the state of the system with error, then the true state \\(N_{t}\\)] is called a ‘latent’ state. We use the term ‘latent state’ to describe any hidden state which can only be inferred indirectly through observation. 19.2 Process error First, some ecological modeling background to get us started. The Ricker model is a classic population dynamics model that accounts for density-dependence in population growth (population growth slows as the density increases). Ricker’s model for population growth is given as \\[ N_{t+1} = N_{t}e^{r(1-N_{t}/K)} \\] where \\(r\\) is the population growth rate and \\(K\\) is the ‘carrying capacity’. If we explicitly include process error in our formulation, we have at least two choices on how to do so. We could add Normally-distributed error to the effective population growth rate in the exponent; in this case, Ricker’s model looks like \\[ N_{t+1} = N_{t}e^{r(1-N_{t}/K)+\\epsilon} \\] where \\[ \\epsilon \\sim N(0,\\sigma^{2}) \\] If we take the log of both sides \\[ log(N_{t+1}) = log(N_{t}) + r\\left(1-\\frac{N_{t}}{K}\\right)+\\epsilon \\] and rewrite as \\[ log(N_{t+1}) \\sim N\\left(\\mbox{log}(N_{t}) + r\\left(1-\\frac{N_{t}}{K}\\right),\\sigma^{2}\\right) \\] we see that process error of this type assumes \\(N_{t}\\) log-normally distributed. Another way of writing this would be \\[ N_{t+1} = N_{t}e^{r(1-N_{t}/K)+\\epsilon} = N_{t}e^{r(1-N_{t}/K)e^{\\epsilon}} \\] Note that \\(E[e^{\\epsilon}] \\neq 1\\), so \\(E[N_{t+1}] \\neq N_{t}e^{r(1-N_{t}/K)}\\) as you might think (or want, if you are building the model). For this reason, people often add a correction factor (see Hilborn and Mangel pg. 146). Another way of adding process error would be to assume \\[ N_{t+1} = N_{t}e^{r(1-N_{t}/K)}+\\epsilon \\\\ \\epsilon \\sim N(0,\\sigma^{2}) \\] or, equivalently, \\[ N_{t+1} = N(N_{t}e^{r(1-N_{t}/K)},\\sigma^{2}) \\] There are no strict rules about how process error should be modeled (or when); such details would be included in the description of a model. The former approach would be called ‘log-normally distributed process error’, whereas the latter would be called ‘normally distributed process error’. To see how all this gets applied in practice, we will walk through the example provided in McCarthy on mountain pygmy possums. The key elements of the biology are as follows: we will build a model that lumps all age-classes together into a single population. In other words, there will be no age- or stage-structure included in the model, although doing so would be a rather straightforward modification of the code. (Stage- and age-structured models are fairly easy to code, but unless there is a lot of information of the abundance of each stage or age, they can be quite difficult to fit…) female pygmy possums are highly territorial, so we need to include density dependence. We do this here using a Ricker model: \\(N_{t+1} = N_{t}e^{r(1-N_{t}/K)}\\), although there are many ways you might want to do this, and various models can be compared as we will discuss next week. Pygmy possums have small population sizes, and thus experience demographic stochasticity. In other words, just by random chance, their populations can fluctuate, and these fluctuations can be important when population sizes are small. We will use the Poisson distribution to model their abundance. (Side note: Abundances are discrete and non-negative and so a Poisson is an obvious choice. However, abundances are often quite large and are frequently modeled with a continuous distribution such as a Normal. The advantage of a Normal distribution is that you can model the variance independent of the mean. The downside is that a normal distribution does not enforce non-negativity, and for this reason, the log-normal is often used.) The model that McCarthy suggests is the following: \\[ N_{t+1} \\sim \\mbox{Pois}(\\lambda_{t+1}) \\\\ log(\\lambda_{t+1}) = log(N_{t}) + r\\left(1-\\frac{N_{t}}{K}\\right) + \\epsilon \\\\ \\epsilon \\sim N(0,\\sigma^{2}) \\] and the code for fitting this in WinBUGS/JAGS is included in Box 9.1. (I have changed his notation to be consistent with the activities in the lab on Wednesday.) Note that McCarthy added some Normally distributed process error to account for environmental variation not otherwise included in the model. Walk through the code in Box 9.1 – any questions? Note that this model ONLY models process error, there is nothing here to deal with observation error. We would have to layer on top of this a model for the observed counts as a function for the true count. We will discuss some ways of doing that now. 19.3 Observation error Observation models also come with a lot of choices, but usually these are more closely linked to information you have because you probably know something about the way in which the data were collected. As above, you could be choosing between log-normally distributed or normally distributed errors. Alternatively, you may have some other observation process altogether (Binomial, Poisson, etc.). Note that in ecology, observation models for an abundance often assume that the number of counted individuals is some fraction of the true number of individuals, such as \\[ N_{t+1,obs} \\sim \\mbox{Binom}(N_{t+1},\\theta) \\] This will not work when there are false positives or double counting or some other “failure mode” for your observations that does not fit into this Binomial framework. 19.4 Other kinds of state=space models Clark and Bjornstad is a classic paper on state-space models. Let’s walk through their discussion on epidemiological models since they represent a different flavor of state-space models that the ones we have been discussing. These so-called S-I-R models track the number of individuals that are S=Susceptible to a disease, I = Infected with a disease, and R=Recovered from a disease (we usually assume no reinfection after recovery). Like all state-space models, the hard part is just getting accounting right, and working out the transition rates or flow of individuals among different states. The number of susceptible individuals in Year \\(t\\) is given by \\[ S_{t} = S_{t-1} + B_{t-1} - I_{t,new} \\] where \\(B_{t-1}\\) is the number of births in the previous year (these are newly susceptible individuals) and \\(I_{t}\\) is the number who are infected in the current time step and are therefore no longer in the susceptible category. The probability of infection in Year \\(t\\) is modeled with a Binomial \\[ I_{t,new} \\sim \\mbox{Binom}(S_{t-1},\\phi_{t-1}) \\] where \\(\\phi_{t-1}\\) is the transmission rate \\[ \\phi_{t-1} = \\beta_{t} \\times \\frac{I_{t-1,new}}{N_{t-1}} \\] and \\(\\beta_{t}\\) is the constant of proportionality between the transmission rate and the fraction of the population that is infected. (Here we have assumed that your probability of being infected increases as the fraction of infected persons in the population also increases.) Clark and Bjornstad, in their modeling of measles, model \\(\\beta_{t}\\) as varying bi-weekly, since they found that the transmission rate depended strongly on whether schools were in or out of session. I am leaving \\(\\beta_{t}\\) as a function of year because this is the more general formulation. Because not all infections are reported, we model reported infections as a Binomial draw from the true number of infections, with the probability \\(\\rho\\) being the detection probability (from a health services perspective). \\[ I_{t,new,obs} \\sim \\mbox{Binom}(I_{t,new},\\rho) \\] Note that in fitting this model, Clark and Bjornstad use a slightly informative prior for \\(\\rho\\). This is precisely when Bayesian methods become most useful, because you might have good information on reporting rates in other epidemics that can inform the data at hand. Note that Clark and Bjornstad do not actually track the total number of infected individuals, which would require some accounting like: \\[ I_{t} = I_{t,new}-R_{t} \\] In their case, they have only (imperfect) observations on newly infected individuals, and are not keeping track of the total number of infected individuals (as you might, for example, when modeling HIV in which regular surveys are conducted to assess infection status). State-space models can get arbitrarily complicated, and are limited almost exclusively by the quality and quantity of data you have to fit the models. 19.5 Missing data One of the most important take-home messages from Clark and Bjornstad has to do with the way that missing data can be accommodated in state-space models: Figure 3.1: From Clark and Bjornstad "],
["week-11-lab.html", "20 Week 11 Lab 20.1 Simple logistic 20.2 Observation-error-only model 20.3 Process-error-only model 20.4 Process-error and observation-error together 20.5 Final thoughts", " 20 Week 11 Lab In this lab, we’re going to gain some practice fitting models with (1) only observation error, (2) only process error, and (3) observation AND process error together. Let’s stop and remind ourselves why this distinction is so important. Let’s assume we have a simple exponential growth model for a population of fish (or cats, or humans, or money, whatever). \\[ N_{t} = N_{0}e^{rt} \\] which equates to the following “regression-type” linear model on logged abundance (unless I specify otherwise, we’re always using natural logs since we usually need to “undo” the exponential). \\[ log(N_{t}) = log(N_{0}+rt) \\] On the log-scale, this is now a straightforward linear regression model, with \\(log(N_{0})\\) as the intercept and \\(r\\) as the slope. (I could have chosen to model this differently, see Box 1 at end, but here I will do everything on the log scale for mathematical simplicity.) If I assume I only have observation error, then the model can be written as \\[ log(N_{t,obs}) \\sim N(log(N_{t}),\\sigma^{2}_{obs}) \\] or, equivalently \\[ log(N_{t,obs}) \\sim N(log(N_{0})+rt,\\sigma^{2}_{obs}) \\] and I assume that “deviations” from the expected value of \\(log(N_{0})+rt\\) are due to observation error. (Implicitly, this is what we do when we do linear regression; the residuals of the model can be thought of as observation error.) In other words, we assume the true abundance really is \\(log(N_{0})+rt\\) and we chaulk up any disparity in the measured value in year to an error in the measurement of abundance in year \\(t\\). Since that measurement error in year \\(t-1\\) has no influence on the measurement in year \\(t\\), the predicted value in year \\(t\\) is just \\(log(N_{0})+rt\\) or, alternatively, \\(log(N_{t-1})+r\\). In sum, we can write this model in two ways, (1) as before, where time \\(t\\) is the predictor \\[ log(N_{t,obs}) \\sim N(log(N_{0})+rt,\\sigma^{2}_{obs}) \\] or (2) using the previous state \\[ log(N_{t,obs}) \\sim N(log(N_{t-1})+r,\\sigma^{2}_{obs}) \\] On the other hand, if you assume that our measurements are perfect (no observation error), then the difference between what you were expecting and what you measured must be due to variation in the value of \\(r\\) itself. (Process error could involve stochasticity on any demographic parameter but here I am assuming stochastic variation on \\(r\\) specifically.) That means that if your measured value in year \\(t\\) was larger than expected, than it really was truly larger, and the expected value in year \\(t+1\\) would also be larger. In this case, we to write the model as \\[ log(N_{t,obs}) = log(N_{t}) \\sim N(log(N_{t-1})+r,\\sigma^{2}_{proc}) \\] Process error (like interest on a bank account) whereas observation error does not. In this case, we cannot (easily) write the model as a function of \\(N_{0}\\) and \\(t\\). In practice, we often build models that assume both observation and process error, and the task is to distinguish how much of the variation is due to each component. This is sometimes quite difficult to do. In lab, we are going to start with the simple logistic model, which adds to the exponential growth model a term for density-dependence. In other words, the model looks like \\[ N_{t+1} = N_{t}e^{r\\left(1-N_{t/K}\\right)} \\] We have added a new parameter, which is the carrying capacity \\(K\\). At the end, we’ll move on to a variation of the logistic model called the theta-logistic model. As you’ll see, while the simple logistic model is fairly straightforward to fit (i.e. no major convergence issues), the theta-logistic model is exceptionally hard to use, as is described in some detail by Clark et al. 20.1 Simple logistic With the simple logistic model described above, we will work through the process of fitting (1) observation error only models, (2) process error only models, and (3) observation and process error models. The observation error only model can be easily fit using both likelihood methods and Bayesian methods. For the observation+process error model, we will jump straight to the Bayesian approaches, noting that Bolker describes a Kalman filter approach that could be used if we wanted to stick with likelihood-based methods. (This is a classic example of a problem that is quite difficult to solve using likelihood methods, but which is rather straightforward to solve using Bayesian methods.) 20.2 Observation-error-only model Exercise #1: The equations representing an observation-error only model (assuming the simple logistic equation is responsible for the year-to-year dynamics of the population and assuming log-normally-distributed observation error) are given by \\[ log(N_{t,obs}) \\sim N(log(N_{t}),\\sigma^{2}_{obs}) \\\\ log(N_{t}) = log(N_{t-1})+r*\\left(1-\\frac{N_{t-1}}{K}\\right) \\] Notice that if you combine the two statements above into one equation \\[ log(N_{t,obs}) \\sim N(log(N_{t-1})+r*\\left(1-\\frac{N_{t-1}}{K}\\right),\\sigma^{2}_{obs}) \\] Here you have a statement that is the likelihood for a single observed abundance. In other words, if you observe abundance \\(N_{t,obs}\\), then dnorm applied to the above expression yields the likelihood for year \\(t\\). The product of likelihoods across all years in your time series would yield the joint likelihood for the entire dataset. What to do with \\(t=0\\) (the first data point)? A reasonable model for this would be \\[ log(N_{0,abs}) \\sim N(log(N_{0}),\\sigma^{2}_{obs}) \\] where \\(N_{0}\\) (i.e. true abundance in year 0) is a free parameter the model will estimate. How many parameters do you have? There are 5 total: \\(r\\), \\(\\theta\\), \\(K\\), \\(\\sigma^{2}_{obs}\\) and somewhat less obviously, \\(N_{0}\\). How many likelihoods are being multiplied together? You have 30 data points (assuming you simply impute the missing datapoint) so there will be 30 PDFs multiplied together, 29 of which will take the form of Eq. 1 and then one more for the first year (Eq. 2). Exercise #1: Fit this model using JAGS. Plot the posterior mean and 95th credible intervals for \\(N_{t}\\) on the original data. Plot scatterplots of the posterior chains for r vs K– do we have correlations among these two parameters? 20.3 Process-error-only model A process-error only model assuming the simple logistic equation is responsible for the year-to-year dynamics and assuming normally-distributed process error. \\[ N_{t,obs} = N_{t} \\\\ log(N_{t} \\sim N\\left(log(N_{t-1})+r*\\left(1-\\frac{N_{t-1}}{K}\\right),\\sigma^{2}_{proc}\\right) \\] What to do with \\(t=0\\) (the first data point)? In this case there is no observation error so the first year has no uncertainty, it is the value measured (\\(N_{0,obs} = N_{0} = 8\\)). So your likelihood has only 29 terms in it, starting at \\(t=1\\). (Your statistical model is now for transitions between one year and the next, and there are only 29 transitions.) How many parameters do you have now? There are now 4 total: \\(r\\), \\(\\theta\\), \\(K\\), and \\(\\sigma^{2}_{obs}\\) . As before, we will fit this model using JAGS, but make sure you understand how you would fit this model using maximum likelihood as well. Exercise #2: Fit the process error only model using JAGS. Plot the posterior mean and 95th credible intervals for on the original data. Plot scatterplots of the posterior chains for r vs K– do we have correlations among these two parameters? (Note that your best fitting model now “connects the dots” of the data. Does it make sense to you why this would be? With no observation error, your model has to connect the data by construction.) Note that JAGS does not allow you to have your data on the left side of an assignment operator, as it will throw an error similar to “XXX is a logical node and cannot be observed”, so you cannot write your JAGS code in the way you would write the equations by hand \\[ N_{t,obs} \\rightarrow N_{t} \\\\ \\mbox{This will throw an error in JAGS!!} \\] In this case, we have to find a way to write the model so the data is on the left hand side of a probability distribution, and we can do that by combining the process model and observation model as follows: \\[ log(N_{t,obs}) \\sim N\\left(log(N_{t-1})+r*\\left(1-\\frac{N_{t-1}}{K}\\right),\\sigma^{2}_{proc}\\right) \\] Make sure you understand why this is equivalent to the original model description. But wait! This looks nearly identical to the observation only model above (Eq. 1)! How are these different you say? In this specific case, we have modelled observation error and process as being normally distributed on the log-scale, and as such they are in fact identical models. The interpretation of the terms are different (and they would be different in simulation, see below), but the estimates will be the same. 20.4 Process-error and observation-error together Exercise #3: Write a JAGS script to fit a model with both process error and observation error. Plot the posterior mean and 95th credible intervals for on the original data. Plot scatterplots of the posterior chains for r vs K– do we have correlations among these two parameters? How about when we plot \\(\\sigma^{2}_{proc}\\) vs \\(\\sigma^{2}_{obs}\\)? (You should find \\(\\sigma^{2}_{proc}\\) and \\(\\sigma^{2}_{obs}\\) strongly correlated with one another. This reflects the fundamental non-identifiability of this model. There is not enough information for the model to distinguish process error and observation error because both are modeled in the same way and so it cannot distinguish them. While such non-identifiability is fairly easy to see from the model itself, in more complex models it is not always easy to see when parameters are non-identifiable, and so plotting posteriors against each other is a good way to check whether parameters are simply “trading off” in the MCMC chains. 20.5 Final thoughts In the example provided at the beginning of lab, we modelled observation error on the log scale as follows \\[ log(N_{t,obs}) \\sim N(log(N_{t}),\\sigma^{2}_{obs}) \\] We can have, alternatively, modelled observation error on the linear scale \\[ N_{t,obs} \\sim N(N_{t},\\sigma^{2}_{obs}) \\] How to choose? It comes down to which you think better captures the observation error process. For example, if you are counting a group of birds and your counting procedure is highly accurate but birds keep running in and out of the study area, you might want to model observation error on the linear scale. However, if your process has a certain error percentage (i.e. $\\(15\\)%$) than perhaps the observation is better modelled on the log-scale. "],
["week-12-lecture.html", "21 Week 12 Lecture 21.1 Mark-recapture modeling 21.2 Cormack-Jolly-Seber 21.3 Method #1: Brute force 21.4 Method #2: Modeling the entire capture history 21.5 What other kind of models might you fit 21.6 Occupancy modelling 21.7 Dynamic state-space models for meta-population dynamics", " 21 Week 12 Lecture ** Read McCarthy and Masters (2005) for lab. We will be working with their data on the European dipper. Today we are going to cover two types of models, mark-recapture models and occupancy models that have very different applications but share the same underlying principles. 21.1 Mark-recapture modeling The basic idea behind mark-recapture is as follows: You catch an animal (hopefully lots of animals). You mark it. You come back at a later date and catch more animals. Some of these are animals with tags. Of those animals not recaptured, you do not know whether they are alive and present at the site but evaded capture, whether they died in the period between mark and recapture, or whether they survived but emigrated to another location. Mark-recapture techniques at one site (absent strict site fidelity) cannot distinguish between permanent emigration and death, and so we usually use the phrase to reflect the fact that some of the missing animals simply moved to another location. That said, repeated recapture events allow you to estimate how many individuals that remained at the site were simply not recaptured. (For example, if the capture history looks like 1001, you know the animal didn’t die, and you can estimate the probability of recapture conditional on presence.) MARK is a very popular software for mark-recapture models that uses maximum likelihood. Since this is a Bayesian class, we will stick to Bayesian solutions to the classic mark-recapture problem. 21.2 Cormack-Jolly-Seber The CJS model assumes and . (These are different because while an animal has to survive every year, detection or non-detection only occurs when there is an effort to recapture or re-sight animals. Thus, you may have a 10 year time series in which 4 recapture surveys were completed. You would, in the most general model then, have 9 probabilities for survival to estimate (\\(\\phi\\)), but only 4 probabilities for detection (\\(p\\)).) Let’s say we have a 7 year time series, and we make an effort to re-sight animals in each year. The illustration below numbers the probability of re-sight according to the year, so the first re-sight attempt is made in Year 2, and is called \\(p_{2}\\). Figure 1.1: Diagram of mark-recapture model. Figure 3.1: Outcomes and their probabilities of the basic mark-recapture model. STOP: Look over these probabilities and make sure they make sense. For example, the probability of surviving three years after marking before being recaptured (or resighted) in the fourth year is given by: \\[ \\phi_{1}\\phi_{2}\\phi_{3}\\phi_{4}(1-p_{1})(1-p_{2})(1-p_{3})p_{4} \\] Mark-recapture model selection usually hinges around how to reduce the full number of parameters to something more estimable, for example, by assuming the detection probabilities are the same across capture attempts. Alternatively, you might model survival or capture probabilities as a function of covariates, such as \\[ logit(\\phi_{i}) = \\beta_{1} + \\beta_{2} X_{i} \\] Note that animals can become trap-happy or trap-shy, which means that catching them a second time can be a lot easier or more difficult than catching them the first time. There are dozens and dozens of papers dealing with these kinds of models. You can model either (or both) survival and capture probabilities as a function of time, which means that even with no other parameters, you are looking at four possible models: (\\(\\phi(t),p(t)\\)), (\\(\\phi(t),p(.)\\)), (\\(\\phi(.),p(t)\\)), (\\(\\phi(.),p(.)\\)), where I have used \\(.\\) to represent no time dependence. How would you decide which model was best? We will get into model comparison methods in a few weeks. McCarthy described two approaches to fitting mark recapture models in JAGS. The first is basically a brute force state-space model in which we explicitly model transitions between each year for each animal. The data required is the full matrix of 0s and 1s recording the capture history for each animal. The second approach is to model the entire capture history (for each animal) at once to work out the likelihood of obtaining that capture history. We will go through both approaches. 21.3 Method #1: Brute force The probability of individual \\(i\\) being alive at time \\(t\\) \\((Z_{i,t}=1)\\) conditional on having been alive at time \\(t-1\\) is given by \\[ Z_{i,t} \\sim Bern(Z_{i,t-1}\\phi) \\] The probability of re-sighting an animal is \\[ Y_{i,t} \\sim Bern(Z_{i,t}p_{i}) \\] Note that \\(Z_{i,t}\\) is a latent state, knowledge of which is inferred only indirectly through \\(Y_{i,t}\\), which is the data. The set up looks like Figure 8.1: Transitions in a state-space model We have a lot of flexibility in modeling capture probabilities. At one extreme, we could model each capture probability separately, and at the other extreme we could assume a constant capture probability for the whole experiment. Somewhere in between these two extremes are models that assume capture probability is drawn from some distribution (this distribution would be governed by ‘hyperparameters’; this is an example of hierarchical modeling which we discussed a few weeks ago), or governed by some covariates (such a time of day, weather, etc.). Exercise: What would some of these observation models (i.e. models for capture probability) look like? 21.4 Method #2: Modeling the entire capture history The idea behind this is as follows. For the entire period between marking and the last recapture, you know the animal is alive. Between the last resighting and the end of the experiment, you don’t know whether the animal is still alive or not. So we divide up the recapture history into two periods, the first being the period in which we know the animal is alive (from initial marking until ), and the second in which we do not know whether the animal is alive or dead. In the first period, you know that the animal survived \\(t_{1}\\) years, and so the detection history of \\(d\\) re-captures and \\(t_{1}-d\\) non-captures: \\[ L_{1} = \\phi^{t_{1}}p^{d}(1-p)^{t_{1}-d} \\] Note that we have to include a term for survival probability; the fact that an animal lived for \\(t_{1}\\) years is informative about that parameter. In the second period, we have to add up all the various possible “paths” of non-detection and/or mortality. In this second period (\\(t_{2}\\) years of non-detection), there are two possibilities: (1) it survived but went undetected for all \\(t_{2}\\) years or (2) it survived and went undetected for \\(t &lt; t_{2}\\) years and then it died. The net result of this is a likelihood that looks like \\[ L_{2} = (1-\\phi)\\sum_{i=1}^{t_{2}}[\\phi(1-p)]^{i-1}+[\\phi(1-p)]^{t_{2}} \\] where \\(\\phi\\) is the probability of survival and \\(p\\) is the probability of detection. (You may need to write out a few terms to convince yourself of this.) The joint likelihood of the entire time series is the product of \\(L_{1}\\) and \\(L_{2}\\). The advantage to modeling the entire capture history in this way is that you have condensed the data down to three numbers for each mark-recapture history \\(t_{1}\\), \\(d\\), and \\(t_{2}\\), so animals that share these metrics can be combined and the entire dataset, which might be quite large, can be greatly compressed for analysis. 21.5 What other kind of models might you fit There are almost an infinite variety of models you might fit to your data. You could build models that include year-specific survival rates (either related to year itself, or to some environmental covariates that relate to inter-annual variability in survival) capture rates that decline with the number of previous captures (once captured, twice shy?) capture rates that depend on the field biologist setting and checking traps etc. 21.6 Occupancy modelling Closely related to mark-recapture models are occupancy models. Assume you have data from a field sampling program in which the presence or absence of a bird species is recorded along transects at different elevations and tree densities. You have one data point (Y=0 \\(\\rightarrow\\) absence, Y=1 \\(\\rightarrow\\) presence) for each transect. The basic model in this case would be a logistic model \\[ Y_{i} \\sim Bern(p_{i}) \\\\ logit(p_{i}) = \\alpha + \\beta* \\mbox{Elev}_{i} + \\gamma*\\mbox{Density}_{i} \\] where I have assumed Elevation and Density as covariates. Many models for habitat suitability look much like this. Implicitly we have assumed 100\\(\\%\\) detection rates, because we assume that our data reflect the true occupancy status. Let’s say that now we admit the possibility that we have imperfect detection, and sometimes fail to detect the target species when it is present. To manage this scenario, we need to add an “observation model”, and we need some additional information that would allow us to separate true absence from non-detection. What we need are repeated surveys over some period of time that the population is closed to immigration and emigration. (This is called a robust sampling design.) These repeat samples allow us to estimate the probability of non-detection when a species is present. We will assume that each transect is surveyed J times each breeding season, and we will assume that the presence of a species in the transect is fixed over that period of time. The observations are now not Bernoulli 0/1 but a Binomial draw that represents the number of times (out of \\(J\\) surveys) that a species was recorded. The resulting model looks something like: \\[ Y_{i} \\sim \\mbox{Binom}(n=J,p=\\mu_{i}) \\\\ \\mu_{i} = Z_{i}*\\phi_{i} \\\\ Z_{i} \\sim \\mbox{Bern}(p_{i}) \\\\ \\mbox{logit}(p_{i}) = \\alpha + \\beta*\\mbox{Elev}_{i}+\\gamma*\\mbox{Density}_{i} \\] \\(Z_{i}\\)=True occupancy (0/1) \\(\\phi_{i}\\)=Probability of detection \\(p_{i}\\)=Probability of occupancy Note that we have introduced a latent unmeasured state variable \\(Z_{i}\\) that is analogous to the latent state representing the status of an animal in a mark-recapture context. Also note that if \\(Y_{i} &gt; 0\\), then \\(\\mu_{i} &gt; 0\\), which means that \\(Z_{i}\\) must equal 1. Transects for which a species was sometimes detected, but not always detected, give us direct information on the probability of detection \\(\\phi_{i}\\). This gives us the ability to estimate the probability of occupancy for transects in which the species was never detected (but which it may occupy). Side note: Repeated measurements during which time the population is “closed” is key. There are some attempts to model situations in which only a single survey is performed, but these require a bunch of other assumptions and constraints. By and large, these models require a sampling scheme in which repeated surveys are attempted. Note that this is basically the site-occupancy model presented by Kery in Chapter 20. We will not have time to get into abundance models, but you can see the basic logic in extending this model to the abundance of animals counted at site \\(i\\) on occasion \\(j\\) (\\(Y_{ij}\\)) as follows: \\[ Y_{ij} \\sim \\mbox{Binom}(N_{i},p_{ij}) \\\\ N_{i} \\sim \\mbox{Pois}(\\lambda) \\\\ \\mbox{logit}(p_{ij}) = \\mbox{whatever} \\] where the number of animals actually present at site \\(i\\) (\\(N_{i}\\)) is drawn from a Poisson distribution that reflects the overall density of individuals in the landscape. 21.7 Dynamic state-space models for meta-population dynamics NB: My discussion here follows closely the paper by Royle and Kéry (2007). In the discussion above, we assumed a totally closed population. Meta-population models, in which we are interested in colonization and extinction dynamics of sites (or ‘patches’), explicitly include the possibility that a site’s occupancy status might change over time. In these cases, we assume a sampling strategy in which site occupancy is allowed to change between ‘primary’ sampling periods (often, but not always, between years) but is assumed fixed at secondary sampling intervals (such as within a breeding season). The secondary sampling allows us to estimate the true probability of occupancy in each year, and changes in true occupancy status years gives us information on the metapopulation dynamics. Note that if we ignored detection failures, we would probably over-estimate the probabilities of extinction and recolonization, because a detection history of 101 (found, not found, found) would be interpreted as one extinction event followed by one colonization event. Allowing for detection failures, there emerges a second possibility, which is continuous occupation with a detection failure in year 2. We model the true occupancy status of site in year (the process model) as \\[ Z_{i,t}|Z_{i,t-1} \\sim \\mbox{Bern}(Z_{i,t-1}\\phi_{t-1}+(1-Z_{i,t-1})\\gamma_{t-1}) \\] where \\[ \\phi_{t-1} = \\mbox{probability of patch survival in year t-1} \\\\ \\gamma_{t-1} = \\mbox{probability of patch recruitment in year t-1} \\] The observation model for observation \\(j\\) at site \\(i\\) in year \\(t\\) looks like \\[ Y_{j,i,t} \\sim \\mbox{Bern}(Z_{i,t}p_{t}) \\] Royle and Kery make an important point about the difference between finite sample estimators and population estimators (see ‘Finite sample estimation’ section on page 1816). In a conservation context this can be an important distinction. I encourage you to read that section carefully if this is likely to apply to your research. "],
["week-12-lab.html", "22 Week 12 Lab 22.1 The ‘zeros’ trick 22.2 The ‘ones’ trick 22.3 Initial values 22.4 First, a warm up 22.5 Fitting mark-recapture models", " 22 Week 12 Lab Before we even get to mark recapture models, we need to introduce a couple of tricks in JAGS that can be used to handle non-standard distributions. 22.1 The ‘zeros’ trick The ‘zeroes trick’ is JAGS goes as follows. Let’s say we want to model \\[ Y \\sim g(\\theta) \\] but \\(G()\\) is not a distribution that is built into JAGS. We can achieve the same effect (i.e. have the same likelihood enter the model) if we instead say that we observed a value of \\(Y=0\\) from the following model \\[ Y \\sim Pois(\\lambda) \\] where \\[ \\lambda = -\\mbox{log}(g(y,\\theta)) \\] and \\(g(y,\\theta)\\) is the likelihood associated with distribution \\(G\\). In other words, the probability of getting a 0 from \\(\\mbox{Pois}(-\\mbox{log}(g(y,\\theta))\\) is just \\(g(y,\\theta)\\), which is the likelihood we wanted in the first place. (Reminder, the pmf of a Poisson is \\(\\frac{\\lambda^{x}}{x!}e^{-\\lambda}\\) so the probability of getting \\(x=0\\) is \\(e^{-\\lambda}\\).) In practice, we add a constant \\(C\\) to get \\(\\lambda = -\\mbox{log}(g(y,\\theta))+C\\) to ensure that \\(\\lambda &gt; 0\\).) Let’s take for example the Cauchy distribution \\[ f(y|\\theta) = \\frac{1}{\\pi}\\frac{1}{1+(y-\\theta)^{2}} \\] The Cauchy is an evil distribution, as you all know. Nevertheless, we can use the zeros trick to sample from it as follows: Zeros[i]&lt;-0 lambda[i]&lt;- -log(1/(1+pow(y[i]-theta,2))) Zeros[i]~dpois(lambda[i]) Notice that I just need the portion of the pdf that involves the data and the parameters because all that matters is that I get something proportional to the correct likelihood, and so I have dropped the constants involved in \\(f(y|\\theta)\\). 22.2 The ‘ones’ trick The ‘ones trick’ is similar, but it uses a dummy variable of 1 drawn from a Binomial distribution: \\[ Y \\sim \\mbox{Bern}(p) \\\\ p = g(y,\\theta) \\] The code for the Cauchy looks like Ones[i]&lt;-1 p[i]&lt;- 1/(1+pow(y[i]-theta,2)) Ones[i]~dbern(p[i]) 22.3 Initial values Sometimes, the hardest part about getting JAGS to run successfully is supplying it with good initial values. For mark-recapture models I suggest trying NAs up to the first capture, 1 through the period of known survival (from marking to last resighting) and 0 for the period where its fate is unknown (after the last resighting). 22.4 First, a warm up Let’s look through some code for occupancy modeling that will serve as a warm up to the Lab and the Problem Set. It has been posted on Bb under “code for Week 12 Lab warm up.R”. This code simulates the detection of an amphibian (usually detected through calling) at 100 sites assuming 30 surveys each breeding season, and detection probabilities that vary as a function of humidity. (This is a bit of a contrived example. We assume 30 replicate surveys at each site, assuming a closed population and assuming that each site has fixed humidity. Nevertheless, these assumptions simplify the data simulation and are easily relaxed.) (Note that this warm up exercise involves occupancy of an animal species at different sites, whereas the lab itself involves mark-recapture of individual animals over several years. Occupancy and mark-recapture are related problems, but the actual model code will be different.) 22.5 Fitting mark-recapture models Mark-recapture models are a huge field of modeling, and so we will only just barely scratch the surface on what can be done. Nevertheless, they are a common application of Bayesian models, and they will give us some practice writing Bayesian models in a wider variety of applications. We are going to start with a fairly straightforward dataset on the survival of the European Dipper, which was described in the paper by McCarthy and Masters (2005) that you read for class this week. I have posted the survival data on Bb; copy and paste from “dipper_data.txt”. (While the code used by McCarthy and Masters is online as part of the supplement, no peaking! You’ll learn more working through it from the beginning.) Write a mark-recapture model to estimate survival (constant over time) and resighting rate (also constant over time) for the Dipper. We’ll start with an uninformative prior, and we have time can go back and discuss how McCarthy and Masters generated a more informative prior if we have time. Exercise #1: Use the Method #1 (Brute force) approach described in lecture on Monday. This is the most straightforward and the easiest to code. Exercise #2: Use the Method #2 (Modelling the entire capture history) approach described on Monday. You’ll have to think carefully how to represent the likelihood for the entire recapture history. (Hint: You may need the ‘ones’ or ‘zeroes’ trick.) "],
["week-13-lecture.html", "23 Week 13 Lecture 23.1 Rejection ABC 23.2 Option #1: Basic rejection ABC 23.3 Option #2: Markov Chain Monte Carlo ABC 23.4 Option #3: Sequential Monte Carlo ABC", " 23 Week 13 Lecture We will start with a quote from Beaumont 2010: “For many problems, although it may be straightforward to write a computer program to simulate data, it may actually be very difficult to work out the likelihood function.” When would we use ABC? ABC should be considered when the mechanism generating your data is so complicated that you don’t have any hope of writing down an actually likelihood for it. It might be because the actual biological or physical mechanism is complicated or because the observation process is complicated - it doesn’t matter. You have data and its generated by some process and you want to figure out how to estimate the parameters of that underlying process. The basics of ABC are simple: (1) Specify some summary metric that you think captures the fit of the model to the data. [Some references call this “D” but I like to use the symbol T since this is more traditional and it matches the symbology we used in Biometry.] In other words, how would you judge whether a model was a “good” model. Usually there are aspects of the data that you think are really important and any good model should match the data in these respects. There are also probably aspects of the data that are not as critical and you’d be satisfied with a model that missed the mark in these respects. (In a population model, for example, you might be most concerned that your model captures the interannual variability in abundance, and less concerned that the models gets the right number for abundance. In this case, the summary statistic might be interannual variation in abundance.) Sufficient statistics A little vocabulary is in order here. A sufficient statistic is a summary metric that captures all the information in the data for a given parameter. Once you calculate the sufficient statistic, you have everything you need to estimate the parameter of interest. For example, if you have data from a Poisson distribution, the sum of the data is all you need to estimate the parameter \\(\\lambda\\). (This seems obvious for this example but there are more complex examples in which the “data compression” of the summary statistic is significant.) I’m bringing this up here because sufficient statistics are always the best summary metric to use for ABC, since there is no loss of information in using the sufficient statistic to compare the simulated data to the real data. However, in many practical cases, the sufficient statistic is unknown so you have to be a bit creative when thinking of summary metrics that capture the key elements of the dataset you have. Simulate the process over the space of all possible parameter values. For each simulation, calculate the summary metric of fit. Reject the parameter combinations that did not yield a summary statistic similar to the actual data. The density of parameter values that were not rejected are left, and they form an approximate posterior distribution. *This is remarkably straightforward! Try a bunch of parameter values and see which ones “work”. Its like throwing spaghetti at the wall and seeing what sticks. Strictly speaking, the algorithm as I have just described it is ABC-rejection sampling, because we are rejecting parameter sets that do not fit the data. Other algorithms for ABC operate more like particle filters, whereby you iterate through the ABC process but at each step you redraw new parameter sets in the vicinity of the ones that were retained at the last stage. To solidify these ideas, let make sure we see the parallels between ABC and the basic rejection sampling we learned in the beginning of the semester. Rejection sampling In rejection sampling, we draw a value \\(x^{(i)}\\) from the prior \\(q(x)\\). [Note that at the time, we called this a candidate distribution, but it is essentially a prior distribution and so we will stick with that terminology here. The scaling constant \\(M\\) is just there to ensure that the likelihood ratio is always \\(\\leq 1\\).] We then calculated the ratio of the likelihoods and accepted the value with a probability equal to the likelihood ratio. [Again, this is a scaled likelihood ratio because we have this arbitrary constant \\(M\\) in the denominator. So if \\(M\\) is really big, we reject most of the samples from the prior/candidate distribution, but they are retained to the likelihood \\(p()\\).] \\[ x^{(i)} \\sim q(x) \\\\ u \\sim \\mbox{Unif}(0,1) \\\\ \\mbox{u} &lt; \\frac{p(x^{(i)})}{M*q(x^{(i)})} \\mbox{, then accept } x^{(i)} \\] This works fine when we can write down the likelihood \\(p()\\) but what happens when we cannot write down the likelihood? That’s when ABC comes in. 23.1 Rejection ABC Same idea as above, but we are drawing parameter(s) from a prior/candidate distribution \\(x^{(i)}\\) and then running them through our simulation. We then accept or reject based on whether the summary metric matches that from the data. \\[ x^{(i)} \\sim q(x) \\\\ \\mbox{Simulate data using the parameter(s) } x^{(i)} \\\\ \\mbox{if } T^{\\mbox{sim}} = T^{\\mbox{data}} \\mbox{, then accept } x^{(i)} \\] So…this sounds pretty straightforward, but often T is a continuous variable and it is exceptionally unlikely that your simulated statistic will yield a summary statistic T that is exactly equal to that for the real data, and so in practice, ABC goes like \\[ x^{(i)} \\sim q(x) \\\\ \\mbox{Simulate data using the parameter(s) } x^{(i)} \\\\ \\mbox{if } \\mbox{distance}(T^{\\mbox{sim}},T^{\\mbox{data}}) &lt; \\epsilon \\mbox{, then accept } x^{(i)} \\] Note that now we had to define some way of defining the distance between two statistics. In one dimension, when you have a single statistic, this is pretty straightforward and you could simply define \\[ \\mbox{distance}(T^{\\mbox{sim}},T^{\\mbox{data}}) = T^{\\mbox{sim}} - T^{\\mbox{data}} \\] However, when you have more than one statistic, you need to find some way to combine them. For instance, its not obvious that if you have one statistic related to mutation rate and another related to convergence time that the distance should simply be the Euclidian distance \\[ \\mbox{distance}(T^{\\mbox{sim}},T^{\\mbox{data}}) = \\sqrt{(MR^{\\mbox{sim}} - MR^{\\mbox{data}})^2 + (CT^{\\mbox{sim}} - CT^{\\mbox{data}})^2} \\] In fact, this measure of distance might make no sense at all. This is all to say that your choice of statistics and your choice of distance are up to you, and should be chosen carefully if your ABC algorithm is going to be able to differentiate ‘good’ models from ‘bad’ ones. ABC comes with some challenges. In particular, how to explore parameter space if you have a large number of parameters? There are many flavors of ABC that have been developed; here I only introduce a few simple ones to give you a sense of how it goes. 23.2 Option #1: Basic rejection ABC The simplest option is simply draw a gazzilion points from your parameter space and try them all. You could draw these parameter sets from your prior distributions for each parameter. You can then use some rejection criteria to select those parameter sets that led to data that were “close enough” to be retained. 23.3 Option #2: Markov Chain Monte Carlo ABC In this method, you will have a Markov Chain of samples. I’ve pasted the pseudocode from Beaumont 2010 below. Note that \\(\\pi(\\theta)\\) is the prior distribution for parameter \\(\\theta\\) and that \\(K()\\) is a PDF describing jumps between values in parameter space. (This is exactly the same as we dealt with previously with we discussed Metropolis-Hastings.) Note that if the jump distribution \\(K()\\) is symmetric, than the ratio in step 3b is equal to 1 and can be dropped. Pseudocode Initialize by sampling \\(\\theta^{(0)} \\sim \\pi(\\theta)\\). \\ At iteration \\(t \\get 1\\), \\ Simulate \\(\\theta^{\\prime} \\sim K(\\theta|\\theta^{(t-1)}\\)). Simulate \\(x \\sim p(x| \\theta^{\\prime}\\)). If \\(\\rho(S(x),S(y)) &lt; \\epsilon\\), a. \\(u \\sim \\mbox{Unif}\\)(0,1); b. \\(\\mbox{if} u \\leq \\pi(\\theta^{\\prime})/\\pi(\\theta^{(t-1)}) \\times K(\\theta^{(t-1)}|\\theta^{\\prime})/K(\\theta^{\\prime}|\\theta^{(t-1)}), \\mbox{then } \\theta^{(t)}=\\theta^{\\prime}\\); c. \\(\\mbox{otherwise } \\theta^{(t)} = \\theta^{(t-1)}\\); \\(\\mbox{otherwise } \\theta^{(t)} = \\theta^{(t-1)}\\). 23.4 Option #3: Sequential Monte Carlo ABC A more sophisticated algorithm is called Sequential Monte Carlo (SMC) ABC. In SMC-ABC, each parameter combination \\(\\theta_{i}\\) is termed a “particle”. If you have \\(m\\) parameters, you can think of each parameter combination as a particle in m-dimensional space. A population of these particles can be written as \\(\\{\\theta_{1},\\theta_{2},\\dots,\\theta_{N}\\}\\). Pseudocode A population of particles \\(\\{\\theta_{1},\\theta_{2},\\dots,\\theta_{N}\\}\\) is selected using the prior distributions for each parameter The simulation/model are used to simulate data \\(x^{*}\\) from \\(\\{\\theta\\}\\) Calculate a distance metric between \\(x^{*}\\) and the observed data Particles with distance less than some tolerance \\(\\epsilon\\) are retained Retained particles are weighted and smoothed to create a distribution from which the next generation of particles will be drawn Reduce the tolerance \\(\\epsilon\\) and start again at step 1 Keep iterating until the desired tolerance \\(\\epsilon\\) is achieved or, more mathematically, At iteration \\(t=1\\), \\ for \\(i = 1,\\dots,N\\), \\ until \\(\\rho(S(x),S(y)) &lt; \\epsilon_{1}\\) \\ simulate ${i}^{(1)} () x p(x|{i}^{(1)}). \\ Set \\(\\omega_{i}^{(1)} = 1/N\\). \\ Take \\(\\tau^{2}_{2}\\) as twice the empirical variance of the \\(\\theta_{i}^{(1)}\\)’s. At iteration \\(2 \\leq t \\leq T\\), \\ for \\(i = 1,\\dots,N\\), \\ until \\(\\rho(S(x),S(y)) &lt; \\epsilon_{t}\\) \\ pick \\(\\theta_{i}^{*}\\) from the \\(\\theta_{j}^{(t-1)}\\)’s with probability \\(\\omega_{j}^{(t-1)}\\); \\ generate \\(\\theta_{i}^{(t)} \\sim K(\\theta | \\theta_{i}^{(*)};\\tau_{t}^{2})\\) and \\(x \\sim p(x|\\theta_{i}^{(t)})\\). \\ Set \\(\\omega_{i}^{(t)} \\propto \\pi(\\theta_{i}^{(t)})/\\sum_{j=1}^{N}\\omega_{j}^{(t-1)}K(\\theta_{i}^{(t)}|\\theta_{i}^{(t-1)};\\tau_{t}^{2}\\)). Take $_{t+1}^{2} as twice the weighted empirical variance of the \\(\\theta_{i}^{(t)}\\)’s. Figure 1.1: Visualization of the alternative sampling options: Rejection Sampling, MCMC, and SMC. "],
["week-14-lecture.html", "24 Week 14 Lecture 24.1 The Mona Lisa", " 24 Week 14 Lecture We’ve spent the last few weeks learning how to run models in JAGS, and now we cycle back to some more basic questions about how hypothesis testing is actually done in a Bayesian framework. Everything covered this week is a matter of continuing development. Different experts and textbooks will disagree on the best method, and the use of any one method may draw ire from reviewers. There are no easy and simply ways to check that your model works; these problems are the same as those in frequentist statistics except that the field is newer and so the approaches are still a matter of some debate and discussion. We usually have two questions when fitting models with data: How do the models compare to one another? How do the models compare to the data? We have to be careful that an excessive concern over model comparison does not keep up from asking the very fundamental question: Are any of these models any good? If none of the models are very good, the whole exercise of model comparison may be a waste of time. (But maybe not – perhaps you are interested in statistically significant covariates even though they explain little of the variation in the data.) ** Stop &amp; discuss: What makes a “good” model?** McCarthy makes an important point when he says that the model should fit both the central tendency and the variation in the data. In other words, let us not forget that a model such as \\[ Y \\sim N(\\mu,\\sigma^{2}) \\] includes both \\(\\mu\\) and \\(\\sigma^{2}\\), the latter of which should not be dismissed as a nuisance parameter to the “main event” of \\(\\mu\\). This is particularly important when building models to be used for prediction. Method #1: Plot the model against the data While it seems almost too simple to be of any use, you should always lot the model against the data. Sometimes, the parameters look fine but the fit is obviously horrible for one reason or another. Method #2: Posterior predictive checks An alternative to asking “How well does the model fit the data we have” is to ask “How well does the model predict new data given the fit to the old data”. To do this we calculate the posterior predictive distribution \\[ p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta)p(\\theta|y)d\\theta \\] \\(\\tilde{y}\\) = new data\\ y = original data \\ \\(\\theta\\) = model parameters \\ \\(p(\\theta|y)\\) = posterior distribution of \\(\\theta\\) Question: What is \\(p(\\tilde{y}|\\theta)\\)? In other words, you want to integrate out the posterior distribution for the parameters to get the posterior distribution for the predicted values. In practice this means doing the following: Step #1: Sample \\(m\\) values of \\(\\theta\\) from the posterior \\(p(\\theta|y)\\) Step #2: For each of these \\(m\\) values, sample from the likelihood \\(p(\\tilde{y}|\\theta)\\) These samples represent draws from the posterior predictive distribution \\(p(\\tilde{y}|y)\\). How do we use \\(p(\\tilde{y}|y)\\)? The most straightforward, and in practice recommended, way to use \\(p(\\tilde{y}|y)\\) is to check the model graphically. Does the model “look” correct for whatever key features you are interested in? Where it fails, does it tell you anything about the model? Does it tell you anything about the data? Remember: All models are wrong, but some are useful. A second, more quantitative but also probably less helpful, approach would be to do this following. Step 1 \\(\\rightarrow\\) Come up with a ‘relevant’ test statistic \\(T\\), i.e. one that has the power to distinguish a good model from a bad model. Note that ‘good’ and ‘bad’ here are context dependent, because it depends on the goals of the modeling. You want to compare models based on the most important predictions, which may or may not be straightforward. In other words, in studying the population growth rate of a species, you may be interested in the maximum population growth rate, which is a function of other life history parameters such as age to maturity. It may be that you are not interested in age to maturity at all, except in as far as it influences maximum population growth rate. In this case, you should compare models based on growth rate and not age to maturity. Often, we are interested in predicting what values we might see if we could “rewind time”; in these cases the posterior predictive distribution is compared against the real data to ask whether or not it is similar in ‘relevant’ ways. Step 2 \\(\\rightarrow\\) Calculate T for the observed data \\(y: T(y)\\). Step 3 \\(\\rightarrow\\) Calculate T for each draw from the posterior predictive distribution \\(\\tilde{y}: T(\\tilde{y}|y)\\). Step 4 \\(\\rightarrow\\) Calculate the fraction of times that \\(T(y) &lt; T(\\tilde{y}|y)\\). (I have flipped this statement around from Gelman and Chalizi 2013 because I think its easier to think of in this way). Sound familiar? It should. This is called the posterior predictive p-value. In other words, we are checking to see whether our data would be considered an extreme outcome of the model as defined by the test statistic. Let’s say that our test statistic exceeds the model prediction test statistic \\(T(\\tilde{y}|y)\\) for \\(&gt;95\\%\\) of model predicted data sets. In this case, the posterior predicted p-value would be \\(&lt;0.05\\), and we would say that we can reject the model as fitting the data. (Bayesians don’t use this phrase, but another way to think about it is that we have established a null hypothesis that the model does fit the data in as far as the characteristic measured by the test statistic \\(T\\), and a p-value\\(&lt;0.05\\) would lead us to reject that null hypothesis.) Recap: A very low p-value says that it is unlikely, , to have obtained data as extreme along the T-dimension as the actual y, i,.e. we are seeing something which would be highly improbable . In other words, you want your model to make your data look typical, not unusual. Note that here we have used the data twice – once to build the model and then again to check the model. This is akin to looking at the \\(R^2\\) of a traditional frequentist regression model. Sure, the model probably fits because it was designed to fit the data. This doesn’t really tell us all that much about whether the model would work on new data. As we discussed in Biometry (in our Shmueli discussion), one solution would be to withhold some data from the model fitting, and use only the withheld data to see if the model is any good (at making predictions). Note that passing the “test” above does not mean the model is guaranteed to be good, because your test statistic T may have low power to detect certain violations. However, it may flag problems with the model that are worth investigating further. 24.1 The Mona Lisa What test statistics would you use to determine if a painting actually was the Mona Lisa? (Thanks to Stefano Allesina for the idea.) Figure 1.1: The Mona Lisa. Source: Wikimedia Commons Click for Answer Compare your test statistics to three “model” Mona Lisas – Were your test statistics powerful enough to reject any of the models as the real Mona Lisa? Figure 3.1: Fake Mona Lisa #1. Figure 8.1: Fake Mona Lisa #2. Figure 8.2: Fake Mona Lisa #3. As I’ve tried to illustrate below, both frequentist and Bayesian statistics can be thought of as following the hypothetico-deductive framework, but the former relies on \\(p(data|model)\\) and the latter on \\(p(model|data)\\). Note that in Step #4 of the Bayesian workflow diagram, I’ve stated that the posterior predictive checks can be used to falsify a model. While true, this isn’t all that helpful, because we know a priori that all models are false. The way to think about this last step is that you are checking to see in what way your model is false and whether your model fails in ways that are important to your application. (Posterior predictive p-values are not without their critics, and here I would refer to Andrew Gelman’s blog, where he airs out the dirty laundry of posterior predictive p-values [which he himself has helped develop].) Figure 24.1: Diagram illustrating how Bayesian and frequentist approaches integrate theory, data, and predictions. Method #3: Deviance Information Criteria (DIC) Whereas Methods #1 and #2 relate to the question “Does my model fit to the data”, they do not directly address the question “Which of these two competing models is better?”. One way of comparing models in a Bayesian context is the Deviance Information Criterion. First, a quick recap of Deviance. Likelihood: The likelihood of a model represents the likelihood (i.e., the probability) that you would have obtained the actual data given a particular model. This is almost always a very small number, since even the “true” model could yield a large number of outcomes, of which yours is just one. Assuming independent data points, the joint likelihood for the whole data set is simply the product of the likelihood of obtaining each data point, and so the joint likelihood is a very very small number (even at its maximum). (This was the moral of the M&amp;M problem set in Biometry.) To deal with these obnoxiously tiny numbers, we focus on the log-likelihood. Deviance: In Biometry, we introduced the idea of Deviance, which was defined as \\[ D = -2 * (LL_{\\mbox{reduced}} - LL_{\\mbox{full}}) \\] where \\(LL_{\\mbox{reduced}}\\)$ refers to the log-likelihood of a reduced (smaller) model, as compared to \\(LL_{\\mbox{full}}\\), which refers to the full model. The ‘full’ model remains a statistical abstract, and usually we are comparing two reduced models, one of which is nested inside the other. The discussion then becomes one of differences in deviance \\[ D = -2 * (LL_{\\mbox{smaller}} - LL_{\\mbox{larger}}) \\] In Bayesian stats, we usually drop the ill-defined ‘full model’ in lieu of simpler definition of ‘deviance’ (which reflects how we use deviance in practice anyways) \\[ D = -2 * LL \\] (In other words, we now just defined deviance without reference to some mythical full model.) The parameter estimates that maximize the likelihood (i.e. the MLEs) are equivalent to the mode of the posterior distribution if non-informative priors are used. (Proving this isn’t trivial, so for now accept this as a fact, though it should be a fairly plausible fact.) Accordingly, the deviance calculated with the posterior mode of each of the parameters will yield the minimum deviance. Put another way, in Bayesian inference, the model with the lowest expected deviance has the highest posterior probability. Since posterior modes are much harder to calculate than posterior means, we usually exploit the symmetry of the posteriors (and hope that they are symmetric) and calculate D with the mean of the posteriors, i.e. \\[ \\hat{D} = D(\\bar{\\theta_{1}},\\bar{\\theta_{2}},\\mbox{etc.}) \\] Easy enough, right? Sadly, no. Because we know that we can always improve fit by adding complexity, but for complex models we have no way to calculate the model complexity. (This will become clearer when we get to hierarchical models in which the number of parameters in the model isn’t even really an integer…) Spiegelhalter et al. (2002) derived an estimate of the number of effective parameters \\[ p_{D} = \\bar{D}-\\hat{D} \\] where \\(\\bar{D}\\) is the mean of the posterior deviance. I won’t make you read the original paper on this, since its pretty dense, and likewise I find McCarthy’s explanation for its intuitiveness rather unconvincing. Nevertheless, this is one way to estimate the effective number of parameters, and it can be generated automatically in WinBUGS so its used quite commonly. (I find this more convincing in practice, as we will demonstrate in lab, because non-hierarchical models with little prior information yield \\(p_{D} \\sim\\) true number of parameters.) (I have assigned the much newer Spiegelhalter et al. (2014) paper because it nicely summarizes the controversies surrounding DIC and reminds us that all this is still a matter of active development among statisticians.) Now we have a way of quantifying model fit (\\(\\hat{D}\\)) and a way of quantifying model complexity (\\(p_{D}\\)). These are combined into the Deviance Information Criterion, which is a direct analog of Aikake’s Information Criterion. \\[ DIC = \\hat{D} + 2p_{D} \\] We can interpret DIC similar to AIC, in that DICs within $$2 are considered equivalent, those falling 2-4 units below the best model having somewhat less support etc. Criticisms of DIC (from Spiegelhalter et al. 2014): a) \\(p_{D}\\) is not invariant to re-parameterization: For example, if you were to rewrite your model in terms of \\(log(\\sigma)\\) instead of \\(\\sigma\\), you would get a different \\(p_{D}\\) even if the priors were made equivalent. (\\(p_{D}\\) can also be negative \\(\\dots\\) hmmm \\(\\dots\\) hardly convincing) b) Lack of consistency (As the size of the dataset grows to infinity, DIC may not converge to the true model) c) Not based on a proper predictive criterion d) “weak” theoretical justification The DIC is an omnibus measure of fit, and its use is fairly contested among statisticians. (I think some of the complaint is that non-statisticians use it blindly without any knowledge of what it actually is or what the caveats might be.) We will discuss a fourth method, Bayes Factors, next week when we dig deeper into the world of multimodel inference. "],
["week-14-lab.html", "25 Week 14 Lab", " 25 Week 14 Lab We are going to redo the analysis by Ward (2008) but using the time series we already worked on last week “voles_data.txt” and other times series provided by Clark et al. (2010). I want you to fit the four different population models used by Ward: Geometric model \\[ N_{t+1} = N_{t}(1+r) \\] (2) Logistic model \\[ N_{t+1} = N_{t} + r N_{t}(1-N_{t}/K) \\] Theta-logistic model \\[ N_{t+1} = N_{t} + r N_{t}(1-(N_{t}/K)^{\\phi}) \\] (4) model with decreased growth rate at low density \\[ N_{t+1} = N_{t} + r N_{t}(N_{t}-a)(K-N_{t})/K^{2} \\] I want you to fit these models using both likelihood and Bayesian methods, assuming log-normally distributed observation error only. (You have most of the code already from previous exercises.) Using AICc, BIC, and DIC, compare the four models for the vole dataset. Do they rank the four models in a consistent way? If not, why not? Does the effective number of parameters \\(p_{D}\\) for DIC seem reasonable? Do a posterior predictive check using a metric of fit of your choice. What test statistic might be relevant here? "],
["week-15-lecture.html", "26 Week 15 Lecture 26.1 A quick step back: What are the goals of model selection? 26.2 Bayesian model averaging 26.3 Beyond Bayes Factors 26.4 Variable selection for nested models 26.5 Prior-data conflict", " 26 Week 15 Lecture Whereas DIC is analogous to AIC, Bayes Factors are analogous to likelihood ratios. We can extend Bayes theorem to compare multiple models as follows: \\[ \\frac{P(M_{1}|D)}{P(M_{2}|D)} = \\frac{P(M_{1})P(D|M_{1})}{P(M_{2})P(D|M_{2})} = \\frac{P(M_{1})}{P(M_{2})} \\times \\frac{P(D|M_{1})}{P(D|M_{2})} \\] (The denominators involved have canceled, see McCarthy.) The term on the left hand side is the odds ratio. The first term on the right-hand-side is simply the ratio of the prior probability of Model 1 vs. Model 2. The second term on the right-hand-side is known as the “Bayes factor”. (This is similar to the likelihood ratio, but the LR is based on those parameters that maximize the likelihood, whereas the Bayes factor integrates over the parameter priors.) So this equation works out to be \\[ \\mbox{posterior odds} = \\mbox{Bayes factor} \\times \\mbox{prior odds} \\] In the very simplest case in which you are comparing two point hypotheses (in other words, comparing two models which postulate different point estimates for the parameter in question), the Bayes factor works out to be the likelihood ratio. However, generally speaking, we have to integrate over the prior distribution under each model, e.g., \\[ P(D|M_{1}) = \\int p(D|\\theta_{M1})P(\\theta_{M1})d\\theta \\] Noting that this is the same as \\[ P(D|M_{1}) = \\int p(D,\\theta_{M1})d\\theta \\] we can see that what we have done is calculate the marginal probability of the data by integrating out the value of the parameter. Even in fairly simple cases, evaluating the Bayes factor can be difficult, because it often has to be done numerically. Where does parsimony come into this? While we tend to take it for granted that the simplest model is always preferred (all else being equal), there is nothing sacred about this, and sometimes you might prefer the more complex model because it includes covariates that you think are biologically relevant even if the data would not otherwise support its inclusion in the best model. Bayes Factors will automatically select for the simpler model (see Jefferys and Berger [1992]), but additionally parsimony can be included in this framework by placing a higher prior probability on smaller models. Likewise, if you think a covariate should be included, but aren’t absolutely stuck on including it at any cost, you may give higher prior weights to models that include this covariate. If the data suggest that the models with this covariate are so much worse than the ones without it, that the posterior probability for the models will tell you that. You could be leaning one way before the model fitting, but allow overwhelming evidence to tilt you another way. 26.1 A quick step back: What are the goals of model selection? Its worth pausing for a minute to discuss what the goals of model selection are. If you are considering different models, you really have two options. Option #1 is to find the one “best” model, and use that as “the model” for your data. Option #2 is to weight the models in some way and to base your inference on the set of models and their relative model weights. This latter option is akin to calculating AIC model weights and doing weighted averages for model parameters. 26.2 Bayesian model averaging We’ve talked on a number of occasions about model averaging approaches, and we can do model averaging in a Bayesian context using the posterior model probabilities. Lunn et al. discuss the difference between “M-closed” situations – in which the true process generating the data is assumed to be in the model set – and “M-open” situations in which one does not believe that any of the models being compared are strictly true. Strictly speaking, Bayesian model averaging assumes truth is in the model set, because the posterior model probabilities really are interpreted as a probability that a model is true. You can also do model averaging using the DIC, just as we did in Biometry using AIC model weights, except that now we are not interpreting the model weights as a probability of model truth, rather we interpret these as a measure of predictive ability. (In other words, model averaging proceeds the same way whether we use the posterior model probabilities or DIC model weights, but the interpretation is different.) Lunn et al. also introduce a bootstrapping approach to Bayesian model averaging. sample with replacement from the original data choose the best-fitting model according to some criterion repeat a large number of times if a model is selected as the best model r\\(\\%\\) of the time, we can interpret that as the model probability \\(p(M|\\mbox{data})\\) I haven’t seen this done in ecological practice and while Lunn et al. provide a couple of references (but only a couple!), its not clear that this approach is actually used very often. This might be worth exploring if you find yourself doing Bayesian model selection, but I don’t think its well developed enough to be used without careful consideration. 26.3 Beyond Bayes Factors Unfortunately, Bayes Factors can be difficult to calculate, and so they are rarely used to compare large numbers of models. In many cases, it is better to include the model selection process into the initial Bayesian model. In other word, instead of fitting two models and post hoc deciding which is better, we include both models in the MCMC sampling and allow the samplers to jump between the two models (remember Metropolis-Hastings?). One of the strategies for doing this is called Reversible Jump MCMC (RJMCMC), and it basically allows for “proposals” both within a model (proposals of new parameters) and between models (i.e. you can make jumps between models). In practice this gets a bit complicated, and so we won’t go into more detail. If you do end up going down this road of Bayesian model selection, RJMCMC is worth exploring further. 26.4 Variable selection for nested models If you have a comprehensive model with many covariates, and your interest is in paring this down to a smaller set of covariates to make the most parsimonious model, that you can use another trick developed by Kuo and Mallick. This procedure is described in nice detail by Darren Wilkinson on his blog, which I have posted on Bb for you to read. 26.5 Prior-data conflict What do we do if the prior and the data do not agree? Perhaps you have an informative prior based on previous work, but it occupies almost non-overlapping parameter space with the data you have in hand? In these cases you have to be careful, as you can end up with posteriors that are the prior and the likelihood, but representing the prior nor the likelihood. (Lunn et al. cite Stephen Senn’s characterization of this: “a Bayesian is someone who, suspecting a donkey and catching a glimpse of what looks like a horse, strong concludes he has seen a mule”.) One solution (see Lunn and references therein) is to choose a prior that is a mixture distribution of the informed prior and an uninformative prior; in a vague sense this will force the Bayesian model to choose between these two competing prior (informed vs vague). In practice, this is well into messy Bayesian territory; the actual details here would likely take you so far astray of the science you are interested in that it may be time to go back to the beginning and think about the modeling approach from scratch. "]
]
