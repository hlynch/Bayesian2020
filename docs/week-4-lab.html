<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Week 4 Lab | Bayesian Data Analysis and Computation Lecture and Lab Notes</title>
  <meta name="description" content="1 Week 4 Lab | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Week 4 Lab | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Week 4 Lab | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2020-09-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="week-7-lab.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#how-do-we-interpret-the-posteriors"><i class="fa fa-check"></i><b>0.1</b> How do we interpret the posteriors?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#a-slight-detour-to-get-us-thinking-about-the-basic-philosophy-behind-bayesian-stats"><i class="fa fa-check"></i><b>0.2</b> A slight detour, to get us thinking about the basic philosophy behind Bayesian stats</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#getting-some-more-practice-with-jags"><i class="fa fa-check"></i><b>0.3</b> Getting some more practice with JAGS</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>1</b> Week 4 Lab</a><ul>
<li class="chapter" data-level="1.1" data-path="week-4-lab.html"><a href="week-4-lab.html#smith-and-gelfand-1992"><i class="fa fa-check"></i><b>1.1</b> Smith and Gelfand (1992)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-7-lab.html"><a href="week-7-lab.html"><i class="fa fa-check"></i><b>2</b> Week 7 Lab</a><ul>
<li class="chapter" data-level="2.1" data-path="week-7-lab.html"><a href="week-7-lab.html#in-sum."><i class="fa fa-check"></i><b>2.1</b> In sum….</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>3</b> Week 9 Lab</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-4-lab" class="section level1">
<h1><span class="header-section-number">1</span> Week 4 Lab</h1>
<p>In lab this week, we are going to play around with writing R code to actually do some sampling.</p>
<p>The goal of this week was to introduce you to various alternative methods of sampling from a distribution, with the ultimate aim of being able to sample from the posterior distribution in a Bayesian analysis.</p>
<p>We have two methods in hand to make draws from an unknown distribution: Rejection Sampling, and Sampling Importance Resampling. Let’s take the following PDF, which is not one of the distributions built-into R and therefore not one we have an easy means of drawing samples from:</p>
<p><span class="math display">\[
f(x|\sigma) = \frac{1}{2\sigma}e^{-|x|/\sigma}
\]</span></p>
<p>Our choice of <span class="math inline">\(\sigma\)</span> here is arbitrary, so lets work with <span class="math inline">\(\sigma=2\)</span>.</p>
<p><strong>Exercise 1</strong>: Write a script to generate samples from this distribution using Rejection Sampling (RS). Keep in mind that this distribution is valid (i.e. ‘has support’) for all <span class="math inline">\(x \in (-\infty,+\infty)\)</span> and therefore your candidate distribution has to also have support over that range.</p>
<p><strong>Exercise 2</strong>: Write a script to generate samples from this distribution using Sampling Importance Resampling (SIR). This is quite similar to rejection sampling except that it does not require you to find a constant M that ensures that your candidate function is always larger than your target function. (This can be handy when your target function is unknown.)</p>
<p>Pseudo code for SIR:</p>
<ul>
<li><p>Sample a large number of random values from a candidate distribution with support over the same range of x values as the target distribution.</p></li>
<li><p>Find the probability of obtaining those values from the target distribution (i.e. the probability density at each <span class="math inline">\(x\)</span> value drawn).</p></li>
<li><p>Normalize these probabilities from Step 2 so they sum to 1.</p></li>
<li><p>Use the (now normalized) probabilities from Step 3 as weights in a resampling of the random values from Step 1. In other words, use the ‘sample’ function in R to sample with replacement from the values drawn in Step 1, and use the probabilities from Step 3 as weights for that bootstrap sampling.</p></li>
<li><p>The samples from Step 4 are the draws from the unknown distribution!</p></li>
</ul>
<p><strong>Exercise 3</strong>: Calculate the E[X] of this distribution using either the samples from IS or those from SIR. (If you did everything correctly, they should be roughly the same.) (Stop and work out what E[X] should be mathematically.) Look back at the Week #4 Lecture and make sure you see why this procedure is closely related to the idea of Monte Carlo Integration. What is the function <span class="math inline">\(g(x)\)</span> in this case?</p>
<p><strong>Exercise 4</strong>: How good is your estimate from Exercise 3? (i.e., calculate the standard error on E[X]. How do we do this? We can either bootstrap from our samples OR [perhaps even better] calculate the SE by actually drawing new sets of samples altogether.)</p>
<div id="smith-and-gelfand-1992" class="section level2">
<h2><span class="header-section-number">1.1</span> Smith and Gelfand (1992)</h2>
<p>Key points:</p>
<ul>
<li><p>Bayesian statistics is all about using the data to go from a prior distribution for model parameters to a posterior distribution for model parameters. In some cases, this can be done directly (e.g., when we have conjugate priors). More often than not, this cannot be done directly. In these cases, we have to settle for a somewhat indirect approach focused on using the data to go from samples from the prior distribution to samples from the posterior distribution. We have replaced manipulations of the pdfs, with stochastic samples from those pdfs.</p></li>
<li><p>Rejection methods require that you can calculate some number M such that the ratio of the candidate distribution to the target distribution is always greater than or equal to one. Sometimes this isn’t possible. In these cases, Smith and Gelfand (1992) introduce another approach called the weighted bootstrap approach. What is it, or in what way is it related to a simple bootstrap? Note that in other places, this approach is called SIR=Sampling – Importance Resampling.</p></li>
</ul>
<p>We will now work through the binomial example discussed by Smith and Gelfand (1992). The basic premise is this: Let’s say you have two Binomial variables</p>
<p><span class="math display">\[
X_{i1} \sim Binomial(n_{i1},\theta_{1}) \\
X_{i2} \sim Binomial(n_{i2},\theta_{2})
\]</span> conditionally independent given <span class="math inline">\(n_{i1}\)</span>, <span class="math inline">\(n_{i2}\)</span>,<span class="math inline">\(\theta_{1}\)</span>, and <span class="math inline">\(\theta_{2}\)</span>. Let’s say that you don’t observe <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(X_{i2}\)</span> directly, but only their sum</p>
<p><span class="math display">\[
Y_{i} = X_{i1}+X_{i2} 
\]</span> For simplicity sake, let’s assume you have three data points, so <span class="math inline">\(i=1,2,3\)</span>.</p>
<p>I replicate the table from Smith and Gelfand here:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">i=1</th>
<th align="center">i=2</th>
<th align="center">i=3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(n_{i1}\)</span></td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n_{i2}\)</span></td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(y_{i}\)</span></td>
<td align="center">7</td>
<td align="center">5</td>
<td align="center">6</td>
</tr>
</tbody>
</table>
<p>So, in words, the situation is this. You have two people flipping coins, and in each trial you know how many each person flipped but you only how many heads they had in total. You want to estimate the probability of each person getting a head.</p>
<p>Before launching into a Bayesian analysis of this, let’s just work out what we would expect using plain-old maximum likelihood estimation.</p>
<p>The likelihood from Smith and Gelfand has two typos, so I am reproducing the correct likelihood below:</p>
<p><span class="math display">\[
\prod^{3}_{i=1}\left[\sum_{ji} {n_{i1} \choose j_i}{n_{i2} \choose y_i-j_i}\theta_{1}^{j_{i}}(1-\theta_{1})^{n_{i1}-j_{i}}\theta_{2}^{y_{i}-j_{i}}(1-\theta_{2})^{n_{i2}-y_{i}+j_{i}}\right]
\]</span> Note that the bounds on <span class="math inline">\(j_{i}\)</span> are as follows: <span class="math inline">\(max(0,y_{i}-n_{i2}) \leq j_{i} \leq min(n_{i1},y_{i})\)</span>.</p>
<p><strong>Important!</strong> They don’t say in the paper, but Smith and Gelfand drop the factorials from their likelihood <span class="math inline">\({n_{i1} \choose j_{i}}{n_{i2} \choose y_{i}-j_{i}}\)</span>. To match with their figure, you should do so as well. The constant does not change the <span class="math inline">\((\theta_{1},\theta_{2})\)</span> location of the peaks of the posterior distribution. However, it does change the posterior and strictly speaking, Smith and Gelfand are incorrect. (In other words, the peaks are still in the same place but the shape of the likelihood is changed.) To get the correct posterior likelihood, the factorials should be left in. But for the purposes of the problem set, we will also drop these terms because doing so will allow us to reproduce what Smith and Gelfand have done.</p>
<p><strong>Exercise 5</strong>: Use a grid search to find the MLEs. Plot the two-dimensional likelihood (since we left off the constants in step 1, this will be an unscaled likelihood, but it doesn’t matter for our purposes)</p>
<p>To apply a Bayesian analysis, we need prior distributions for <span class="math inline">\(\theta_{1}\)</span> and <span class="math inline">\(\theta_{2}\)</span>. We will follow Smith and Gelfand’s lead and just use the unit square (so Unif(0,1) for both). What other prior distributions might we have used?</p>
<p>We will use sampling importance resampling to sample from the posterior distribution.</p>
<p>To do this, first generate a sample from the prior distributions to get prior samples of <span class="math inline">\((\theta_{1},\theta_{2})\)</span>. Then calculate the likelihood of the data for each sample from the prior <span class="math inline">\((\theta_{1},\theta_{2})\)</span>.</p>
<p>The probability of resampling each sample from the prior is</p>
<p><span class="math display">\[
q = \frac{\mbox{Likelihood}(\theta_{1},\theta_{2};y)}{\sum \mbox{Likelihood}(\theta_{1},\theta_{2};y)}
\]</span> Use this procedure to generate a posterior distribution for <span class="math inline">\(\theta_{1}\)</span> and <span class="math inline">\(\theta_{2}\)</span>.</p>
<p><strong>Exercise 6</strong>: Plot the posterior distribution in the manner of Smith and Gelfand (1992) Figure 2 on top of the likelihood surface from question 2. (It might be easier to visualize if you plot the surface as a contour plot, rather than as a color image.) Are they similar? Different?</p>
<p><strong>Exercise 7</strong>: What does this procedure tell us about how the support for the prior (the range of values “permitted” under the prior for which the pdf&gt;0) affects the support for the posterior? In light of this, why does Figure 2 from Smith and Gelfand (1992) look funny?</p>
<p><strong>Exercise 8</strong>: What would the posterior samples look like if the priors for <span class="math inline">\(\theta_{1}\)</span> and <span class="math inline">\(\theta_{2}\)</span> were drawn from Unif(0,0.5) instead of Unif(0,1)?</p>





</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-7-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
