<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>24 Week 14 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes</title>
  <meta name="description" content="24 Week 14 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="24 Week 14 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="24 Week 14 Lecture | Bayesian Data Analysis and Computation Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2020-09-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-13-lecture.html"/>
<link rel="next" href="week-14-lab.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a><ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#introduction-to-this-course"><i class="fa fa-check"></i><b>1.1</b> Introduction to this course</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#some-probability-vocabulary"><i class="fa fa-check"></i><b>1.2</b> Some probability vocabulary</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#statistical-philosophy-and-the-foundations-of-bayesian-analysis"><i class="fa fa-check"></i><b>1.3</b> Statistical philosophy and the foundations of Bayesian analysis</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#testing-jags-installation"><i class="fa fa-check"></i><b>1.4</b> Testing JAGS installation</a></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#for-more-information-about-this-weeks-topic"><i class="fa fa-check"></i><b>1.5</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab.html"><a href="week-1-lab.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab</a></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#bayes-theorem-and-all-that-follows-from-it"><i class="fa fa-check"></i><b>3.1</b> Bayes Theorem and all that follows from it</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#how-do-we-interpret-the-posteriors"><i class="fa fa-check"></i><b>3.2</b> How do we interpret the posteriors?</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#a-slight-detour-to-get-us-thinking-about-the-basic-philosophy-behind-bayesian-stats"><i class="fa fa-check"></i><b>3.3</b> A slight detour, to get us thinking about the basic philosophy behind Bayesian stats</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#getting-some-more-practice-with-jags"><i class="fa fa-check"></i><b>3.4</b> Getting some more practice with JAGS</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#for-more-information-about-this-weeks-topic-1"><i class="fa fa-check"></i><b>3.5</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>4</b> Week 3 Lecture</a><ul>
<li class="chapter" data-level="4.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#how-do-we-obtain-priors"><i class="fa fa-check"></i><b>4.1</b> How do we obtain priors?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#published-literature-as-a-source-of-prior-information"><i class="fa fa-check"></i><b>4.1.1</b> Published literature as a source of prior information</a></li>
<li class="chapter" data-level="4.1.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#expert-opinion"><i class="fa fa-check"></i><b>4.1.2</b> Expert opinion</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#conjugacy"><i class="fa fa-check"></i><b>4.2</b> Conjugacy</a></li>
<li class="chapter" data-level="4.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#sensitivity-analysis"><i class="fa fa-check"></i><b>4.3</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="4.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#for-more-information-about-this-weeks-topic-2"><i class="fa fa-check"></i><b>4.4</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lab.html"><a href="week-3-lab.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lab</a><ul>
<li class="chapter" data-level="5.1" data-path="week-3-lab.html"><a href="week-3-lab.html#congugacy"><i class="fa fa-check"></i><b>5.1</b> Congugacy</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lab.html"><a href="week-3-lab.html#moment-matching-two-distributions"><i class="fa fa-check"></i><b>5.2</b> Moment Matching two distributions</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lab.html"><a href="week-3-lab.html#from-prior-to-posterior-to-prior"><i class="fa fa-check"></i><b>5.3</b> From Prior to Posterior to Prior</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lab.html"><a href="week-3-lab.html#adding-data-one-at-a-time-or-all-at-once"><i class="fa fa-check"></i><b>5.4</b> Adding data: One at a time or all at once?</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lab.html"><a href="week-3-lab.html#what-impact-did-the-choice-of-prior-have"><i class="fa fa-check"></i><b>5.5</b> What impact did the choice of prior have?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>6</b> Week 4 Lecture</a><ul>
<li class="chapter" data-level="6.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#conjugacy-aside-how-to-actually-calculate-the-posterior"><i class="fa fa-check"></i><b>6.1</b> Conjugacy aside, how to actually calculate the posterior</a></li>
<li class="chapter" data-level="6.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#monte-carlo-methods"><i class="fa fa-check"></i><b>6.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="6.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#rejection-sampling"><i class="fa fa-check"></i><b>6.3</b> Rejection Sampling</a></li>
<li class="chapter" data-level="6.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#adaptive-rejection-sampling"><i class="fa fa-check"></i><b>6.4</b> Adaptive Rejection Sampling</a></li>
<li class="chapter" data-level="6.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#monte-carlo-integration"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="6.6" data-path="week-4-lecture.html"><a href="week-4-lecture.html#sometimes-you-just-want-the-integral"><i class="fa fa-check"></i><b>6.6</b> Sometimes you just want the integral…</a></li>
<li class="chapter" data-level="6.7" data-path="week-4-lecture.html"><a href="week-4-lecture.html#importance-sampling"><i class="fa fa-check"></i><b>6.7</b> Importance Sampling</a></li>
<li class="chapter" data-level="6.8" data-path="week-4-lecture.html"><a href="week-4-lecture.html#sampling-importance-resampling"><i class="fa fa-check"></i><b>6.8</b> Sampling Importance Resampling</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lab</a><ul>
<li class="chapter" data-level="7.1" data-path="week-4-lab.html"><a href="week-4-lab.html#smith-and-gelfand-1992"><i class="fa fa-check"></i><b>7.1</b> Smith and Gelfand (1992)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>8</b> Week 5 Lecture</a><ul>
<li class="chapter" data-level="8.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.1</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="8.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#metropolis-algorithm"><i class="fa fa-check"></i><b>8.2</b> Metropolis algorithm</a></li>
<li class="chapter" data-level="8.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-messy-reality-hybrid-of-m-h-and-gibbs"><i class="fa fa-check"></i><b>8.3</b> The Messy reality = Hybrid of M-H and Gibbs</a></li>
<li class="chapter" data-level="8.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#convergence"><i class="fa fa-check"></i><b>8.4</b> Convergence</a></li>
<li class="chapter" data-level="8.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#bayesian-change-point-example"><i class="fa fa-check"></i><b>8.5</b> Bayesian change point example</a></li>
<li class="chapter" data-level="8.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#hierarchical-model"><i class="fa fa-check"></i><b>8.6</b> Hierarchical model</a></li>
<li class="chapter" data-level="8.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#for-more-information-about-this-weeks-topic-3"><i class="fa fa-check"></i><b>8.7</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lab</a><ul>
<li class="chapter" data-level="9.1" data-path="week-5-lab.html"><a href="week-5-lab.html#gibbs-sampler"><i class="fa fa-check"></i><b>9.1</b> Gibbs Sampler</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>10</b> Week 6 Lab</a><ul>
<li class="chapter" data-level="10.1" data-path="week-6-lab.html"><a href="week-6-lab.html#fitting-a-distribution"><i class="fa fa-check"></i><b>10.1</b> Fitting a distribution</a></li>
<li class="chapter" data-level="10.2" data-path="week-6-lab.html"><a href="week-6-lab.html#one-way-anova"><i class="fa fa-check"></i><b>10.2</b> One-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-7-lecture.html"><a href="week-7-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 7 Lecture</a><ul>
<li class="chapter" data-level="11.1" data-path="week-7-lecture.html"><a href="week-7-lecture.html#class-projects"><i class="fa fa-check"></i><b>11.1</b> Class projects</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="week-7-lab.html"><a href="week-7-lab.html"><i class="fa fa-check"></i><b>12</b> Week 7 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>13</b> Week 8 Lecture</a></li>
<li class="chapter" data-level="14" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lab</a></li>
<li class="chapter" data-level="15" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>15</b> Week 9 Lecture</a><ul>
<li class="chapter" data-level="15.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#the-probability-of-estimability"><i class="fa fa-check"></i><b>15.1</b> The probability of estimability</a></li>
<li class="chapter" data-level="15.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#multilevel-modelling-ala-gelman-and-hill"><i class="fa fa-check"></i><b>15.2</b> Multilevel modelling ala Gelman and Hill</a></li>
<li class="chapter" data-level="15.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#in-sum."><i class="fa fa-check"></i><b>15.3</b> In sum….</a></li>
<li class="chapter" data-level="15.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#nice-et-al.-2014"><i class="fa fa-check"></i><b>15.4</b> Nice et al. (2014)</a></li>
<li class="chapter" data-level="15.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#for-more-information-about-this-weeks-topic-4"><i class="fa fa-check"></i><b>15.5</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lab</a></li>
<li class="chapter" data-level="17" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>17</b> Week 10 Lecture</a><ul>
<li class="chapter" data-level="17.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#convergence-1"><i class="fa fa-check"></i><b>17.1</b> Convergence</a></li>
<li class="chapter" data-level="17.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#testing-for-convergence"><i class="fa fa-check"></i><b>17.2</b> Testing for convergence</a></li>
<li class="chapter" data-level="17.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#gelman-rubin-statistic"><i class="fa fa-check"></i><b>17.3</b> Gelman-Rubin statistic</a></li>
<li class="chapter" data-level="17.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#the-take-away-what-should-we-be-checking-after-we-run-our-models"><i class="fa fa-check"></i><b>17.4</b> The take away: What should we be checking after we run our models</a></li>
<li class="chapter" data-level="17.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#missing-data"><i class="fa fa-check"></i><b>17.5</b> Missing data</a></li>
<li class="chapter" data-level="17.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#initial-values"><i class="fa fa-check"></i><b>17.6</b> Initial values</a></li>
<li class="chapter" data-level="17.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#sample-scripts-and-output-for-prior-posterior-overlap"><i class="fa fa-check"></i><b>17.7</b> Sample scripts and output for prior-posterior overlap</a></li>
<li class="chapter" data-level="17.8" data-path="week-10-lecture.html"><a href="week-10-lecture.html#for-more-information-about-this-weeks-topic-5"><i class="fa fa-check"></i><b>17.8</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lab</a></li>
<li class="chapter" data-level="19" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>19</b> Week 11 Lecture</a><ul>
<li class="chapter" data-level="19.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#dynamical-time-series-models"><i class="fa fa-check"></i><b>19.1</b> Dynamical (time series) models</a></li>
<li class="chapter" data-level="19.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#process-error"><i class="fa fa-check"></i><b>19.2</b> Process error</a></li>
<li class="chapter" data-level="19.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#observation-error"><i class="fa fa-check"></i><b>19.3</b> Observation error</a></li>
<li class="chapter" data-level="19.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#other-kinds-of-statespace-models"><i class="fa fa-check"></i><b>19.4</b> Other kinds of state=space models</a></li>
<li class="chapter" data-level="19.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#missing-data-1"><i class="fa fa-check"></i><b>19.5</b> Missing data</a></li>
<li class="chapter" data-level="19.6" data-path="week-11-lecture.html"><a href="week-11-lecture.html#for-more-information-about-this-weeks-topic-6"><i class="fa fa-check"></i><b>19.6</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lab</a><ul>
<li class="chapter" data-level="20.1" data-path="week-11-lab.html"><a href="week-11-lab.html#simple-logistic"><i class="fa fa-check"></i><b>20.1</b> Simple logistic</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lab.html"><a href="week-11-lab.html#observation-error-only-model"><i class="fa fa-check"></i><b>20.2</b> Observation-error-only model</a></li>
<li class="chapter" data-level="20.3" data-path="week-11-lab.html"><a href="week-11-lab.html#process-error-only-model"><i class="fa fa-check"></i><b>20.3</b> Process-error-only model</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lab.html"><a href="week-11-lab.html#process-error-and-observation-error-together"><i class="fa fa-check"></i><b>20.4</b> Process-error and observation-error together</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lab.html"><a href="week-11-lab.html#final-thoughts"><i class="fa fa-check"></i><b>20.5</b> Final thoughts</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>21</b> Week 12 Lecture</a><ul>
<li class="chapter" data-level="21.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mark-recapture-modeling"><i class="fa fa-check"></i><b>21.1</b> Mark-recapture modeling</a></li>
<li class="chapter" data-level="21.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#cormack-jolly-seber"><i class="fa fa-check"></i><b>21.2</b> Cormack-Jolly-Seber</a></li>
<li class="chapter" data-level="21.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#method-1-brute-force"><i class="fa fa-check"></i><b>21.3</b> Method #1: Brute force</a></li>
<li class="chapter" data-level="21.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#method-2-modeling-the-entire-capture-history"><i class="fa fa-check"></i><b>21.4</b> Method #2: Modeling the entire capture history</a></li>
<li class="chapter" data-level="21.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#what-other-kind-of-models-might-you-fit"><i class="fa fa-check"></i><b>21.5</b> What other kind of models might you fit</a></li>
<li class="chapter" data-level="21.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#occupancy-modelling"><i class="fa fa-check"></i><b>21.6</b> Occupancy modelling</a></li>
<li class="chapter" data-level="21.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#dynamic-state-space-models-for-meta-population-dynamics"><i class="fa fa-check"></i><b>21.7</b> Dynamic state-space models for meta-population dynamics</a></li>
<li class="chapter" data-level="21.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#for-more-information-about-this-weeks-topic-7"><i class="fa fa-check"></i><b>21.8</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lab</a><ul>
<li class="chapter" data-level="22.1" data-path="week-12-lab.html"><a href="week-12-lab.html#the-zeros-trick"><i class="fa fa-check"></i><b>22.1</b> The ‘zeros’ trick</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lab.html"><a href="week-12-lab.html#the-ones-trick"><i class="fa fa-check"></i><b>22.2</b> The ‘ones’ trick</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lab.html"><a href="week-12-lab.html#initial-values-1"><i class="fa fa-check"></i><b>22.3</b> Initial values</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lab.html"><a href="week-12-lab.html#first-a-warm-up"><i class="fa fa-check"></i><b>22.4</b> First, a warm up</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lab.html"><a href="week-12-lab.html#fitting-mark-recapture-models"><i class="fa fa-check"></i><b>22.5</b> Fitting mark-recapture models</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>23</b> Week 13 Lecture</a><ul>
<li class="chapter" data-level="23.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#rejection-abc"><i class="fa fa-check"></i><b>23.1</b> Rejection ABC</a></li>
<li class="chapter" data-level="23.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-1-basic-rejection-abc"><i class="fa fa-check"></i><b>23.2</b> Option #1: Basic rejection ABC</a></li>
<li class="chapter" data-level="23.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-2-markov-chain-monte-carlo-abc"><i class="fa fa-check"></i><b>23.3</b> Option #2: Markov Chain Monte Carlo ABC</a></li>
<li class="chapter" data-level="23.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#option-3-sequential-monte-carlo-abc"><i class="fa fa-check"></i><b>23.4</b> Option #3: Sequential Monte Carlo ABC</a></li>
<li class="chapter" data-level="23.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#for-more-information-about-this-weeks-topic-8"><i class="fa fa-check"></i><b>23.5</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 14 Lecture</a><ul>
<li class="chapter" data-level="24.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#the-mona-lisa"><i class="fa fa-check"></i><b>24.1</b> The Mona Lisa</a></li>
<li class="chapter" data-level="24.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#for-more-information-about-this-weeks-topic-9"><i class="fa fa-check"></i><b>24.2</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>25</b> Week 14 Lab</a></li>
<li class="chapter" data-level="26" data-path="week-15-lecture.html"><a href="week-15-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 15 Lecture</a><ul>
<li class="chapter" data-level="26.1" data-path="week-15-lecture.html"><a href="week-15-lecture.html#a-quick-step-back-what-are-the-goals-of-model-selection"><i class="fa fa-check"></i><b>26.1</b> A quick step back: What are the goals of model selection?</a></li>
<li class="chapter" data-level="26.2" data-path="week-15-lecture.html"><a href="week-15-lecture.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>26.2</b> Bayesian model averaging</a></li>
<li class="chapter" data-level="26.3" data-path="week-15-lecture.html"><a href="week-15-lecture.html#beyond-bayes-factors"><i class="fa fa-check"></i><b>26.3</b> Beyond Bayes Factors</a></li>
<li class="chapter" data-level="26.4" data-path="week-15-lecture.html"><a href="week-15-lecture.html#variable-selection-for-nested-models"><i class="fa fa-check"></i><b>26.4</b> Variable selection for nested models</a></li>
<li class="chapter" data-level="26.5" data-path="week-15-lecture.html"><a href="week-15-lecture.html#prior-data-conflict"><i class="fa fa-check"></i><b>26.5</b> Prior-data conflict</a></li>
<li class="chapter" data-level="26.6" data-path="week-15-lecture.html"><a href="week-15-lecture.html#for-more-information-about-this-weeks-topic-10"><i class="fa fa-check"></i><b>26.6</b> For more information about this week’s topic</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Data Analysis and Computation Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-14-lecture" class="section level1">
<h1><span class="header-section-number">24</span> Week 14 Lecture</h1>
<p>Papers to read this week:</p>
<ul>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/GelmanShalizi2013.pdf">Gelman and Shalizi 2013</a></li>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/PittMyung2002.pdf">Pitt and Myung 2002</a></li>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/SpiegelhalterEtAl2014.pdf">Spiegelhalter et al. 2014</a></li>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/GreenlandPoole2013.pdf">Greenland and Poole 2013</a></li>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/Ward2008.pdf">Ward 2008</a></li>
</ul>
<p>We’ve spent the last few weeks learning how to run models in JAGS, and now we cycle back to some more basic questions about how hypothesis testing is actually done in a Bayesian framework. <strong>Everything covered this week is a matter of continuing development.</strong> Different experts and textbooks will disagree on the best method, and the use of any one method may draw ire from reviewers. There are no easy and simply ways to check that your model works; these problems are the same as those in frequentist statistics except that the field is newer and so the approaches are still a matter of some debate and discussion.</p>
<p>We usually have two questions when fitting models with data:</p>
<ol style="list-style-type: decimal">
<li><p>How do the models compare to one another?</p></li>
<li><p>How do the models compare to the data?</p></li>
</ol>
<p>We have to be careful that an excessive concern over model comparison does not keep up from asking the very fundamental question: Are any of these models any good? If none of the models are very good, the whole exercise of model comparison may be a waste of time. (But maybe not – perhaps you are interested in statistically significant covariates even though they explain little of the variation in the data.)</p>
<p>** Stop &amp; discuss: What makes a “good” model?**</p>
<p>McCarthy makes an important point when he says that the model should fit both the central tendency and the variation in the data. In other words, let us not forget that a model such as</p>
<p><span class="math display">\[
Y \sim N(\mu,\sigma^{2})
\]</span> includes both <span class="math inline">\(\mu\)</span> <strong>and</strong> <span class="math inline">\(\sigma^{2}\)</span>, the latter of which should not be dismissed as a nuisance parameter to the “main event” of <span class="math inline">\(\mu\)</span>. This is particularly important when building models to be used for prediction.</p>
<p><strong>Method #1: Plot the model against the data</strong></p>
<p>While it seems almost too simple to be of any use, you should always lot the model against the data. Sometimes, the parameters look fine but the fit is obviously horrible for one reason or another.</p>
<p><strong>Method #2: Posterior predictive checks</strong></p>
<p>An alternative to asking “How well does the model fit the data we have” is to ask “How well does the model predict new data given the fit to the old data”. To do this we calculate the posterior predictive distribution</p>
<p><span class="math display">\[
p(\tilde{y}|y) = \int p(\tilde{y}|\theta)p(\theta|y)d\theta
\]</span> <span class="math inline">\(\tilde{y}\)</span> = new data\ y = original data \ <span class="math inline">\(\theta\)</span> = model parameters \ <span class="math inline">\(p(\theta|y)\)</span> = posterior distribution of <span class="math inline">\(\theta\)</span></p>
<p><strong>Question: What is <span class="math inline">\(p(\tilde{y}|\theta)\)</span>?</strong></p>
<p>In other words, you want to integrate out the posterior distribution for the parameters to get the posterior distribution for the predicted values.</p>
<p>In practice this means doing the following:</p>
<p>Step #1: Sample <span class="math inline">\(m\)</span> values of <span class="math inline">\(\theta\)</span> from the posterior <span class="math inline">\(p(\theta|y)\)</span> Step #2: For each of these <span class="math inline">\(m\)</span> values, sample from the likelihood <span class="math inline">\(p(\tilde{y}|\theta)\)</span></p>
<p>These samples represent draws from the posterior predictive distribution <span class="math inline">\(p(\tilde{y}|y)\)</span>.</p>
<p>How do we use <span class="math inline">\(p(\tilde{y}|y)\)</span>? The most straightforward, and in practice recommended, way to use <span class="math inline">\(p(\tilde{y}|y)\)</span> is to check the model graphically. Does the model “look” correct for whatever key features you are interested in? Where it fails, does it tell you anything about the model? Does it tell you anything about the data?</p>
<p><strong>Remember: All models are wrong, but some are useful.</strong></p>
<p>A second, more quantitative but also probably less helpful, approach would be to do this following.</p>
<p>Step 1 <span class="math inline">\(\rightarrow\)</span> Come up with a ‘relevant’ test statistic <span class="math inline">\(T\)</span>, i.e. one that has the power to distinguish a good model from a bad model. Note that ‘good’ and ‘bad’ here are context dependent, because it depends on the goals of the modeling. You want to compare models based on the most important predictions, which may or may not be straightforward. In other words, in studying the population growth rate of a species, you may be interested in the maximum population growth rate, which is a function of other life history parameters such as age to maturity. It may be that you are not interested in age to maturity at all, except in as far as it influences maximum population growth rate. In this case, you should compare models based on growth rate and not age to maturity.</p>
<p>Often, we are interested in predicting what values we might see if we could “rewind time”; in these cases the posterior predictive distribution is compared against the real data to ask whether or not it is similar in ‘relevant’ ways.</p>
<p>Step 2 <span class="math inline">\(\rightarrow\)</span> Calculate T for the observed data <span class="math inline">\(y: T(y)\)</span>.</p>
<p>Step 3 <span class="math inline">\(\rightarrow\)</span> Calculate T for each draw from the posterior predictive distribution <span class="math inline">\(\tilde{y}: T(\tilde{y}|y)\)</span>.</p>
<p>Step 4 <span class="math inline">\(\rightarrow\)</span> Calculate the fraction of times that <span class="math inline">\(T(y) &lt; T(\tilde{y}|y)\)</span>. (I have flipped this statement around from Gelman and Chalizi 2013 because I think its easier to think of in this way). Sound familiar? It should. This is called the <strong>posterior predictive p-value</strong>. In other words, we are checking to see whether our data would be considered an extreme outcome of the model as defined by the test statistic. Let’s say that our test statistic exceeds the model prediction test statistic <span class="math inline">\(T(\tilde{y}|y)\)</span> for <span class="math inline">\(&gt;95\%\)</span> of model predicted data sets. In this case, the posterior predicted p-value would be <span class="math inline">\(&lt;0.05\)</span>, and we would say that we can reject the model as fitting the data. (Bayesians don’t use this phrase, but another way to think about it is that we have established a null hypothesis that the model does fit the data in as far as the characteristic measured by the test statistic <span class="math inline">\(T\)</span>, and a p-value<span class="math inline">\(&lt;0.05\)</span> would lead us to reject that null hypothesis.)</p>
<p>Recap: A very low p-value says that it is unlikely, , to have obtained data as extreme along the T-dimension as the actual y, i,.e. we are seeing something which would be highly improbable . In other words, you want your model to make your data look typical, not unusual.</p>
<p>Note that here we have used the data twice – once to build the model and then again to check the model. This is akin to looking at the <span class="math inline">\(R^2\)</span> of a traditional frequentist regression model. Sure, the model probably fits because it was designed to fit the data. This doesn’t really tell us all that much about whether the model would work on new data. As we discussed in Biometry (in our Shmueli discussion), one solution would be to withhold some data from the model fitting, and use only the withheld data to see if the model is any good (at making predictions).</p>
<p>Note that passing the “test” above does not mean the model is guaranteed to be good, because <strong>your test statistic T may have low power to detect certain violations</strong>. However, it may flag problems with the model that are worth investigating further.</p>
<div id="the-mona-lisa" class="section level2">
<h2><span class="header-section-number">24.1</span> The Mona Lisa</h2>
<p>What test statistics would you use to determine if a painting actually was the Mona Lisa? (Thanks to Stefano Allesina for the idea.)</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="MonaLisaOriginal.png" alt="The Mona Lisa. Source: Wikimedia Commons" width="25%" />
<p class="caption">
Figure 1.1: The Mona Lisa. Source: Wikimedia Commons
</p>
</div>
<p><details> <summary>Click for Answer</summary> <span style="color: blueviolet;"> Compare your test statistics to three “model” Mona Lisas – Were your test statistics powerful enough to reject any of the models as the real Mona Lisa?</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="MonaLisa1.png" alt="Fake Mona Lisa #1." width="25%" />
<p class="caption">
Figure 2.1: Fake Mona Lisa #1.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="MonaLisa2.png" alt="Fake Mona Lisa #2." width="25%" />
<p class="caption">
Figure 6.1: Fake Mona Lisa #2.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="MonaLisa3.png" alt="Fake Mona Lisa #3." width="25%" />
<p class="caption">
Figure 8.1: Fake Mona Lisa #3.
</p>
</div>
<p></span> </details></p>
<p>As I’ve tried to illustrate below, both frequentist and Bayesian statistics can be thought of as following the hypothetico-deductive framework, but the former relies on <span class="math inline">\(p(data|model)\)</span> and the latter on <span class="math inline">\(p(model|data)\)</span>.</p>
<p>Note that in Step #4 of the Bayesian workflow diagram, I’ve stated that the posterior predictive checks can be used to falsify a model. While true, this isn’t all that helpful, because we know a priori that <strong>all models are false</strong>. The way to think about this last step is that you are checking to see in what way your model is false and whether your model fails in ways that are important to your application.</p>
<p>(Posterior predictive p-values are not without their critics, and here I would refer to Andrew Gelman’s blog, where he airs out the dirty laundry of posterior predictive p-values [which he himself has helped develop].)</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="BayesianVsFrequentist.png" alt="Diagram illustrating how Bayesian and frequentist approaches integrate theory, data, and predictions." width="100%" />
<p class="caption">
Figure 24.1: Diagram illustrating how Bayesian and frequentist approaches integrate theory, data, and predictions.
</p>
</div>
<p><strong>Method #3: Deviance Information Criteria (DIC)</strong></p>
<p>Whereas Methods #1 and #2 relate to the question “Does my model fit to the data”, they do not directly address the question “Which of these two competing models is better?”. One way of comparing models in a Bayesian context is the Deviance Information Criterion.</p>
<p>First, a quick recap of Deviance.</p>
<p><strong>Likelihood</strong>: The likelihood of a model represents the likelihood (i.e., the probability) that you would have obtained the actual data given a particular model. This is almost always a very small number, since even the “true” model could yield a large number of outcomes, of which yours is just one. Assuming independent data points, the joint likelihood for the whole data set is simply the product of the likelihood of obtaining each data point, and so the joint likelihood is a very very small number (even at its maximum). (This was the moral of the M&amp;M problem set in Biometry.) To deal with these obnoxiously tiny numbers, we focus on the log-likelihood.</p>
<p><strong>Deviance</strong>: In Biometry, we introduced the idea of Deviance, which was defined as</p>
<p><span class="math display">\[
D = -2 * (LL_{\mbox{reduced}} - LL_{\mbox{full}})
\]</span> where <span class="math inline">\(LL_{\mbox{reduced}}\)</span>$ refers to the log-likelihood of a reduced (smaller) model, as compared to <span class="math inline">\(LL_{\mbox{full}}\)</span>, which refers to the full model. The ‘full’ model remains a statistical abstract, and usually we are comparing two reduced models, one of which is nested inside the other. The discussion then becomes one of differences in deviance</p>
<p><span class="math display">\[
D = -2 * (LL_{\mbox{smaller}} - LL_{\mbox{larger}})
\]</span></p>
<p>In Bayesian stats, we usually drop the ill-defined ‘full model’ in lieu of simpler definition of ‘deviance’ (which reflects how we use deviance in practice anyways)</p>
<p><span class="math display">\[
D = -2 * LL
\]</span></p>
<p>(In other words, we now just defined deviance without reference to some mythical full model.)</p>
<p>The parameter estimates that maximize the likelihood (i.e. the MLEs) are equivalent to the mode of the posterior distribution if non-informative priors are used. (Proving this isn’t trivial, so for now accept this as a fact, though it should be a fairly plausible fact.)</p>
<p>Accordingly, the deviance calculated with the posterior mode of each of the parameters will yield the minimum deviance. Put another way, in Bayesian inference, the model with the lowest expected deviance has the highest posterior probability. Since posterior modes are much harder to calculate than posterior means, we usually exploit the symmetry of the posteriors (and hope that they are symmetric) and calculate D with the mean of the posteriors, i.e.</p>
<p><span class="math display">\[
\hat{D} = D(\bar{\theta_{1}},\bar{\theta_{2}},\mbox{etc.})
\]</span></p>
<p>Easy enough, right? Sadly, no. Because we know that we can always improve fit by adding complexity, but for complex models we have no way to calculate the model complexity. (This will become clearer when we get to hierarchical models in which the number of parameters in the model isn’t even really an integer…)</p>
<p>Spiegelhalter et al. (2002) derived an estimate of the number of effective parameters</p>
<p><span class="math display">\[
p_{D} = \bar{D}-\hat{D}
\]</span> where <span class="math inline">\(\bar{D}\)</span> is the mean of the posterior deviance. I won’t make you read the original paper on this, since its pretty dense, and likewise I find McCarthy’s explanation for its intuitiveness rather unconvincing. Nevertheless, this is one way to estimate the effective number of parameters, and it can be generated automatically in WinBUGS so its used quite commonly. (I find this more convincing in practice, as we will demonstrate in lab, because non-hierarchical models with little prior information yield <span class="math inline">\(p_{D} \sim\)</span> true number of parameters.)</p>
<p>(I have assigned the much newer Spiegelhalter et al. (2014) paper because it nicely summarizes the controversies surrounding DIC and reminds us that all this is still a matter of active development among statisticians.)</p>
<p>Now we have a way of quantifying model fit (<span class="math inline">\(\hat{D}\)</span>) and a way of quantifying model complexity (<span class="math inline">\(p_{D}\)</span>). These are combined into the Deviance Information Criterion, which is a direct analog of Aikake’s Information Criterion.</p>
<p><span class="math display">\[
DIC = \hat{D} + 2p_{D}
\]</span> We can interpret DIC similar to AIC, in that DICs within $$2 are considered equivalent, those falling 2-4 units below the best model having somewhat less support etc.</p>
<p>Criticisms of DIC (from Spiegelhalter et al. 2014):</p>
<p>a) <span class="math inline">\(p_{D}\)</span> is not invariant to re-parameterization: For example, if you were to rewrite your model in terms of <span class="math inline">\(log(\sigma)\)</span> instead of <span class="math inline">\(\sigma\)</span>, you would get a different <span class="math inline">\(p_{D}\)</span> even if the priors were made equivalent. (<span class="math inline">\(p_{D}\)</span> can also be negative <span class="math inline">\(\dots\)</span> hmmm <span class="math inline">\(\dots\)</span> hardly convincing)</p>
<p>b) Lack of consistency (As the size of the dataset grows to infinity, DIC may not converge to the true model)</p>
<p>c) Not based on a proper predictive criterion</p>
<p>d) “weak” theoretical justification</p>
<p>The DIC is an omnibus measure of fit, and its use is fairly contested among statisticians. (I think some of the complaint is that non-statisticians use it blindly without any knowledge of what it actually is or what the caveats might be.)</p>
<p>We will discuss a fourth method, Bayes Factors, next week when we dig deeper into the world of multimodel inference.</p>
</div>
<div id="for-more-information-about-this-weeks-topic-9" class="section level2">
<h2><span class="header-section-number">24.2</span> For more information about this week’s topic</h2>
<ul>
<li><a href="https://github.com/hlynch/Bayesian2020/tree/master/_data/KassRaftery1995.pdf">Kass and Raftery 1995</a></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-13-lecture.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-14-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
