hist(rtpois(1000, a = 1, b = 212, lambda = 84) )
library(truncnorm)
install.packages("truncnorm")
library(truncnorm)
hist(rtpois(1000, a = 1, b = 212, lambda = 84) )
install.packages("extraDistr")
install.packages("popbio")
install.packages("lattice")
library(readxl)
tauk_131_04_02_s01 <- read_excel("~/Desktop/tauk-131-04-02-s01.xlsx")
View(tauk_131_04_02_s01)
hist(tauk_131_04_02_s01$`Current abundance`)
hist(!is.na(tauk_131_04_02_s01$`Current abundance`))
tauk_131_04_02_s01$`Current abundance`
as.numeric(tauk_131_04_02_s01$`Current abundance`)
hist(as.numeric(tauk_131_04_02_s01$`Current abundance`))
hist(as.numeric(tauk_131_04_02_s01$`Current abundance`),breaks=30)
hist(log(as.numeric(tauk_131_04_02_s01$`Current abundance`),breaks=30))
hist(log(as.numeric(tauk_131_04_02_s01$`Current abundance`+1)),breaks=30)
hist(log(as.numeric(tauk_131_04_02_s01$`Current abundance`)+1),breaks=30)
hist(log(as.numeric(tauk_131_04_02_s01$`Current abundance`)>0),breaks=30)
hist(log(as.numeric(tauk_131_04_02_s01$`Current abundance`[as.numeric(tauk_131_04_02_s01$`Current abundance`)>0])),breaks=30)
hist(log(as.numeric(tauk_131_04_02_s01$`Current abundance`[as.numeric(tauk_131_04_02_s01$`Current abundance`)>0])),breaks=30,freq=TRUE)
hist(log(as.numeric(tauk_131_04_02_s01$`Current abundance`[as.numeric(tauk_131_04_02_s01$`Current abundance`)>0])),breaks=30,freq=F)
install.packages("mvnmle")
library(mvnmle)
data(apple)
mlest(apple)
View(apple)
mean(apple$size)
mean(apple$worms)
mean(apple$worms,na.rm=T)
var(apple$worms)
var(apple$worms,na.rm=T)
var(apple$size,na.rm=T)
var(apple$worm,apple$size,na.rm=T)
var(apple$worm,apple$size)
data(missvals)
View(missvals)
mlest(missvals)
load("~/Downloads/MCMCzpeak.rda")
dim(MCMCzpeak)
names(MCMCzpeak)
length(MCMCzpeak)
dim(MCMCzpeak[[1]])
dim(MCMCzpeak[[2]])
dim(MCMCzpeak[[3]])
names(MCMCzpeak[[1]])
MCMCzpeak[[1]][1:3,1:3]
library("gdata")
library("coda")
library("RPostgreSQL")
library("dplyr")
library(aws.s3)
install.packages("gdata")
install.packages("RPostgreSQL")
install.packages("dplyr")
install.packages("aws.s3")
version<-"sw-thnnlb3u7oclv6ey6ro4"
s3load(paste('ModelBuild/GlobalModel/',version,'/MCMCzpeak.rda', sep = ''), bucket = "penguinmap")
library("gdata")
library("coda")
library("RPostgreSQL")
library("dplyr")
library(aws.s3)
s3load(paste('ModelBuild/GlobalModel/',version,'/MCMCzpeak.rda', sep = ''), bucket = "penguinmap")
sql <- "
select distinct maxcount.site_id, maxcount.season, maxcount.chick, adelielist.count_type, maxcount.count, adelielist.accuracy from
(select site_id, season, case when count_type = 'nests' or count_type = 'adults' then 0 else 1 end as chick, max(count) as count from preprocessed
where season >= 1982 and season <= 2015 and common_name='ad{\\''e}lie penguin' and count > 0 and not preprocessed_id = 192
group by site_id, season, chick) maxcount
inner join
(select site_id, season, case when count_type = 'nests' or count_type = 'adults' then 0 else 1 end as chick, count_type, accuracy, count from preprocessed
where season >= 1982 and season <= 2015 and common_name='ad{\\''e}lie penguin' and count > 0 and not preprocessed_id = 192) adelielist
on maxcount.site_id = adelielist.site_id and maxcount.season = adelielist.season and maxcount.chick = adelielist.chick and maxcount.count = adelielist.count
order by site_id, season, chick;"
pw <- {'p3nguins2017'} #ec2
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = 'mapppd', host = 'mapppdread.cbmvud2go9ni.us-east-1.rds.amazonaws.com', port = 5432, user = 'mapppd_public', password = pw) #ec2
work1 <- dbGetQuery(con, sql)
head(work1)
work2 <- work1 %>%
group_by(site_id, season) %>%
summarise(count_total = sum(count)) %>%
select(site_id, season) %>%
arrange(site_id, season)
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("purrr")
install.packages("purrr")
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
work2 <- work1 %>%
group_by(site_id, season) %>%
summarise(count_total = sum(count)) %>%
select(site_id, season) %>%
arrange(site_id, season)
work2$count <- 0
work2$transition <- 0
for (i in 2:dim(work2)[1]) {
if (work2$site_id[i]==work2$site_id[i-1]) {
if (work2$season[i]==work2$season[i-1] + 1) {
work2$transition[i-1] <- 1
work2$count[i] <- 1
work2$count[i-1] <- 1
}
}
}
work3 <- work2 %>%
group_by(site_id) %>%
summarise(transitions = sum(transition)) %>%
filter(transitions >= 5) %>%
select(site_id, transitions)
work4 <- merge(work3, work2, by = "site_id")
work5 <- work4 %>%
filter(count > 0) %>%
select(site_id, season, transitions, transition)  %>%
arrange(site_id, season)
work6 <- tbl_df(merge(work5, SiteList, by = "site_id", all.x = TRUE)); dim(work5); dim(work6)
# merge median zpeak and predicted growth rate (lambda) for each site year combination
work6$zpeak <- NA
work6$predicted <- NA
load("~/Downloads/SiteList.rda")
work6 <- tbl_df(merge(work5, SiteList, by = "site_id", all.x = TRUE)); dim(work5); dim(work6)
# merge median zpeak and predicted growth rate (lambda) for each site year combination
work6$zpeak <- NA
work6$predicted <- NA
for(i in 1:dim(work6)[1]){
work6$zpeak[i] <- as.integer(round(summary(MCMCzpeak[, matchcols(MCMCzpeak[[1]], with = paste("zpeak\\[", work6$site[i], ",", (work6$season[i] - 1981), "\\]", sep = ""))])[[2]][3]))
work6$predicted[i] <- exp(summary(MCMCzr[, matchcols(MCMCzr[[1]], with = paste("zr\\[", work6$site[i], ",", (work6$season[i] - 1981), "\\]", sep = ""))])[[2]][3])
}
head(work6)
load("~/Downloads/MCMCzr.rda")
for(i in 1:dim(work6)[1]){
work6$zpeak[i] <- as.integer(round(summary(MCMCzpeak[, matchcols(MCMCzpeak[[1]], with = paste("zpeak\\[", work6$site[i], ",", (work6$season[i] - 1981), "\\]", sep = ""))])[[2]][3]))
work6$predicted[i] <- exp(summary(MCMCzr[, matchcols(MCMCzr[[1]], with = paste("zr\\[", work6$site[i], ",", (work6$season[i] - 1981), "\\]", sep = ""))])[[2]][3])
}
head(work6)
head(work6$zpeak)
work6$zpeak[1,]
work6$zpeak[1]
work6[1,]
work6[1:5,]
work6$site_id
unique(work6$site_id)
dim(MCMCzpeak)
dim(MCMCzpeak[1])
dim(MCMCzpeak[[1]])
280*40
280*38
278*38
278*37
278*35
278*34
279*34
279*35
379*30
9576/266
head(SiteList)
median(MCMCzpeak[[1]][,1])
mean(MCMCzpeak[[1]][,1])
quantile(MCMCzpeak[[1]][,1],c(0.025,0.975))
quantile(MCMCzpeak[[1]][,1],c(0.05,0.95))
quantile(MCMCzpeak[[1]][,37],c(0.05,0.95))
quantile(c(MCMCzpeak[[1]][,1],MCMCzpeak[[2]][,1],MCMCzpeak[[3]][,1]),c(0.025,0.975))
quantile(c(MCMCzpeak[[1]][,1],MCMCzpeak[[2]][,1],MCMCzpeak[[3]][,1]),c(0.05,0.95))
adelie_scenes_per_site <- read.csv("~/Downloads/adelie_scenes_per_site.csv")
View(adelie_scenes_per_site)
hist(adelie_scenes_per_site$n_scenes)
hist(adelie_scenes_per_site$n_scenes,breaks=30)
load("~/Downloads/MCMCzstate.rda")
length(MCMCzstate)
MCMCzstate[[1]]
dim(MCMCzstate[[1]])
MCMCsummary(MCMCzstate,
params = 'zstate\\[2,',
ISB = FALSE,
round = 2,
func = function(x) quantile(x, probs = c(.05, .95)),
func_name = c("5%", "95%"))
library(MCMCvis)
install.packages("MCMCvis")
library(MCMCvis)
MCMCsummary(MCMCzstate,
params = 'zstate\\[2,',
ISB = FALSE,
round = 2,
func = function(x) quantile(x, probs = c(.05, .95)),
func_name = c("5%", "95%"))
X <- seq(2, 12, 0.1)
sigma <- 0.10
m <- 1000
n <- 10    # redo for all n in c(10,100,1000)
abundance <- array(data=NA, dim=c(length(X), n, m))
for (i in 1:length(X))  # iterate over mean colony size
{
for (j in 1:n) # iterate over the colonies to be summed
{
abundance[i,j,] <- rlnorm(m,X[i],sigma)  # approximately but not exactly 10% error
}
}
median.abundance <- apply(abundance, c(1,2), FUN=median) # take the median abundance for each colony
sum.median <- apply(median.abundance, 1, sum)
sum.abundance <- apply(abundance, c(1,3), FUN=sum)
median.sum <- apply(sum.abundance, 1, median)
total.bias.10 <- median.sum-sum.median
dim(totsal.bias.10)
dim(total.bias.10)
length(total.bias.10)
plot(total.bias.10)
plot(total.bias.10/sum.median)
n<-1000    # redo for all n in c(10,100,1000)
abundance <- array(data=NA, dim=c(length(X), n, m))
for (i in 1:length(X))  #iterate over mean colony size
{
for (j in 1:n) # iterate over the colonies to be summed
{
abundance[i,j,] <- rlnorm(m, X[i], sigma)  #approximately but not exactly 10% error
}
}
median.abundance <- apply(abundance, c(1,2), FUN=median) # take the median abundance for each colony, dim = length(X),n
sum.median <- apply(median.abundance, 1, sum) # sum the medians, dim = length(X)
sum.abundance <- apply(abundance, c(1,3), FUN=sum) # sum the samples, dim = length(X), m
median.sum <- apply(sum.abundance, 1, median)  #take the median across the sums, dim = length(X)
total.bias.1000 <- median.sum-sum.median
plot(total.bias.1000/sum.median)
plot(total.bias.1000)
plot(sum.median)
plot(total.bias.1000)
plot(total.bias.1000/sum.median)
plot(total.bias.1000)
hist(rlnorm(10000,12,0.1))
X <- seq(2, 12, 0.1)
sigma <- 0.50
m <- 1000
n<-1000    # redo for all n in c(10,100,1000)
abundance <- array(data=NA, dim=c(length(X), n, m))
for (i in 1:length(X))  #iterate over mean colony size
{
for (j in 1:n) # iterate over the colonies to be summed
{
abundance[i,j,] <- rlnorm(m, X[i], sigma)  #approximately but not exactly 10% error
}
}
median.abundance <- apply(abundance, c(1,2), FUN=median) # take the median abundance for each colony, dim = length(X),n
sum.median <- apply(median.abundance, 1, sum) # sum the medians, dim = length(X)
sum.abundance <- apply(abundance, c(1,3), FUN=sum) # sum the samples, dim = length(X), m
median.sum <- apply(sum.abundance, 1, median)  #take the median across the sums, dim = length(X)
total.bias.1000 <- median.sum-sum.median
plot(total.bias.1000/sum.median)
?rlnorm
dim(abundance)
sum.abundance2 <- apply(abundance, c(2,3), FUN=sum)
dim(sum.abundance2)
prior<-rlnorm(1000,0,1e4)
hist(prior)
data<-c(6,0,1,2,1,7,1,5,2,0)
?dpois
dpois(data,lambda=prior[1])
sum(dpois(data,lambda=prior[1],log=T))
sum(dpois(data,lambda=prior[2],log=T))
sum(dpois(data,lambda=prior[3],log=T))
sum(dpois(data,lambda=prior[4],log=T))
prior[1:4]
prior<-rlnorm(1000,0,10)
hist(perior)
hist(prior)
hist(rlnorm(1000,0,0.1))
hist(rlnorm(1000,0,0.5))
hist(rlnorm(1000,0,1))
hist(rlnorm(1000,0,2))
hist(rlnorm(1000,0,1))
hist(rlnorm(1000,1,1))
posterior<-c()
for (i in 1:length(prior))
{
posterior<-c(posterior,sum(dpois(data,lambda=prior[i],log=T))+dlnorm(data,lambda=prior[i],log=T))
}
?dlnorm
posterior<-c()
for (i in 1:length(prior))
{
posterior<-c(posterior,sum(dpois(data,lambda=prior[i],log=T))+dlnorm(prior[i],1,1,log=T))
}
hist(posterior)
quantile(posterior)
hist(exp(posterior))
quantile(exp(posterior))
dlnorm(prior[1],1,1)
dlnorm(prior[2],1,1)
dlnorm(prior[3],1,1)
prior[1:3]
dpois(data,prior[1])
dpois(data,prior[2])
dpois(data,prior[3])
plot(prior,posterior)
plot(prior,posterior,xlim=c(0,10))
mean(data)
plot(prior,posterior,xlim=c(0,5))
plot(prior,exp(posterior),xlim=c(0,5))
setwd("~/Documents/Projects/Bayesian2020")
bookdown::render_book("index.Rmd")
install.packages("ggExtra")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
hist(rlnorm(10000,10,0.2))
quantile(rlnorm(10000,10,0.2))
quantile(rlnorm(10000,10,0.2),probs=c(0.025,0.975))
